[["index.html", "Simulación Estadística Prólogo", " Simulación Estadística Rubén Fernández Casal (ruben.fcasal@udc.es), Ricardo Cao (rcao@udc.es) Edición: Agosto de 2022. Impresión: 2022-08-24 Prólogo Este libro contiene los apuntes de la asignatura de Simulación Estadística del Máster en Técnicas Estadísticas. Este libro ha sido escrito en R-Markdown empleando el paquete bookdown y está disponible en el repositorio Github: rubenfcasal/simbook. Se puede acceder a la versión en línea a través del siguiente enlace: https://rubenfcasal.github.io/simbook/index.html. donde puede descargarse en formato pdf. Para poder ejecutar los ejemplos mostrados en el libro es recomendable emplear el paquete simres, no disponible actualmente en CRAN, aunque se puede instalar la versión de desarrollo en GitHub): # install.packages(&quot;remotes&quot;) remotes::install_github(&quot;rubenfcasal/simres&quot;) Alternativamente se pueden emplear los ficheros de la carpeta codigo. Para instalar los paquetes necesarios se puede emplear el siguiente comando: pkgs &lt;- c(&#39;tictoc&#39;, &#39;boot&#39;, &#39;randtoolbox&#39;, &#39;MASS&#39;, &#39;DEoptim&#39;, &#39;nortest&#39;, &#39;geoR&#39;, &#39;copula&#39;, &#39;sm&#39;, &#39;car&#39;, &#39;tseries&#39;, &#39;forecast&#39;, &#39;plot3D&#39;, &#39;rgl&#39;, &#39;rngWELL&#39;, &#39;randtoolbox&#39;) install.packages(setdiff(pkgs, installed.packages()[,&quot;Package&quot;]), dependencies = TRUE) # Si aparecen errores debidos a incompatibilidades entre las versiones de los paquetes, # probar a ejecutar en lugar de lo anterior: # install.packages(pkgs, dependencies = TRUE) # Instala todos... Para generar el libro (compilar) serán necesarios paquetes adicionales, para lo que se recomendaría consultar el libro de Escritura de libros con bookdown en castellano. Pueden ser también de interés los enlaces mostrados en el Apéndice A. Este obra está bajo una licencia de Creative Commons Reconocimiento-NoComercial-SinObraDerivada 4.0 Internacional (esperamos poder liberarlo bajo una licencia menos restrictiva más adelante). "],["intro.html", "Capítulo 1 Introducción a la simulación", " Capítulo 1 Introducción a la simulación Cuando pensamos en ciencia pensamos en experimentos y en modelos. Se experimenta una y otra vez sobre el fenómeno real que se desea conocer mejor para, con la información así acumulada, construir un modelo teórico, que no es sino una representación simplificada (más o menos acertada) del fenómeno real. Como el modelo se formula en términos matemáticos, en general es susceptible de un estudio analítico del que poder sacar conclusiones. La simulación ofrece una alternativa a esa última fase del proceso, y sustituye (en parte o completamente) el estudio analítico por más experimentación, pero esta vez sobre el propio modelo en lugar de sobre la realidad. Así, se puede definir la simulación como una técnica que consiste en realizar experimentos sobre el modelo de un sistema (experimentos de muestreo si la simulación incorpora aleatoriedad), con el objetivo de recopilar información bajo determinadas condiciones. "],["conceptos-básicos.html", "1.1 Conceptos básicos", " 1.1 Conceptos básicos La experimentación directa sobre la realidad puede tener muchos inconvenientes, entre otros: Coste elevado: por ejemplo cuando las pruebas son destructivas o si es necesario esperar mucho tiempo para observar los resultados. Puede no ser ética: por ejemplo la experimentación sobre seres humanos o la dispersión de un contaminante. Puede resultar imposible: por ejemplo cuando se trata de un acontecimiento futuro o una alternativa en el pasado. Además la realidad puede ser demasiado compleja como para ser estudiada directamente y resultar preferible trabajar con un modelo del sistema real. Un modelo no es más que un conjunto de variables junto con ecuaciones matemáticas que las relacionan y restricciones sobre dichas variables. Habría dos tipos de modelos: Modelos deterministas: en los que bajo las mismas condiciones (fijados los valores de las variables explicativas) se obtienen siempre los mismos resultados. Modelos estocásticos (con componente aleatoria): tienen en cuenta la incertidumbre asociada al modelo. Tradicionalmente se supone que esta incertidumbre es debida a que no se dispone de toda la información sobre las variables que influyen en el fenómeno en estudio (puede ser debida simplemente a que haya errores de medida), lo que se conoce como aleatoriedad aparente: Nothing in Nature is random a thing appears random only through the incompleteness of our knowledge.  Spinoza, Baruch (Ethics, 1677) aunque hoy en día gana peso la idea de la física cuántica de que en el fondo hay una aleatoriedad intrínseca1. La modelización es una etapa presente en la mayor parte de los trabajos de investigación, especialmente en las ciencias experimentales. El modelo debería considerar las variables más relevantes para explicar el fenómeno en estudio y las principales relaciones entre ellas. La inferencia estadística proporciona herramientas para estimar los parámetros y contrastar la validez de un modelo estocástico a partir de los datos observados. La idea es emplear el modelo, asumiendo que es válido, para resolver el problema de interés. Si se puede obtener la solución de forma analítica, esta suele ser exacta (aunque en ocasiones solo se dispone de soluciones aproximadas, basadas en resultados asintóticos, o que dependen de suposiciones que pueden ser cuestionables) y a menudo la resolución también es rápida. Cuando la solución no se puede obtener de modo analítico (o si la aproximación disponible no es adecuada) se puede recurrir a la simulación. De esta forma se pueden obtener resultados para un conjunto más amplio de modelos, que pueden ser mucho más complejos. Nos centraremos en el caso de la simulación estocástica: las conclusiones se obtienen generando repetidamente simulaciones del modelo aleatorio. Muchas veces se emplea la denominación de método Monte Carlo2 como sinónimo de simulación estocástica, pero realmente se trata de métodos especializados que emplean simulación para resolver problemas que pueden no estar relacionados con un modelo estocástico de un sistema real. Por ejemplo, en el Capítulo ?? se tratarán métodos de integración y optimización Monte Carlo. 1.1.1 Ejemplo Supongamos que nos regalan un álbum con \\(n = 75\\) cromos, que se venden sobres con \\(m = 6\\) cromos por 0.8, y que estamos interesados en el número de sobres que hay que comprar para completar la colección, por ejemplo en su valor medio. Podemos aproximar la distribución del número de sobres para completar la colección a partir de \\(nsim=1000\\) simulaciones de coleccionistas de cromos: # Parámetros n &lt;- 75 # Número total de cromos m &lt;- 6 # Número de cromos en cada sobre repe &lt;- TRUE # Repetición de cromos en cada sobre # Número de simulaciones nsim &lt;- 1000 # Resultados simulación nsobres &lt;- numeric(nsim) # evol &lt;- vector(&quot;list&quot;, nsim) # Fijar semilla set.seed(1) # Bucle simulación for (isim in 1:nsim) { # seed &lt;- .Random.seed # .Random.seed &lt;- seed album &lt;- logical(n) i &lt;- 0 # Número de sobres while(sum(album) &lt; n) { i &lt;- i + 1 album[sample(n,m, replace = repe)] &lt;- TRUE } nsobres[isim] &lt;- i } Distribución del número de sobres para completar la colección (aproximada por simulación): hist(nsobres, breaks = &quot;FD&quot;, freq = FALSE, main = &quot;&quot;, xlab = &quot;Número de sobres&quot;) lines(density(nsobres)) Aproximación por simulación del número medio de sobres para completar la colección: sol &lt;- mean(nsobres) sol ## [1] 61.775 Número mínimo de sobres para asegurar de que se completa la colección con una probabilidad del 95%: nmin &lt;- quantile(nsobres, probs = 0.95) ceiling(nmin) ## 95% ## 92 # Reserva de dinero para poder completar la colección el 95% de las veces: ceiling(nmin)*0.8 ## 95% ## 73.6 hist(nsobres, breaks = &quot;FD&quot;, freq = FALSE, main = &quot;&quot;, xlab = &quot;Número de sobres&quot;) lines(density(nsobres)) abline(v = sol) abline(v = nmin, lty = 2) Por supuesto, la distribución del gasto necesario para completar la colección es esta misma reescalada. res &lt;- simres::mc.plot(nsobres*0.8) Aproximación del gasto medio: res$approx # sol*0.8 ## [1] 49.42 En el Ejercicio 1.5 se propone modificar este código para obtener información adicional sobre la evolución del número de cromos distintos dependiendo de los sobres comprados por un coleccionista. 1.1.2 Ventajas e inconvenientes de la simulación Ventajas (Shannon, 1975): Cuando la resolución analítica no puede llevarse a cabo. Cuando existen medios de resolver analíticamente el problema pero dicha resolución es complicada y costosa (o solo proporciona una solución aproximada). Si se desea experimentar antes de que exista el sistema (pruebas para la construcción de un sistema). Cuando es imposible experimentar sobre el sistema real por ser dicha experimentación destructiva. En ocasiones en las que la experimentación sobre el sistema es posible pero no ética. En sistemas que evolucionan muy lentamente en el tiempo. El principal incoveniente puede ser el tiempo de computación necesario, aunque gracias a la gran potencia de cálculo de los computadores actuales, se puede obtener rápidamente una solución aproximada en la mayor parte de los problemas susceptibles de ser modelizados. Además siempre están presentes los posibles problemas debidos a emplear un modelo: La construcción de un buen modelo puede ser una tarea muy costosa (compleja, laboriosa y requerir mucho tiempo; e.g. modelos climáticos). Frecuentemente el modelo omite variables o relaciones importantes entre ellas (los resultados pueden no ser válidos para el sistema real). Resulta difícil conocer la precisión del modelo formulado. Otro problema de la simulación es que se obtienen resultados para unos valores concretos de los parámetros del modelo, por lo que en principio resultaría complicado extrapolar las conclusiones a otras situaciones. 1.1.3 Aplicaciones de la simulación La simulación resulta de utilidad en multitud de contextos diferentes. Los principales campos de aplicación son: Estadística: Muestreo, remuestreo Aproximación de distribuciones (de estadísticos, estimadores) Realización de contrastes, intervalos de confianza Comparación de estimadores, contrastes Validación teoría (distribución asintótica) Inferencia Bayesiana Optimización: Algoritmos genéticos, temple simulado Análisis numérico: Aproximación de integrales, resolución de ecuaciones Computación: Diseño, verificación y validación de algoritmos Criptografía: Protocolos de comunicación segura Física: Simulación de fenómenos naturales En los capítulos ?? y ?? nos centraremos en algunas de las aplicaciones de utilidad en Estadística. Como ejemplo, en física cuántica, la ecuación de Schrödinger es un modelo determinista que describe la evolución en el tiempo de la función de onda de un sistema. Sin embargo, como las funciones de onda pueden cambiar de forma aleatoria al realizar una medición, se emplea la regla de Born para modelar las probabilidades de las distintas posibilidades (algo que inicialmente generó rechazo, dió lugar a la famosa frase de Einstein Dios no juega a los dados, pero experimentos posteriores parecen confirmar). Por tanto en la práctica se emplea un modelo estocástico. Estos métodos surgieron a finales de la década de 1940 como resultado del trabajo realizado por Stanislaw Ulam y John von Neumann en el proyecto Manhattan para el desarrollo de la bomba atómica. Al parecer, como se trataba de una investigación secreta, Nicholas Metropolis sugirió emplear el nombre clave de Monte-Carlo en referencia al casino de Monte Carlo de Mónaco. "],["tipos-de-números-aleatorios.html", "1.2 Tipos de números aleatorios", " 1.2 Tipos de números aleatorios El primer requisito para poder realizar simulación estocástica sería disponer de números aleatorios. Se distingue entre tres tipos de secuencias: números aleatorios puros (true random): se caracteriza porque no existe ninguna regla o plan que nos permita conocer sus valores. números pseudo-aleatorios: simulan realizaciones de una variable aleatoria (uniforme), números cuasi-aleatorios: secuencias deterministas con una distribución más regular en el rango considerado. 1.2.1 Números aleatorios puros Normalmente son obtenidos por procesos físicos (loterías, ruletas, ruidos) y, hasta hace una décadas, se almacenaban en tablas de dígitos aleatorios. Por ejemplo, en 1955 la Corporación RAND publicó el libro A Million Random Digits with 100,000 Normal Deviates que contenía números aleatorios generados mediante una ruleta electrónica conectada a una computadora (ver Figura 1.1). Figura 1.1: Líneas 10580-10594, columnas 21-40, del libro A Million Random Digits with 100,000 Normal Deviates. El procedimiento que se utilizaba para seleccionar de una tabla, de forma manual, números aleatorios en un rango de 1 a m era el siguiente: Se selecciona al azar un punto de inicio en la tabla y la dirección que se seguirá. Se agrupan los dígitos de forma que cubran el valor de m. Se va avanzado en la dirección elegida, seleccionando los valores menores o iguales que m y descartando el resto. Hoy en día están disponibles generadores de números aleatorios online, por ejemplo: RANDOM.ORG: ruido atmosférico (ver paquete random en R). HotBits: desintegración radiactiva. Aunque para un uso profesional es recomendable emplear generadores implementados mediante hardware: Intel Digital Random Number Generator An Overview of Hardware based True Random Number Generators Sus principales aplicaciones hoy en día son en criptografía y juegos de azar, donde resulta especialmente importante su impredecibilidad. El uso de números aleatorios puros presenta dos grandes incovenientes. El principal para su aplicación en el campo de la Estadística (y en otros casos) es que los valores generados deberían ser independientes e idénticamente distribuidos con distribución conocida, algo que resulta difícil (o imposible) de garantizar. Siempre está presente la posible aparición de sesgos, principalmente debidos a fallos del sistema o interferencias. Por ejemplo, en el caso de la máquina RAND, fallos mecánicos en el sistema de grabación de los datos causaron problemas de aleatoriedad (Hacking, 1965, p. 129). El otro inconveniente estaría relacionado con su reproducibilidad, por lo que habría que almacenarlos en tablas si se quieren volver a reproducir los resultados. A partir de la década de 1960, al disponer de computadoras de mayor velocidad, empezó a resultar más eficiente generar valores mediante software en lugar de leerlos de tablas. 1.2.2 Números cuasi-aleatorios Algunos problemas, como la integración numérica (en el Capítulo ?? se tratarán métodos de integración Monte Carlo), no dependen realmente de la aleatoriedad de la secuencia. Para evitar generaciones poco probables, se puede recurrir a secuencias cuasi-aleatorias, también denominadas sucesiones de baja discrepancia (hablaríamos entonces de métodos cuasi-Monte Carlo). La idea sería que la proporción de valores en una región cualquiera sea siempre aproximadamente proporcional a la medida de la región (como sucedería en media con la distribución uniforme, aunque no necesariamente para una realización concreta). Por ejemplo, el paquete randtoolbox de R implementa métodos para la generación de secuencias cuasi-aleatorias (ver Figura 1.2). library(randtoolbox) n &lt;- 2000 par.old &lt;- par( mfrow=c(1,3)) plot(halton(n, dim = 2), xlab = &#39;x1&#39;, ylab = &#39;x2&#39;) plot(sobol(n, dim = 2), xlab = &#39;x1&#39;, ylab = &#39;x2&#39;) plot(torus(n, dim = 2), xlab = &#39;x1&#39;, ylab = &#39;x2&#39;) Figura 1.2: Secuencias cuasi-aleatorias bidimensionales obtenidas con los métodos de Halton (izquierda), Sobol (centro) y Torus (derecha). par(par.old) En este libro sólo consideraremos los números pseudoaleatorios y por comodidad se eliminará el prefijo pseudo en algunos casos. 1.2.3 Números pseudo-aleatorios La mayoría de los métodos de simulación se basan en la posibilidad de generar números pseudoaleatorios que imiten las propiedades de valores independientes de la distribución \\(\\mathcal{U}(0,1)\\), es decir, que imiten las propiedades de una muestra aleatoria simple3 de esta distribución. El procedimiento habitual para obtener estas secuencias es emplear un algoritmo recursivo denominado generador: \\[x_{i} = f\\left( x_{i-1}, x_{i-2}, \\cdots, x_{i-k}\\right)\\] donde: \\(k\\) es el orden del generador. \\(\\left( x_{0},x_{1},\\cdots,x_{k-1}\\right)\\) es la semilla (estado inicial). El periodo o longitud del ciclo es la longitud de la secuencia antes de que vuelva a repetirse. Lo denotaremos por \\(p\\). Los números de la sucesión son predecibles, conociendo el algoritmo y la semilla. Sin embargo, si no se conociesen, no se debería poder distinguir una serie de números pseudoaleatorios de una sucesión de números verdaderamente aleatoria (utilizando recursos computacionales razonables). En caso contrario esta predecibilidad puede dar lugar a serios problemas (e.g. http://eprint.iacr.org/2007/419). Como regla general, por lo menos mientras se está desarrollando un programa, interesa fijar la semilla de aleatorización. Permite la reproducibilidad de los resultados. Facilita la depuración del código. Todo generador de números pseudoaleatorios mínimamente aceptable debe comportarse como si proporcionase muestras genuinas de datos independientes de una \\(\\mathcal{U}(0,1)\\). Otras propiedades de interés son: Reproducibilidad a partir de la semilla. Periodo suficientemente largo. Eficiencia (rapidez y requerimientos de memoria). Portabilidad. Generación de sub-secuencias (computación en paralelo). Parsimonia. Es importante asegurarse de que el generador empleado es adecuado: Random numbers should not be generated with a method chosen at random.  Knuth, D.E. (TAOCP, 2002) Se dispone de una gran cantidad de algoritmos. Los primeros intentos (cuadrados medios, método de Lehmer) resultaron infructuosos, pero al poco tiempo ya se propusieron métodos que podían ser ampliamente utilizados (estableciendo adecuadamente sus parámetros). Entre ellas podríamos destacar: Generadores congruenciales. Registros desfasados. Combinaciones de distintos algoritmos. La recomendación sería emplear un algoritmo conocido y que haya sido estudiado en profundidad (por ejemplo el generador Mersenne-Twister empleado por defecto en R, propuesto por Matsumoto y Nishimura, 1998). Además, sería recomendable utilizar alguna de las implementaciones disponibles en múltiples librerías, por ejemplo: GNU Scientific Library (GSL): http://www.gnu.org/software/gsl/manual StatLib: http://lib.stat.cmu.edu Numerical recipes: http://www.nrbook.com/nr3 UNU.RAN (paquete Runuran): http://statmath.wu.ac.at/unuran En este libro nos centraremos en los generadores congruenciales, descritos en la Sección 2.1. Estos métodos son muy simples, aunque con las opciones adecuadas podrían ser utilizados en pequeños estudios de simulación. Sin embargo, su principal interés es que constituyen la base de los generadores avanzados habitualmente considerados. Aunque hay que distinguir entre secuencia y muestra. En un problema de inferencia, en principio estamos interesados en una característica desconocida de la población. En cambio, en un problema de simulación la población es el modelo y lo conocemos por completo (no obstante el problema de simulación puede surgir como solución de un problema de inferencia). "],["rrng.html", "1.3 Números aleatorios en R", " 1.3 Números aleatorios en R La generación de números pseudoaleatorios en R es una de las mejores disponibles en paquetes estadísticos. Entre las herramientas implementadas en el paquete base de R podemos destacar: set.seed(entero): permite establecer la semilla (y el generador). RNGkind(): selecciona el generador. rdistribución(n,...): genera valores aleatorios de la correspondiente distribución. Por ejemplo, runif(n, min = 0, max = 1), generaría n valores de una uniforme. Se puede acceder al listado completo de las funciones disponibles en el paquete stats mediante el comando ?distributions. sample(): genera muestras aleatorias de variables discretas y permutaciones (se tratará en el Capítulo ??). simulate(): genera realizaciones de la respuesta de un modelo ajustado. Además están disponibles otros paquetes que implementan distribuciones adicionales (ver CRAN Task View: Probability Distributions). Entre ellos podríamos destacar los paquetes distr (clases S4; con extensiones en otros paquetes) y distr6 (clases R6). La semilla se almacena (en globalenv) en .Random.seed; es un vector de enteros cuya dimensión depende del tipo de generador: No debe ser modificado manualmente; se guarda con el entorno de trabajo (por ejemplo, si se guarda al terminar la sesión en un fichero .RData, se restaurará la semilla al iniciar una nueva sesión y podremos continuar con las simulaciones). Si no se especifica con set.seed (o no existe) se genera a partir del reloj del sistema. Puede ser recomendable almacenarla antes de generar simulaciones, e.g. seed &lt;- .Random.seed. Esto permite reproducir los resultados y facilita la depuración de posibles errores. En la mayoría de los ejemplos de este libro se generan todos los valores de una vez, se guardan y se procesan vectorialmente (normalmente empleando la función apply). En problemas mas complejos, en los que no es necesario almacenar todas las simulaciones, puede ser preferible emplear un bucle para generar y procesar cada simulación iterativamente. Por ejemplo podríamos emplear el siguiente esquema: # Fijar semilla set.seed(1) for (isim in 1:nsim) { seed &lt;- .Random.seed # Si se produce un error, podremos depurarlo ejecutando: # .Random.seed &lt;- seed ... # Generar valores pseudoaleatorios ... } o alternativamente fijar la semilla en cada iteración, por ejemplo: for (isim in 1:nsim) { set.seed(isim) ... # Generar valores pseudoaleatorios ... } 1.3.1 Opciones Normalmente no nos va a interesar cambiar las opciones por defecto de R para la generación de números pseudoaleatorios. Para establecer estas opciones podemos emplear los argumentos kind = NULL, normal.kind = NULL y sample.kind = NULL en las funciones RNGkind() o set.seed(). A continuación se muestran las distintas opciones (resaltando en negrita los valores por defecto): kind especifica el generador pseudoaleatorio (uniforme): Wichmann-Hill: Ciclo \\(6.9536\\times10^{12}\\) Marsaglia-Multicarry: Ciclo mayor de \\(2^{60}\\) Super-Duper: Ciclo aprox. \\(4.6\\times10^{18}\\) (S-PLUS) Mersenne-Twister: Ciclo \\(2^{19937}-1\\) y equidistribution en 623 dimensiones. Knuth-TAOCP-2002: Ciclo aprox. \\(2^{129}\\). Knuth-TAOCP user-supplied: permite emplear generadores adicionales. normal.kind selecciona el método de generación de normales (se tratará más adelante): Kinderman-Ramage, Buggy Kinderman-Ramage, Ahrens-Dieter, Box-Muller, Inversion , o user-supplied. sample.kind selecciona el método de generación de uniformes discretas (el empleado por la función sample(), que cambió ligeramente4 a partir de la versión 3.6.0 de R): Rounding (versión anterior a 3.6.0) o Rejection. Estas opciones están codificadas (con índices comenzando en 0) en el primer componente de la semilla: set.seed(1) .Random.seed[1] ## [1] 10403 Los dos últimos dígitos se corresponden con el generador, las centenas con el método de generación de normales y las decenas de millar con el método uniforme discreto. 1.3.2 Paquetes de R Otros paquetes de R que pueden ser de interés: setRNG contiene herramientas que facilitan operar con la semilla (dentro de funciones,). random permite la descarga de números true random desde RANDOM.ORG. randtoolbox implementa generadores más recientes (rngWELL) y generación de secuencias cuasi-aleatorias. RDieHarder implementa diversos contrastes para el análisis de la calidad de un generador y varios generadores. Runuran interfaz para la librería UNU.RAN para la generación (automática) de variables aleatorias no uniformes (ver Hörmann et al., 2004). rsprng, rstream y rlecuyer implementan la generación de múltiples secuencias (para programación paralela). gls, rngwell19937, randaes, SuppDists, lhs, mc2d, fOptions,  1.3.3 Tiempo de CPU La velocidad del generador suele ser una característica importante (también medir los tiempos, de cada iteración y de cada procedimento, en estudios de simulación). Para evaluar el rendimiento están disponibles en R distintas herramientas: proc.time(): permite obtener tiempo de computación real y de CPU. tini &lt;- proc.time() # Código a evaluar tiempo &lt;- proc.time() - tini system.time(expresión): muestra el tiempo de computación (real y de CPU) de expresión. Por ejemplo, podríamos emplear las siguientes funciones para ir midiendo los tiempos de CPU durante una simulación: CPUtimeini &lt;- function() { .tiempo.ini &lt;&lt;- proc.time() .tiempo.last &lt;&lt;- .tiempo.ini } CPUtimeprint &lt;- function() { tmp &lt;- proc.time() cat(&quot;Tiempo última operación:\\n&quot;) print(tmp-.tiempo.last) cat(&quot;Tiempo total operación:\\n&quot;) print(tmp-.tiempo.ini) .tiempo.last &lt;&lt;- tmp } Llamando a CPUtimeini() donde se quiere empezar a contar, y a CPUtimeprint() para imprimir el tiempo total y el tiempo desde la última llamada a una de estas funciones. Ejemplo: funtest &lt;- function(n) mad(runif(n)) CPUtimeini() result1 &lt;- funtest(10^6) CPUtimeprint() ## Tiempo última operación: ## user system elapsed ## 0.13 0.00 0.12 ## Tiempo total operación: ## user system elapsed ## 0.13 0.00 0.12 result2 &lt;- funtest(10^3) CPUtimeprint() ## Tiempo última operación: ## user system elapsed ## 0 0 0 ## Tiempo total operación: ## user system elapsed ## 0.13 0.00 0.12 Hay diversos paquetes que implementan herramientas similares, por ejemplo: El paquete tictoc: tic(\"mensaje\"): inicia el temporizador y almacena el tiempo de inicio junto con el mensaje en una pila. toc(): calcula el tiempo transcurrido desde la llamada correspondiente a tic(). library(tictoc) ## Timing nested code tic(&quot;outer&quot;) result1 &lt;- funtest(10^6) tic(&quot;middle&quot;) result2 &lt;- funtest(10^3) tic(&quot;inner&quot;) result3 &lt;- funtest(10^2) toc() # inner ## inner: 0 sec elapsed toc() # middle ## middle: 0.01 sec elapsed toc() # outer ## outer: 0.09 sec elapsed ## Timing in a loop and analyzing the results later using tic.log(). tic.clearlog() for (i in 1:10) { tic(i) result &lt;- funtest(10^4) toc(log = TRUE, quiet = TRUE) } # log.txt &lt;- tic.log(format = TRUE) # log.lst &lt;- tic.log(format = FALSE) log.times &lt;- do.call(rbind.data.frame, tic.log(format = FALSE)) str(log.times) ## &#39;data.frame&#39;: 10 obs. of 3 variables: ## $ tic: num 4.65 4.65 4.65 4.65 4.65 4.65 4.65 4.67 4.67 4.67 ## $ toc: num 4.65 4.65 4.65 4.65 4.65 4.65 4.67 4.67 4.67 4.67 ## $ msg: chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... tic.clearlog() # timings &lt;- unlist(lapply(log.lst, function(x) x$toc - x$tic)) log.times$timings &lt;- with(log.times, toc - tic) summary(log.times$timings) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 0.000 0.000 0.002 0.000 0.020 La función cpu.time() del paquete simres: cpu.time(restart = TRUE): inicia el temporizador y almacena el tiempo de inicio. cpu.time(): calcula el tiempo (real y de CPU) total (desde tiempo de inicio) y parcial (desde la última llamada a esta función). Hay que tener en cuenta que, por construcción, aunque se realicen en la mismas condiciones (en el mismo equipo), los tiempos de CPU en R pueden variar ligeramente entre ejecuciones. Si se quieren estudiar tiempos de computación de forma más precisa, se recomendaría promediar los tiempos de varias ejecuciones. Para ello se pueden emplear las herramientas del paquete microbenchmark. No obstante, para los fines de este libro no será necesaria tanta precisión. Finalmente, si los tiempos de computación no fuesen asumibles, para identificar los cuellos de botella y mejorar el código para optimizar la velocidad, podríamos emplear la función Rprof(fichero). Esta función permite evaluar el rendimiento muestreando la pila en intervalos para determinar en que funciones se emplea el tiempo de computación. Después de ejecutar el código, llamando a Rprof(NULL) se desactiva el muestreo y con summaryRprof(fichero) se muestran los resultados (para analizarlos puede resultar de utilidad el paquete proftools). Para evitar problemas de redondeo con tamaños extremadamente grandes; ver bug PR#17494. "],["ejercicios.html", "1.4 Ejercicios", " 1.4 Ejercicios Ejercicio 1.1 Sea \\((X,Y)\\) es un vector aleatorio con distribución uniforme en el cuadrado \\([-1,1]\\times\\lbrack-1,1]\\) de área 4. Aproximar mediante simulación \\(P\\left(X + Y \\leq 0 \\right)\\) y compararla con la probabilidad teórica (obtenida aplicando la regla de Laplace \\(\\frac{\\text{área favorable}}{\\text{área posible}}\\)). Aproximar el valor de \\(\\pi\\) mediante simulación a partir de \\(P\\left( X^2 +Y^2 \\leq 1 \\right)\\). Ver solución en Sección D.1.1. Ejercicio 1.2 (Experimento de Bernoulli) Consideramos el experimento de Bernoulli consistente en el lanzamiento de una moneda. Empleando la función sample, obtener 1000 simulaciones del lanzamiento de una moneda (0 = cruz, 1 = cara), suponiendo que no está trucada. Aproximar la probabilidad de cara a partir de las simulaciones. En R pueden generarse valores de la distribución de Bernoulli mediante la función rbinom(nsim, size=1, prob). Generar un gráfico de lineas considerando en el eje \\(X\\) el número de lanzamientos (de 1 a 10000) y en el eje \\(Y\\) la frecuencia relativa del suceso cara (puede ser recomendable emplear la función cumsum). Ver solución en Sección D.1.2. Ejercicio 1.3 (Simulación de un circuito) Simular el paso de corriente a través del siguiente circuito, donde figuran las probabilidades de que pase corriente por cada uno de los interruptores: Considerar que cada interruptor es una variable aleatoria de Bernoulli independiente para simular 1000 valores de cada una de ellas. Nota: R maneja internamente los valores lógicos como 1 (TRUE) y 0 (FALSE). Recíprocamente, cualquier número puede ser tratado como lógico (al estilo de C). El entero 0 es equivalente a FALSE y cualquier entero distinto de 0 a TRUE. Ver solución en Sección D.1.3. Ejercicio 1.4 (El problema del Caballero de Méré) En 1651, el Caballero de Méré le planteó a Pascal una pregunta relacionada con las apuestas y los juegos de azar: ¿es ventajoso apostar a que en cuatro lanzamientos de un dado se obtiene al menos un seis? Este problema generó una fructífera correspondencia entre Pascal y Fermat que se considera, simbólicamente, como el nacimiento del Cálculo de Probabilidades. Escribir una función que simule el lanzamiento de \\(n\\) dados. El parámetro de entrada es el número de lanzamientos \\(n\\), que toma el valor 4 por defecto, y la salida debe ser TRUE si se obtiene al menos un 6 y FALSE en caso contrario. Utilizar la función anterior para simular \\(nsim=10000\\) jugadas de este juego y calcular la proporción de veces que se gana la apuesta (obtener al menos un 6 en \\(n\\) lanzamientos), usando \\(n=4\\). Comparar el resultado con la probabilidad teórica \\(1-(5/6)^{n}\\). Ver solución en Sección D.1.4. Ejercicio 1.5 (variación del problema del coleccionista (cadena de Markov)) Continuando con el ejemplo de la Sección 1.1.1 (álbum con \\(n = 75\\) cromos y sobres con \\(m = 6\\)). A partir de \\(nsim=2000\\) simulaciones de coleccionistas de cromos, aproximar por simulación la evolución del proceso de compra de un coleccionista (número de cromos distintos dependiendo de los sobres comprados). Ver solución en Sección D.1.5. "],["gen-pseudo.html", "Capítulo 2 Generación de números pseudoaleatorios", " Capítulo 2 Generación de números pseudoaleatorios Como ya se comentó, los distintos métodos de simulación requieren disponer de secuencias de números pseudoaleatorios que imiten las propiedades de generaciones independientes de una distribución \\(\\mathcal{U}(0,1)\\). En primer lugar nos centraremos en el caso de los generadores congruenciales. A pesar de su simplicidad, podrían ser adecuados en muchos casos y constituyen la base de los generadores avanzados habitualmente considerados. Posteriormente se dará una visión de las diferentes herramientas para estudiar la calidad de un generador de números pseudoaleatorios. "],["gen-cong.html", "2.1 Generadores congruenciales lineales", " 2.1 Generadores congruenciales lineales En los generadores congruenciales lineales se considera una combinación lineal de los últimos \\(k\\) enteros generados y se calcula su resto al dividir por un entero fijo \\(m\\). En el método congruencial simple (de orden \\(k = 1\\)), partiendo de una semilla inicial \\(x_0\\), el algoritmo secuencial es el siguiente: \\[\\begin{aligned} x_{i} &amp; = (ax_{i-1}+c) \\bmod m \\\\ u_{i} &amp; = \\dfrac{x_{i}}{m} \\\\ i &amp; =1,2,\\ldots \\end{aligned}\\] donde \\(a\\) (multiplicador), \\(c\\) (incremento) y \\(m\\) (módulo) son enteros positivos5 fijados de antemano (los parámetros de este generador). Si \\(c=0\\) el generador se denomina congruencial multiplicativo (Lehmer, 1951) y en caso contrario se dice que es mixto (Rotenburg, 1960). Obviamente los parámetros y la semilla determinan los valores generados, que también se pueden obtener de forma no recursiva: \\[x_{i}=\\left( a^{i}x_0+c\\frac{a^{i}-1}{a-1}\\right) \\bmod m\\] Este método está implementado6 en la función rlcg() del paquete simres, imitando el funcionamiento del generador uniforme de R (ver también simres::rng(); fichero rng.R): simres::rlcg ## function(n, seed = as.numeric(Sys.time()), a = 7^5, c = 0, m = 2^31 - 1) { ## u &lt;- numeric(n) ## for(i in 1:n) { ## seed &lt;- (a * seed + c) %% m ## u[i] &lt;- seed/m # (seed + 1)/(m + 1) ## } ## # Almacenar semilla y parámetros ## assign(&quot;.rng&quot;, list(seed = seed, type = &quot;lcg&quot;, ## parameters = list(a = a, c = c, m = m)), envir = globalenv()) ## # .rng &lt;&lt;- list(seed = seed, type = &quot;lcg&quot;, parameters = list(a = a, c = c, m = m)) ## # Para continuar con semilla y parámetros: ## # with(.rng, rlcg(n, seed, parameters$a, parameters$c, parameters$m)) ## # Devolver valores ## return(u) ## } ## &lt;bytecode: 0x000000002f7288a0&gt; ## &lt;environment: namespace:simres&gt; Ejemplos de parámetros: \\(c=0\\), \\(a=2^{16}+3=65539\\) y \\(m=2^{31}\\), generador RANDU de IBM (no recomendable). \\(c=0\\), \\(a=7^{5}=16807\\) y \\(m=2^{31}-1\\) (primo de Mersenne), Park y Miller (1988) minimal standar, empleado por las librerías IMSL y NAG. \\(c=0\\), \\(a=48271\\) y \\(m=2^{31}-1\\) actualización del minimal standar propuesta por Park, Miller y Stockmeyer (1993). A pesar de su simplicidad, una adecuada elección de los parámetros permite obtener de manera eficiente secuencias de números aparentemente i.i.d. \\(\\mathcal{U}(0,1)\\). Durante los primeros años, el procedimiento habitual consistía en escoger \\(m\\) de forma que se pudiera realizar eficientemente la operación del módulo, aprovechando la arquitectura del ordenador (por ejemplo \\(m = 2^{31}\\) si se emplean enteros con signo de 32 bits). Posteriormente se seleccionaban \\(c\\) y \\(a\\) de forma que el período \\(p\\) fuese lo más largo posible (o suficientemente largo), empleando los resultados mostrados a continuación. Teorema 2.1 (Hull y Dobell, 1962) Un generador congruencial tiene período máximo (\\(p=m\\)) si y solo si: \\(c\\) y \\(m\\) son primos relativos (i.e. \\(m.c.d.(c, m) = 1\\)). \\(a-1\\) es múltiplo de todos los factores primos de \\(m\\) (i.e. \\(a \\equiv 1 \\bmod q\\), para todo \\(q\\) factor primo de \\(m\\)). Si \\(m\\) es múltiplo de \\(4\\), entonces \\(a-1\\) también lo ha de ser (i.e. \\(m \\equiv 0 \\bmod 4\\Rightarrow a \\equiv 1 \\bmod 4\\)). Algunas consecuencias: Si \\(m\\) primo, \\(p=m\\) si y solo si \\(a=1\\). Un generador multiplicativo no cumple la condición 1 (\\(m.c.d.(0, m)=m\\)). Teorema 2.2 Un generador multiplicativo tiene período máximo (\\(p=m-1\\)) si: \\(m\\) es primo. \\(a\\) es una raiz primitiva de \\(m\\) (i.e. el menor entero \\(q\\) tal que \\(a^{q}=1 \\bmod m\\) es \\(q=m-1\\)). Además de preocuparse de la longitud del ciclo, las secuencias generadas deben aparentar muestras i.i.d. \\(\\mathcal{U}(0,1)\\). Uno de los principales problemas es que los valores generados pueden mostrar una clara estructura reticular. Este es el caso por ejemplo del generador RANDU de IBM muy empleado en la década de los 70 (ver Figura 2.1)7. Por ejemplo, el conjunto de datos randu contiene 400 tripletas de números sucesivos obtenidos con la implementación de VAX/VMS 1.5 (1977). library(simres) system.time(u &lt;- rlcg(n = 9999, seed = 543210, a = 2^16 + 3, c = 0, m = 2^31)) ## user system elapsed ## 0 0 0 # xyz &lt;- matrix(u, ncol = 3, byrow = TRUE) xyz &lt;- stats::embed(u, 3) library(plot3D) # points3D(xyz[,1], xyz[,2], xyz[,3], colvar = NULL, phi = 60, # theta = -50, pch = 21, cex = 0.2) points3D(xyz[,3], xyz[,2], xyz[,1], colvar = NULL, phi = 60, theta = -50, pch = 21, cex = 0.2) Figura 2.1: Grafico de dispersión de tripletas del generador RANDU de IBM (contenidas en 15 planos). En general todos los generadores de este tipo van a presentar estructuras reticulares. Marsaglia (1968) demostró que las \\(k\\)-uplas de un generadores multiplicativo están contenidas en a lo sumo \\(\\left(k!m\\right)^{1/k}\\) hiperplanos paralelos (para más detalles sobre la estructura reticular, ver por ejemplo Ripley, 1987, sección 2.7). Por tanto habría que seleccionar adecuadamente \\(m\\) y \\(c\\) (\\(a\\) solo influiría en la pendiente) de forma que la estructura reticular sea imperceptible teniendo en cuenta el número de datos que se pretende generar (por ejemplo de forma que la distancia mínima entre los puntos sea próxima a la esperada en teoría). Se han propuesto diversas pruebas (ver Sección 2.3) para determinar si un generador tiene problemas de este tipo y se han realizado numerosos estudios para determinadas familias (e.g. Park y Miller, 1988, estudiaron que parámetros son adecuados para \\(m=2^{31}-1\\)). En ciertos contextos muy exigentes (por ejemplo en criptografía), se recomienda considerar un periodo de seguridad \\(\\approx \\sqrt{p}\\) para evitar este tipo de problemas. Aunque estos generadores tienen limitaciones en su capacidad para producir secuencias muy largas de números i.i.d. \\(\\mathcal{U}(0,1)\\), son un elemento básico en generadores más avanzados (siguiente sección). Ejemplo 2.1 Consideramos el generador congruencial, de ciclo máximo, definido por: \\[\\begin{aligned} x_{n+1} &amp; =(5x_{n}+1)\\ \\bmod\\ 512,\\nonumber\\\\ u_{n+1} &amp; =\\frac{x_{n+1}}{512},\\ n=0,1,\\dots\\nonumber \\end{aligned}\\] Generar 500 valores de este generador, obtener el tiempo de CPU, representar su distribución mediante un histograma (en escala de densidades) y compararla con la densidad teórica. set.rng(321, &quot;lcg&quot;, a = 5, c = 1, m = 512) # Establecer semilla y parámetros nsim &lt;- 500 system.time(u &lt;- rng(nsim)) ## user system elapsed ## 0 0 0 hist(u, freq = FALSE) abline(h = 1) # Densidad uniforme Figura 2.2: Histograma de los valores generados. En este caso concreto la distribución de los valores generados es aparentemente más uniforme de lo que cabría esperar, lo que induciría a sospechar de la calidad de este generador (ver Ejemplo 2.2 en Sección 2.3). Calcular la media de las simulaciones (mean) y compararla con la teórica. La aproximación por simulación de la media teórica es: mean(u) ## [1] 0.4999609 La media teórica es 0.5. Error absoluto \\(3.90625\\times 10^{-5}\\). Aproximar (mediante simulación) la probabilidad del intervalo \\((0.4;0.8)\\) y compararla con la teórica. La probabilidad teórica es 0.8 - 0.4 = 0.4 La aproximación mediante simulación: sum((0.4 &lt; u) &amp; (u &lt; 0.8))/nsim ## [1] 0.402 mean((0.4 &lt; u) &amp; (u &lt; 0.8)) # Alternativa ## [1] 0.402 Se supone además que \\(a\\), \\(c\\) y \\(x_0\\) son menores que \\(m\\), ya que, dadas las propiedades algebraicas de la suma y el producto en el conjunto de clases de resto módulo \\(m\\) (que es un anillo), cualquier otra elección de valores mayores o iguales que \\(m\\) tiene un equivalente verificando esta restricción. Aunque de forma no muy eficiente. Para evitar problemas computacionales, se recomienda realizar el cálculo de los valores empleando el método de Schrage (ver Bratley et al., 1987; LEcuyer, 1988). Alternativamente se podría utilizar la función plot3d del paquete rgl, y rotar la figura (pulsando con el ratón) para ver los hiperplanos: rgl::plot3d(xyz) "],["extensiones.html", "2.2 Extensiones", " 2.2 Extensiones Se han considerado diversas extensiones del generador congruencial lineal simple: Lineal múltiple: \\(x_{i}= a_0 + a_1 x_{i-1} + a_2 x_{i-2} + \\cdots + a_{k} x_{i-k} \\bmod m\\), con periodo \\(p\\leq m^{k}-1\\). No lineal: \\(x_{i} = f\\left( x_{i-1}, x_{i-2}, \\cdots, x_{i-k} \\right) \\bmod m\\). Por ejemplo \\(x_{i} = a_0 + a_1 x_{i-1} + a_2 x_{i-1}^2 \\bmod m\\). Matricial: \\(\\boldsymbol{x}_{i} = A_0 + A_1\\boldsymbol{x}_{i-1} + A_2\\boldsymbol{x}_{i-2} + \\cdots + A_{k}\\boldsymbol{x}_{i-k} \\bmod m\\). Un ejemplo de generador congruencia lineal múltiple es el denominado generador de Fibonacci retardado (Fibonacci-lagged generator; Knuth, 1969): \\[x_n = (x_{n-37} + x_{n-100}) \\bmod 2^{30},\\] con un período aproximado de \\(2^{129}\\) y que puede ser empleado en R (lo cual no sería en principio recomendable; ver Knuth Recent News 2002) estableciendo kind a \"Knuth-TAOCP-2002\" o \"Knuth-TAOCP\" en la llamada a set.seed() o RNGkind(). El generador Mersenne-Twister (Matsumoto y Nishimura, 1998), empleado por defecto en R, de periodo \\(2^{19937}-1\\) y equidistribution en 623 dimensiones, se puede expresar como un generador congruencial matricial lineal. En cada iteración (twist) genera 624 valores (los últimos componentes de la semilla son los 624 enteros de 32 bits correspondientes, el segundo componente es el índice/posición correspondiente al último valor devuelto; el conjunto de enteros solo cambia cada 624 generaciones). set.seed(1) u &lt;- runif(1) seed &lt;- .Random.seed u &lt;- runif(623) sum(seed != .Random.seed) ## [1] 1 # Solo cambia el índice: seed[2]; .Random.seed[2] ## [1] 1 ## [1] 624 u &lt;- runif(1) # Cada 624 generaciones cambia el conjunto de enteros y el índice se inicializa sum(seed != .Random.seed) ## [1] 624 seed[2]; .Random.seed[2] ## [1] 1 ## [1] 1 Un caso particular del generador lineal múltiple son los denominados generadores de registros desfasados (más relacionados con la criptografía). Se generan bits de forma secuencial considerando \\(m=2\\) y \\(a_{i} \\in \\left \\{ 0,1\\right \\}\\) y se van combinando \\(l\\) bits para obtener valores en el intervalo \\((0, 1)\\), por ejemplo \\(u_i = 0 . x_{it+1} x_{it+2} \\ldots x_{it+l}\\), siendo \\(t\\) un parámetro denominado aniquilación (Tausworthe, 1965). Los cálculos se pueden realizar rápidamente mediante operaciones lógicas (los sumandos de la combinación lineal se traducen en un o exclusivo XOR), empleando directamente los registros del procesador (ver por ejemplo, Ripley, 1987, Algoritmo 2.1). Otras alternativas consisten en la combinanción de varios generadores, las más empleadas son: Combinar las salidas: por ejemplo \\(u_{i}=\\sum_{l=1}^L u_{i}^{(l)} \\bmod 1\\), donde \\(u_{i}^{(l)}\\) es el \\(i\\)-ésimo valor obtenido con el generador \\(l\\). Barajar las salidas: por ejemplo se crea una tabla empleando un generador y se utiliza otro para seleccionar el índice del valor que se va a devolver y posteriormente actualizar. El generador LEcuyer-CMRG (LEcuyer, 1999), empleado como base para la generación de múltiples secuencias en el paquete parallel, combina dos generadores concruenciales lineales múltiples de orden \\(k=3\\) (el periodo aproximado es \\(2^{191}\\)). "],["calgen.html", "2.3 Análisis de la calidad de un generador", " 2.3 Análisis de la calidad de un generador Para verificar si un generador tiene las propiedades estadísticas deseadas hay disponibles una gran cantidad de test de hipótesis y métodos gráficos, incluyendo métodos genéricos (de bondad de ajuste y aleatoriedad) y contrastes específicos para generadores aleatorios. Se trata principalmente de contrastar si las muestras generadas son i.i.d. \\(\\mathcal{U}\\left(0,1\\right)\\) (análisis univariante). Aunque los métodos más avanzados tratan de contrastar si las \\(d\\)-uplas: \\[(U_{t+1},U_{t+2},\\ldots,U_{t+d}); \\ t=(i-1)d, \\ i=1,\\ldots,m\\] son i.i.d. \\(\\mathcal{U}\\left(0,1\\right)^{d}\\) (uniformes independientes en el hipercubo; análisis multivariante). En el Apéndice B se describen algunos de estos métodos. En esta sección emplearemos únicamente métodos genéricos, ya que también pueden ser de utilidad para evaluar generadores de variables no uniformes y para la construcción de modelos del sistema real (e.g. para modelar variables que se tratarán como entradas del modelo general). Sin embargo, los métodos clásicos pueden no ser muy adecuados para evaluar generadores de números pseudoaleatorios (ver LEcuyer y Simard, 2007). La recomendación sería emplear baterías de contrastes recientes, como las descritas en la Subsección 2.3.2. Hay que destacar algunas diferencias entre el uso de este tipo de métodos en inferencia y en simulación. Por ejemplo, si empleamos un constrate de hipótesis del modo habitual, desconfiamos del generador si la muestra (secuencia) no se ajusta a la distribución teórica (\\(p\\)-valor \\(\\leq \\alpha\\)). En simulación, además, también se sospecha si se ajusta demasiado bien a la distribución teórica (\\(p\\)-valor \\(\\geq1-\\alpha\\)), lo que indicaría que no reproduce adecuadamente la variabilidad. Uno de los contrastes más conocidos es el test chi-cuadrado de bondad de ajuste (chisq.test para el caso discreto). Aunque si la variable de interés es continua, habría que discretizarla (con la correspondiente perdida de información). Por ejemplo, se podría emplear la función simres::chisq.cont.test() (fichero test.R), que imita a las incluidas en R: simres::chisq.cont.test ## function(x, distribution = &quot;norm&quot;, nclass = floor(length(x)/5), ## output = TRUE, nestpar = 0, ...) { ## # Función distribución ## q.distrib &lt;- eval(parse(text = paste(&quot;q&quot;, distribution, sep = &quot;&quot;))) ## # Puntos de corte ## q &lt;- q.distrib((1:(nclass - 1))/nclass, ...) ## tol &lt;- sqrt(.Machine$double.eps) ## xbreaks &lt;- c(min(x) - tol, q, max(x) + tol) ## # Gráficos y frecuencias ## if (output) { ## xhist &lt;- hist(x, breaks = xbreaks, freq = FALSE, ## lty = 2, border = &quot;grey50&quot;) ## # Función densidad ## d.distrib &lt;- eval(parse(text = paste(&quot;d&quot;, distribution, sep = &quot;&quot;))) ## curve(d.distrib(x, ...), add = TRUE) ## } else { ## xhist &lt;- hist(x, breaks = xbreaks, plot = FALSE) ## } ## # Cálculo estadístico y p-valor ## O &lt;- xhist$counts # Equivalente a table(cut(x, xbreaks)) pero más eficiente ## E &lt;- length(x)/nclass ## DNAME &lt;- deparse(substitute(x)) ## METHOD &lt;- &quot;Pearson&#39;s Chi-squared test&quot; ## STATISTIC &lt;- sum((O - E)^2/E) ## names(STATISTIC) &lt;- &quot;X-squared&quot; ## PARAMETER &lt;- nclass - nestpar - 1 ## names(PARAMETER) &lt;- &quot;df&quot; ## PVAL &lt;- pchisq(STATISTIC, PARAMETER, lower.tail = FALSE) ## # Preparar resultados ## classes &lt;- format(xbreaks) ## classes &lt;- paste(&quot;(&quot;, classes[-(nclass + 1)], &quot;,&quot;, classes[-1], &quot;]&quot;, ## sep = &quot;&quot;) ## RESULTS &lt;- list(classes = classes, observed = O, expected = E, ## residuals = (O - E)/sqrt(E)) ## if (output) { ## cat(&quot;\\nPearson&#39;s Chi-squared test table\\n&quot;) ## print(as.data.frame(RESULTS)) ## } ## if (any(E &lt; 5)) ## warning(&quot;Chi-squared approximation may be incorrect&quot;) ## structure(c(list(statistic = STATISTIC, parameter = PARAMETER, p.value = PVAL, ## method = METHOD, data.name = DNAME), RESULTS), class = &quot;htest&quot;) ## } ## &lt;bytecode: 0x00000000152dce60&gt; ## &lt;environment: namespace:simres&gt; Ejemplo 2.2 (análisis de un generador congruencial, continuación) Continuando con el generador congruencial del Ejemplo 2.1: set.rng(321, &quot;lcg&quot;, a = 5, c = 1, m = 512) # Establecer semilla y parámetros nsim &lt;- 500 u &lt;- rng(nsim) Al aplicar el test chi-cuadrado obtendríamos: chisq.cont.test(u, distribution = &quot;unif&quot;, nclass = 10, nestpar = 0, min = 0, max = 1) Figura 2.3: Gráfico resultante de aplicar la función chisq.cont.test() comparando el histograma de los valores generados con la densidad uniforme. ## ## Pearson&#39;s Chi-squared test table ## classes observed expected residuals ## 1 (-1.490116e-08, 1.000000e-01] 51 50 0.1414214 ## 2 ( 1.000000e-01, 2.000000e-01] 49 50 -0.1414214 ## 3 ( 2.000000e-01, 3.000000e-01] 49 50 -0.1414214 ## 4 ( 3.000000e-01, 4.000000e-01] 50 50 0.0000000 ## 5 ( 4.000000e-01, 5.000000e-01] 51 50 0.1414214 ## 6 ( 5.000000e-01, 6.000000e-01] 51 50 0.1414214 ## 7 ( 6.000000e-01, 7.000000e-01] 49 50 -0.1414214 ## 8 ( 7.000000e-01, 8.000000e-01] 50 50 0.0000000 ## 9 ( 8.000000e-01, 9.000000e-01] 50 50 0.0000000 ## 10 ( 9.000000e-01, 9.980469e-01] 50 50 0.0000000 ## ## Pearson&#39;s Chi-squared test ## ## data: u ## X-squared = 0.12, df = 9, p-value = 1 Alternativamente, por ejemplo si solo se pretende aplicar el contraste, se podría emplear la función simres::freq.test() (fichero test.R) para este caso particular (ver Sección B.3.1). Como se muestra en la Figura 2.3 el histograma de la secuencia generada es muy plano (comparado con lo que cabría esperar de una muestra de tamaño 500 de una uniforme), y consecuentemente el \\(p\\)-valor del contraste chi-cuadrado es prácticamente 1, lo que indicaría que este generador no reproduce adecuadamente la variabilidad de una distribución uniforme. Otro contraste de bondad de ajuste muy conocido es el test de Kolmogorov-Smirnov, implementado en ks.test (ver Sección B.1.5). Este contraste de hipótesis compara la función de distribución bajo la hipótesis nula con la función de distribución empírica (ver Sección B.1.2), representadas en la Figura 2.4: # Distribución empírica curve(ecdf(u)(x), type = &quot;s&quot;, lwd = 2) curve(punif(x, 0, 1), add = TRUE) Figura 2.4: Comparación de la distribución empírica de la secuencia generada con la función de distribución uniforme. Podemos realizar el contraste con el siguiente código: # Test de Kolmogorov-Smirnov ks.test(u, &quot;punif&quot;, 0, 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: u ## D = 0.0033281, p-value = 1 ## alternative hypothesis: two-sided En la Sección B.1 se describen con más detalle estos contrastes de bondad de ajuste. Adicionalmente podríamos estudiar la aleatoriedad de los valores generados (ver Sección B.2), por ejemplo mediante un gráfico secuencial y el de dispersión retardado. plot(as.ts(u)) Figura 2.5: Gráfico secuencial de los valores generados. plot(u[-nsim],u[-1]) Figura 2.6: Gráfico de dispersión retardado de los valores generados. También podemos analizar las autocorrelaciones (las correlaciones de \\((u_{i},u_{i+k})\\), con \\(k=1,\\ldots,K\\)): acf(u) Figura 2.7: Autocorrelaciones de los valores generados. Por ejemplo, para contrastar si las diez primeras autocorrelaciones son nulas podríamos emplear el test de Ljung-Box: Box.test(u, lag = 10, type = &quot;Ljung&quot;) ## ## Box-Ljung test ## ## data: u ## X-squared = 22.533, df = 10, p-value = 0.01261 2.3.1 Repetición de contrastes Los contrastes se plantean habitualmente desde el punto de vista de la inferencia estadística: se realiza una prueba sobre la única muestra disponible. Si se realiza una única prueba, en las condiciones de \\(H_0\\) hay una probabilidad \\(\\alpha\\) de rechazarla. En simulación tiene mucho más sentido realizar un gran número de pruebas: La proporción de rechazos debería aproximarse al valor de \\(\\alpha\\) (se puede comprobar para distintos valores de \\(\\alpha\\)). La distribución del estadístico debería ajustarse a la teórica bajo \\(H_0\\) (se podría realizar un nuevo contraste de bondad de ajuste). Los \\(p\\)-valores obtenidos deberían ajustarse a una \\(\\mathcal{U}\\left(0,1\\right)\\) (se podría realizar también un contraste de bondad de ajuste). Este procedimiento es también el habitual para validar un método de contraste de hipótesis por simulación (ver Sección ??). Ejemplo 2.3 Continuando con el generador congruencial RANDU, podemos pensar en estudiar la uniformidad de los valores generados empleando repetidamente el test chi-cuadrado: # Valores iniciales set.rng(543210, &quot;lcg&quot;, a = 2^16 + 3, c = 0, m = 2^31) # Establecer semilla y parámetros # set.seed(543210) n &lt;- 500 nsim &lt;- 1000 estadistico &lt;- numeric(nsim) pvalor &lt;- numeric(nsim) # Realizar contrastes for(isim in 1:nsim) { u &lt;- rng(n) # Generar # u &lt;- runif(n) tmp &lt;- freq.test(u, nclass = 100) # tmp &lt;- chisq.cont.test(u, distribution = &quot;unif&quot;, nclass = 100, # output = FALSE, nestpar = 0, min = 0, max = 1) estadistico[isim] &lt;- tmp$statistic pvalor[isim] &lt;- tmp$p.value } Por ejemplo, podemos comparar la proporción de rechazos observados con los que cabría esperar con los niveles de significación habituales: { cat(&quot;Proporción de rechazos al 1% =&quot;, mean(pvalor &lt; 0.01), &quot;\\n&quot;) # sum(pvalor &lt; 0.01)/nsim cat(&quot;Proporción de rechazos al 5% =&quot;, mean(pvalor &lt; 0.05), &quot;\\n&quot;) # sum(pvalor &lt; 0.05)/nsim cat(&quot;Proporción de rechazos al 10% =&quot;, mean(pvalor &lt; 0.1), &quot;\\n&quot;) # sum(pvalor &lt; 0.1)/nsim } ## Proporción de rechazos al 1% = 0.014 ## Proporción de rechazos al 5% = 0.051 ## Proporción de rechazos al 10% = 0.112 Las proporciones de rechazo obtenidas deberían comportarse como una aproximación por simulación de los niveles teóricos. En este caso no se observa nada extraño, por lo que no habría motivos para sospechar de la uniformidad de los valores generados (aparentemente no hay problemas con la uniformidad de este generador). Adicionalmente, si queremos estudiar la proporción de rechazos (el tamaño del contraste) para los posibles valores de \\(\\alpha\\), podemos emplear la distribución empírica del \\(p\\)-valor (proporción de veces que resultó menor que un determinado valor): # Distribución empírica plot(ecdf(pvalor), do.points = FALSE, lwd = 2, xlab = &#39;Nivel de significación&#39;, ylab = &#39;Proporción de rechazos&#39;) abline(a = 0, b = 1, lty = 2) # curve(punif(x, 0, 1), add = TRUE) Figura 2.8: Proporción de rechazos con los distintos niveles de significación. También podemos estudiar la distribución del estadístico del contraste. En este caso, como la distribución bajo la hipótesis nula está implementada en R, podemos compararla fácilmente con la de los valores generados (debería ser una aproximación por simulación de la distribución teórica): # Histograma hist(estadistico, breaks = &quot;FD&quot;, freq = FALSE, main = &quot;&quot;) curve(dchisq(x, 99), add = TRUE) Figura 2.9: Distribución del estadístico del constraste. Además de la comparación gráfica, podríamos emplear un test de bondad de ajuste para contrastar si la distribución del estadístico es la teórica bajo la hipótesis nula: # Test chi-cuadrado (chi-cuadrado sobre chi-cuadrado) # chisq.cont.test(estadistico, distribution=&quot;chisq&quot;, nclass=20, nestpar=0, df=99) # Test de Kolmogorov-Smirnov ks.test(estadistico, &quot;pchisq&quot;, df = 99) ## ## One-sample Kolmogorov-Smirnov test ## ## data: estadistico ## D = 0.023499, p-value = 0.6388 ## alternative hypothesis: two-sided En este caso la distribución observada del estadístico es la que cabría esperar de una muestra de este tamaño de la distribución teórica, por tanto, según este criterio, aparentemente no habría problemas con la uniformidad de este generador (hay que recordar que estamos utilizando contrastes de hipótesis como herramienta para ver si hay algún problema con el generador, no tiene mucho sentido hablar de aceptar o rechazar una hipótesis). En lugar de estudiar la distribución del estadístico de contraste siempre podemos analizar la distribución del \\(p\\)-valor. Mientras que la distribución teórica del estadístico depende del contraste y puede ser complicada, la del \\(p\\)-valor es siempre una uniforme. # Histograma hist(pvalor, freq = FALSE, main = &quot;&quot;) abline(h=1) # curve(dunif(x,0,1), add=TRUE) Figura 2.10: Distribución del \\(p\\)-valor del constraste. # Test chi-cuadrado # chisq.cont.test(pvalor, distribution=&quot;unif&quot;, nclass=20, nestpar=0, min=0, max=1) # Test de Kolmogorov-Smirnov ks.test(pvalor, &quot;punif&quot;, min = 0, max = 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: pvalor ## D = 0.023499, p-value = 0.6388 ## alternative hypothesis: two-sided Como podemos observar, obtendríamos los mismos resultados que al analizar la distribución del estadístico. Alternativamente podríamos emplear la función rephtest() del paquete simres: set.rng(543210, &quot;lcg&quot;, a = 2^16 + 3, c = 0, m = 2^31) # res &lt;- rephtest(n = 30, test = chisq.cont.test, rand.gen = rng, # distribution = &quot;unif&quot;, output = FALSE, nestpar = 0) res &lt;- rephtest(n = 30, test = freq.test, rand.gen = rng) str(res) ## List of 2 ## $ statistics: num [1:1000] 0.2 0.6 6.2 0 1.4 0.8 0.8 7.2 2.6 0.6 ... ## $ p.values : num [1:1000] 0.905 0.741 0.045 1 0.497 ... ## - attr(*, &quot;class&quot;)= chr &quot;rhtest&quot; ## - attr(*, &quot;method&quot;)= chr &quot;Chi-squared test for given probabilities&quot; ## - attr(*, &quot;names.stat&quot;)= chr &quot;X-squared&quot; ## - attr(*, &quot;parameter&quot;)= Named num 2 ## ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; summary(res) ## Proportion of rejections: ## 1% 5% 10% 25% 50% ## 0.010 0.053 0.112 0.210 0.626 plot(res) 2.3.2 Baterías de contrastes Hay numerosos ejemplos de generadores que pasaron diferentes test de uniformidad y aleatoriedad pero que fallaron estrepitosamente al considerar nuevos contrastes diseñados específicamente para generadores aleatorios (ver Marsaglia et al., 1990). Por este motivo, el procedimiento habitual en la práctica es aplicar un número más o menos elevado de contrastes (de distinto tipo y difíciles de pasar, e.g. Marsaglia y Tsang, 2002), de forma que si el generador los pasa tendremos mayor confianza en que sus propiedades son las adecuadas. Este conjunto de pruebas es lo que se denomina batería de contrastes. Una de las primeras se introdujo en Knuth (1969) y de las más recientes podríamos destacar: Diehard tests (The Marsaglia Random Number CDROM, 1995): http://www.stat.fsu.edu/pub/diehard (versión archivada el 2016-01-25). Dieharder (Brown y Bauer, 2003): Dieharder Page, paquete RDieHarder. TestU01 (LEcuyer y Simard, 2007): http://simul.iro.umontreal.ca/testu01/tu01.html. NIST test suite (National Institute of Standards and Technology, USA, 2010): http://csrc.nist.gov/groups/ST/toolkit/rng. Para más detalles, ver por ejemplo8: Marsaglia, G. y Tsang, W.W. (2002). Some difficult-to-pass tests of randomness. Journal of Statistical Software, 7(3), 1-9. Demirhan, H. y Bitirim, N. (2016). CryptRndTest: an R package for testing the cryptographic randomness. The R Journal, 8(1), 233-247. Estas baterías de contrastes se suelen emplear si el generador va a ser utilizado en criptografía o si es muy importante la impredecibilidad (normalmente con generadores de números verdaderamente aleatorios por hardware). Si el objetivo es únicamente obtener resultados estadísticos (como en nuestro caso) no sería tan importante que el generador no superase alguno de estos test. También puede ser de interés el enlace Randomness Tests: A Literature Survey y la entidad certificadora (gratuita) en línea CAcert. "],["ejercicios-1.html", "2.4 Ejercicios", " 2.4 Ejercicios Ejercicio 2.1 (Método de los cuadrados medios) Uno de los primeros generadores utilizados fue el denominado método de los cuadrados medios propuesto por Von Neumann (1946). Con este procedimiento se generan números pseudoaleatorios de 4 dígitos de la siguiente forma: Se escoge un número de cuatro dígitos \\(x_0\\) (semilla). Se eleva al cuadrado (\\(x_0^2\\)) y se toman los cuatro dígitos centrales (\\(x_1\\)). Se genera el número pseudo-aleatorio como \\[u_1=\\frac{x_1}{10^{4}}.\\] Volver al paso ii y repetir el proceso. Para obtener los \\(k\\) (número par) dígitos centrales de \\(x_{i}^2\\) se puede utilizar que: \\[x_{i+1}=\\left\\lfloor \\left( x_{i}^2-\\left\\lfloor \\dfrac{x_{i}^2}{10^{(2k-\\frac{k}2)}}\\right\\rfloor 10^{(2k-\\frac{k}2)}\\right) /10^{\\frac{k}2}\\right\\rfloor\\] Este algoritmo está implementado en la función simres::rvng() (ver también simres::rng(); fichero rng.R): simres::rvng ## function(n, seed = as.numeric(Sys.time()), k = 4) { ## seed &lt;- seed %% 10^k ## aux &lt;- 10^(2*k-k/2) ## aux2 &lt;- 10^(k/2) ## u &lt;- numeric(n) ## for(i in 1:n) { ## z &lt;- seed^2 ## seed &lt;- trunc((z - trunc(z/aux)*aux)/aux2) ## u[i] &lt;- seed/10^k ## } ## # Almacenar semilla y parámetros ## assign(&quot;.rng&quot;, list(seed = seed, type = &quot;vm&quot;, parameters = list(k = k)), ## envir = globalenv()) ## # .rng &lt;&lt;- list(seed = seed, type = &quot;vm&quot;, parameters = list(k = k)) ## # Para continuar con semilla y parámetros: ## # with(.rng, rvng(n, seed, parameters$k)) ## # Devolver valores ## return(u) ## } ## &lt;bytecode: 0x000000003326ec80&gt; ## &lt;environment: namespace:simres&gt; Estudiar las características del generador de cuadrados medios a partir de una secuencia de 500 valores. Emplear únicamente métodos gráficos. Ejercicio 2.2 Considerando el generador congruencial multiplicativo de parámetros \\(a=7^{5}=16807\\), \\(c=0\\) y \\(m=2^{31}-1\\) (minimal standar de Park y Miller, 1988). ¿Se observan los mismos problemas que con el algoritmo RANDU al considerar las tripletas \\((x_{k},x_{k+1},x_{k+2})\\)? "],["resultados.html", "Capítulo 3 Análisis de resultados de simulación", " Capítulo 3 Análisis de resultados de simulación En este capítulo nos centraremos en la aproximación mediante simulación de la media teórica de un estadístico a partir de la media muestral de una secuencia de simulaciones de dicho estadístico. La aproximación de una probabilidad sería un caso particular considerando una variable de Bernoulli. En primer lugar se tratará el análisis de la convergencia y la precisión de la aproximación por simulación. Al final del capítulo se incluye una breve introducción a los problemas de estabilización y dependencia (con los que nos solemos encontrar en simulación dinámica y MCMC). "],["convergencia.html", "3.1 Convergencia", " 3.1 Convergencia Supongamos que estamos interesados en aproximar la media teórica \\(\\mu = E\\left( X\\right)\\) a partir de una secuencia i.i.d. \\(X_{1}\\), \\(X_{2}\\), \\(\\ldots\\), \\(X_{n}\\) obtenida mediante simulación, utilizando para ello la media muestral \\(\\bar{X}_{n}\\). Una justificación teórica de la validez de esta aproximación es la ley (débil9) de los grandes números: Teorema 3.1 (Ley débil de los grandes números; Khintchine, 1928) Si \\(X_{1}\\), \\(X_{2}\\), \\(\\ldots\\) es una secuencia de variables aleatorias independientes e idénticamente distribuidas con media finita \\(E\\left( X_{i}\\right) =\\mu\\) (i.e. \\(E\\left( \\left\\vert X_{i} \\right\\vert \\right) &lt; \\infty\\)) entonces \\(\\overline{X}_{n}=\\left( X_{1}+\\ldots +X_{n}\\right) /n\\) converge en probabilidad a \\(\\mu\\): \\[\\overline{X}_{n}\\ \\overset{p}{ \\longrightarrow }\\ \\mu\\] Es decir, para cualquier \\(\\varepsilon &gt;0\\): \\[\\lim\\limits_{n\\rightarrow \\infty }P\\left( \\left\\vert \\overline{X}_{n}-\\mu \\right\\vert &lt;\\varepsilon \\right) = 1.\\] Ejemplo 3.1 (Aproximación de una probabilidad) Simulamos una variable aleatoria \\(X\\) con distribución de Bernoulli de parámetro \\(p=0.5\\): p &lt;- 0.5 set.seed(1) nsim &lt;- 10000 # nsim &lt;- 100 rx &lt;- runif(nsim) &lt; p # rbinom(nsim, size = 1, prob = p) La aproximación por simulación de \\(E(X) = p\\) será: mean(rx) ## [1] 0.5047 Podemos generar un gráfico con la evolución de la aproximación: plot(cumsum(rx)/1:nsim, type = &quot;l&quot;, lwd = 2, xlab = &quot;Número de generaciones&quot;, ylab = &quot;Proporción muestral&quot;, ylim = c(0, 1)) abline(h = mean(rx), lty = 2) # valor teórico abline(h = p) Figura 3.1: Aproximación de la proporción en función del número de generaciones. 3.1.1 Detección de problemas de convergencia En la ley débil se requiere como condición suficiente que \\(E\\left( \\left\\vert X_{i} \\right\\vert \\right) &lt; \\infty\\), en caso contrario la media muestral puede no converger a una constante. Un ejemplo conocido es la distribución de Cauchy: set.seed(1) nsim &lt;- 10000 rx &lt;- rcauchy(nsim) # rx &lt;- rt(nsim, df = 2) plot(cumsum(rx)/1:nsim, type = &quot;l&quot;, lwd = 2, xlab = &quot;Número de generaciones&quot;, ylab = &quot;Media muestral&quot;) Figura 3.2: Evolución de la media muestral de una distribución de Cauchy en función del número de generaciones. Como conclusión, para detectar problemas de convergencia es especialmente recomendable representar la evolución de la aproximación de la característica de interés (sobre el número de generaciones), además de realizar otros análisis descriptivos de las simulaciones. Por ejemplo, en este caso podemos observar los valores que producen estos saltos mediante un gráfico de cajas: boxplot(rx) Figura 3.3: Gráfico de cajas de 10000 generaciones de una distribución de Cauchy. 3.1.2 Precisión Una forma de medir la precisión de un estimador es utilizando su varianza, o también su desviación típica que recibe el nombre de error estándar. En el caso de la media muestral \\(\\overline{X}_{n}\\), suponiendo además que \\(Var\\left( X_{i}\\right) = \\sigma^{2}&lt;\\infty\\), un estimador insesgado de \\(Var\\left( \\overline{X}_{n}\\right) =\\sigma ^{2}/n\\) es: \\[\\widehat{Var}\\left( \\overline{X}_{n}\\right) = \\frac{\\widehat{S}_n^{2}}{n}\\] donde: \\[\\widehat{S}_{n}^{2}=\\dfrac{1}{n-1}\\sum\\limits_{i=1}^{n}\\left( X_{i}- \\overline{X}\\right) ^{2}\\] es la cuasi-varianza muestral10. Los valores obtenidos servirían como medidas básicas de la precisión de la aproximación, aunque su principal aplicación es la construcción de intervalos de confianza. Si se endurecen las suposiciones de la ley débil de los grandes números (Teorema 3.1), exigiendo la existencia de varianza finita (\\(E\\left( X_{i}^2 \\right) &lt; \\infty\\)), se dispone de un resultado más preciso sobre las variaciones de la aproximación por simulación en torno al límite teórico. Teorema 3.2 (central del límite, CLT) Si \\(X_{1}\\), \\(X_{2}\\), \\(\\ldots\\) es una secuencia de variables aleatorias independientes e idénticamente distribuidas con \\(E\\left( X_{i}\\right) =\\mu\\) y \\(Var\\left( X_{i}\\right) = \\sigma ^{2}&lt;\\infty\\), entonces la media muestral estandarizada converge en distribución a una normal estándar: \\[Z_{n}=\\frac{\\overline{X}_{n}-\\mu }{\\frac{\\sigma }{\\sqrt{n}}} \\overset{d}{ \\longrightarrow } N(0,1).\\] Es decir, \\(\\lim\\limits_{n\\rightarrow \\infty }F_{Z_{n}}(z)=\\Phi (z)\\). Por tanto, un intervalo de confianza asintótico para \\(\\mu\\) es: \\[IC_{1-\\alpha }(\\mu ) = \\left( \\overline{X}_{n} - z_{1-\\alpha /2}\\dfrac{\\widehat{S}_{n}}{\\sqrt{n}},\\ \\overline{X}_n+z_{1-\\alpha /2}\\dfrac{\\widehat{S}_{n}}{\\sqrt{n}} \\right).\\] También podemos utilizar el error máximo (con nivel de confianza \\(1-\\alpha\\)) de la estimación \\(z_{1-\\alpha /2}\\dfrac{\\widehat{S}_{n}}{\\sqrt{n}}\\) como medida de su precisión. El CLT es un resultado asintótico. En la práctica la convergencia es aleatoria, ya que depende de la muestra simulada (las generaciones pseudoaleatorias). Además, la convergencia puede considerarse lenta, en el sentido de que, por ejemplo, para doblar la precisión (disminuir el error a la mitad), necesitaríamos un número de generaciones cuatro veces mayor (Ver Sección 3.2). Pero una ventaja es que este error no depende del número de dimensiones (en el caso multidimensional puede ser mucho más rápida que otras alternativas numéricas; ver Apéndice C). Ejemplo 3.2 (Aproximación de la media de una distribución normal) xsd &lt;- 1 xmed &lt;- 0 set.seed(1) nsim &lt;- 1000 rx &lt;- rnorm(nsim, xmed, xsd) La aproximación por simulación de la media será: mean(rx) ## [1] -0.01164814 Como medida de la precisión de la aproximación podemos utilizar el error máximo: 2*sd(rx)/sqrt(nsim) ## [1] 0.06545382 (es habitual emplear 2 en lugar de 1.96, lo que se correspondería con \\(1 - \\alpha = 0.9545\\) en el caso de normalidad). Podemos añadir también los correspondientes intervalos de confianza al gráfico de convergencia: n &lt;- 1:nsim est &lt;- cumsum(rx)/n # (cumsum(rx^2) - n*est^2)/(n-1) # Varianzas muestrales esterr &lt;- sqrt((cumsum(rx^2)/n - est^2)/(n-1)) # Errores estándar plot(est, type = &quot;l&quot;, lwd = 2, xlab = &quot;Número de generaciones&quot;, ylab = &quot;Media y rango de error&quot;, ylim = c(-1, 1)) abline(h = est[nsim], lty=2) lines(est + 2*esterr, lty=3) lines(est - 2*esterr, lty=3) abline(h = xmed) Figura 3.4: Gráfico de convergencia incluyendo el error de la aproximación. La ley fuerte establece la convergencia casi segura. Esto sería también válido para el caso de una proporción, donde \\(E(X) = p\\), \\(Var(X) = p(1-p)\\) y \\(\\hat{p}_{n} = \\overline{X}_{n}\\), obteniéndose que: \\[\\widehat{Var}\\left( \\hat{p}_{n}\\right) = \\frac{\\widehat{S}_n^{2}}{n} = \\frac{\\hat{p}_{n}(1-\\hat{p}_{n})}{n-1},\\] aunque lo más habitual es emplear: \\[\\frac{S_n^{2}}{n} = \\frac{\\hat{p}_{n}(1-\\hat{p}_{n})}{n},\\] donde \\(S_n^{2}\\) es la varianza muestral. Hay que tener en cuenta que en simulación el número de generaciones es normalmente grande y en la práctica no va haber diferencias apreciables. "],["num-gen.html", "3.2 Determinación del número de generaciones", " 3.2 Determinación del número de generaciones Lo más habitual es seleccionar un valor de \\(n\\) del orden de varias centenas o millares. En los casos en los que la simulación se utiliza para aproximar una característica central de la distribución (como una media) puede bastar un número de generaciones del orden de \\(n = 100, 200, 500\\). Sin embargo, en otros casos, por ejemplo para aproximar el p-valor de un contraste de hipótesis o construir intervalos de confianza, pueden ser necesarios valores del tipo \\(n = 1000, 2000, 5000, 10000\\). En muchas ocasiones puede interesar obtener una aproximación con un nivel de precisión fijado. Para una precisión absoluta \\(\\varepsilon\\), se trata de determinar \\(n\\) de forma que: \\[z_{1-\\alpha /2}\\dfrac{\\widehat{S}_{n}}{\\sqrt{n}}&lt;\\varepsilon\\] Un algoritmo podría ser el siguiente: Hacer \\(j=0\\) y fijar un tamaño inicial \\(n_{0}\\) (e.g. 30 ó 60). Generar \\(\\left\\{ X_{i}\\right\\} _{i=1}^{n_{0}}\\) y calcular \\(\\overline{X}_{n_0}\\) y \\(\\widehat{S}_{n_{0}}\\). Mientras \\(\\left. z_{1-\\alpha /2}\\widehat{S}_{n_j}\\right/ \\sqrt{n_{j}}&gt;\\varepsilon\\) hacer: 3.1. \\(j=j+1\\). 3.2. \\(n_{j}=\\left\\lceil \\left( \\left. z_{1-\\alpha /2}\\widehat{S} _{n_{j-1}}\\right/ \\varepsilon \\right)^{2}\\right\\rceil\\). 3.3. Generar \\(\\left\\{ X_{i}\\right\\}_{i=n_{j-1}+1}^{n_j}\\) y calcular \\(\\overline{X}_{n_j}\\) y \\(\\widehat{S}_{n_j}\\). Devolver \\(\\overline{X}_{n_j}\\) y \\(\\left. z_{1-\\alpha /2}\\widehat{S}_{n_j}\\right/ \\sqrt{n_{j}}\\). Para una precisión relativa \\(\\varepsilon \\left\\vert \\mu \\right\\vert\\) se procede análogamente de forma que: \\[z_{1-\\alpha /2}\\dfrac{\\widehat{S}_{n}}{\\sqrt{n}}&lt;\\varepsilon \\left\\vert \\overline{X}_{n}\\right\\vert .\\] "],["el-problema-de-la-dependencia.html", "3.3 El problema de la dependencia", " 3.3 El problema de la dependencia En el caso de dependencia, bajo condiciones muy generales se verifica la ley débil de los grandes números. Sin embargo, la estimación de la precisión se complica: \\[Var\\left( \\overline{X}\\right) =\\frac{1}{n^{2}}\\left( \\sum_{i=1}^{n}Var\\left( X_{i} \\right) + 2\\sum_{i&lt;j}Cov\\left( X_{i},X_{j}\\right) \\right).\\] Ejemplo 3.3 (aproximación de una proporción bajo dependencia, cadena de Markov) Supongamos que en A Coruña llueve de media uno de cada tres días al año, y que la probabilidad de que un día llueva solo depende de lo que ocurrió el día anterior, siendo 0.94 si el día anterior llovió y 0.03 si no. Podemos generar valores de la variable indicadora de día lluvioso con el siguiente código: # Variable dicotómica 0/1 (FALSE/TRUE) set.seed(1) nsim &lt;- 10000 alpha &lt;- 0.03 # prob de cambio si seco beta &lt;- 0.06 # prob de cambio si lluvia rx &lt;- logical(nsim) # x == &quot;llueve&quot; rx[1] &lt;- FALSE # El primer día no llueve for (i in 2:nsim) rx[i] &lt;- if (rx[i-1]) runif(1) &gt; beta else runif(1) &lt; alpha Si generamos el gráfico de convergencia asumiendo independencia: n &lt;- 1:nsim est &lt;- cumsum(rx)/n esterr &lt;- sqrt(est*(1-est)/(n-1)) # OJO! Supone independencia plot(est, type=&quot;l&quot;, lwd=2, ylab=&quot;Probabilidad&quot;, xlab=&quot;Número de simulaciones&quot;, ylim=c(0,0.6)) abline(h = est[nsim], lty=2) lines(est + 2*esterr, lty=2) lines(est - 2*esterr, lty=2) abline(h = 1/3, col=&quot;darkgray&quot;) # Probabilidad teórica Figura 3.5: Gráfico de convergencia incluyendo el error de la aproximación (calculado asumiendo independencia). La probabilidad teórica, obtenida empleando resultados de cadenas de Markov, es \\(p = 1/3\\) y la aproximación de la proporción sería correcta (es consistente): est[nsim] ## [1] 0.3038 Sin embargo, al ser datos dependientes, la aproximación anterior del error estándar no es adecuada: esterr[nsim] ## [1] 0.004599203 En este caso al haber dependencia positiva se produce una subestimación del verdadero error estándar. Podemos generar el gráfico de autocorrelaciones: acf(as.numeric(rx)) Figura 3.6: Correlograma de la secuencia indicadora de días de lluvia. El gráfico anterior sugiere que si solo tomamos 1 de cada 25 valores podría ser razonable asumir independencia. lag &lt;- 24 xlag &lt;- c(rep(FALSE, lag), TRUE) rxi &lt;- rx[xlag] acf(as.numeric(rxi)) Figura 3.7: Correlograma de la subsecuencia de días de lluvia obtenida seleccionando uno de cada 25 valores. nrxi &lt;- length(rxi) n &lt;- 1:nrxi est &lt;- cumsum(rxi)/n esterr &lt;- sqrt(est*(1-est)/(n-1)) plot(est, type=&quot;l&quot;, lwd=2, ylab=&quot;Probabilidad&quot;, xlab=paste(&quot;Número de simulaciones /&quot;, lag + 1), ylim=c(0,0.6)) abline(h = est[length(rxi)], lty=2) lines(est + 2*esterr, lty=2) # Supone independencia lines(est - 2*esterr, lty=2) abline(h = 1/3, col=&quot;darkgray&quot;) # Prob. teor. cadenas Markov Figura 3.8: Gráfico de convergencia de la aproximación de la probabilidad a partir de la subsecuencia de días de lluvia (calculando el error de aproximación asumiendo independencia). Esta forma de proceder podría ser adecuada para tratar de aproximar la precisión: esterr[nrxi] ## [1] 0.02277402 pero no sería eficiente para aproximar la media. Siempre será preferible emplear todas las observaciones. Por ejemplo, se podría pensar en considerar las medias de grupos de 24 valores consecutivos y suponer que hay independencia entre ellas: rxm &lt;- rowMeans(matrix(rx, ncol = lag, byrow = TRUE)) nrxm &lt;- length(rxm) n &lt;- 1:nrxm est &lt;- cumsum(rxm)/n esterr &lt;- sqrt((cumsum(rxm^2)/n - est^2)/(n-1)) # Errores estándar plot(est, type=&quot;l&quot;, lwd=2, ylab=&quot;Probabilidad&quot;, xlab=paste(&quot;Número de simulaciones /&quot;, lag + 1), ylim=c(0,0.6)) abline(h = est[length(rxm)], lty=2) lines(est + 2*esterr, lty=2) # OJO! Supone independencia lines(est - 2*esterr, lty=2) abline(h = 1/3, col=&quot;darkgray&quot;) # Prob. teor. cadenas Markov Figura 3.9: Gráfico de convergencia de las medias por lotes. Esta es la idea del método de medias por lotes (batch means; macro-micro replicaciones) para la estimación de la varianza. En el ejemplo anterior se calcula el error estándar de la aproximación por simulación de la proporción: esterr[nrxm] ## [1] 0.01582248 pero si el objetivo es la aproximación de la varianza (de la variable y no de las medias por lotes), habrá que reescalarlo adecuadamente. Supongamos que la correlación entre \\(X_i\\) y \\(X_{i+k}\\) es aproximadamente nula, y consideramos las subsecuencias (lotes) \\((X_{t+1},X_{t+2},\\ldots,X_{t+k})\\) con \\(t=(j-1)k\\), \\(j=1,\\ldots,m\\) y \\(n = mk\\). Entonces: \\[\\begin{aligned} Var \\left(\\bar X \\right) &amp;= Var \\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right) = Var \\left( \\frac{1}{m}\\sum_{j=1}^m \\left(\\frac{1}{k} \\sum_{t=(i-1)k + 1}^{ik} X_t\\right) \\right) \\\\ &amp;\\approx \\frac{1}{m^2} \\sum_{j=1}^m Var \\left(\\frac{1}{k} \\sum_{t=(i-1)k + 1}^{ik} X_t\\right) \\approx \\frac{1}{m} Var \\left(\\bar{X}_k \\right) \\end{aligned}\\] donde \\(\\bar{X}_k\\) es la media de una subsecuencia de longitud \\(k\\). var.aprox &lt;- nsim * esterr[length(rxm)]^2 var.aprox ## [1] 2.50351 Obtenida asumiendo independencia entre las medias por lotes, y que será una mejor aproximación que asumir independencia entre las generaciones de la variable: var(rx) ## [1] 0.2115267 Alternativamente se podría recurrir a la generación de múltiples secuencias independientes entre sí: # Variable dicotómica 0/1 (FALSE/TRUE) set.seed(1) nsim &lt;- 1000 nsec &lt;- 10 alpha &lt;- 0.03 # prob de cambio si seco beta &lt;- 0.06 # prob de cambio si lluvia rxm &lt;- matrix(FALSE, nrow = nsec, ncol= nsim) for (i in 1:nsec) { # rxm[i, 1] &lt;- FALSE # El primer día no llueve # rxm[i, 1] &lt;- runif(1) &lt; 1/2 # El primer día llueve con probabilidad 1/2 rxm[i, 1] &lt;- runif(1) &lt; 1/3 # El primer día llueve con probabilidad 1/3 (ideal) for (j in 2:nsim) rxm[i, j] &lt;- if (rxm[i, j-1]) runif(1) &gt; beta else runif(1) &lt; alpha } La idea sería considerar las medias de las series como una muestra independiente de una nueva variable y estimar su varianza de la forma habitual: # Media de cada secuencia n &lt;- 1:nsim est &lt;- apply(rxm, 1, function(x) cumsum(x)/n) matplot(n, est, type = &#39;l&#39;, lty = 3, col = &quot;lightgray&quot;, ylab=&quot;Probabilidad&quot;, xlab=&quot;Número de simulaciones&quot;) # Aproximación mest &lt;- apply(est, 1, mean) lines(mest, lwd = 2) abline(h = mest[nsim], lty = 2) # Precisión mesterr &lt;- apply(est, 1, sd)/sqrt(nsec) lines(mest + 2*mesterr, lty = 2) lines(mest - 2*mesterr, lty = 2) # Prob. teor. cadenas Markov abline(h = 1/3, col=&quot;darkgray&quot;) Figura 3.10: Gráfico de convergencia de la media de 10 secuencias generadas de forma independiente. # Aproximación final mest[nsim] # mean(rxm) ## [1] 0.3089 # Error estándar mesterr[nsim] ## [1] 0.02403491 Trataremos este tipo de problemas en la diagnosis de algoritmos de simulación Monte Carlo de Cadenas de Markov (MCMC). Aparecen también en la simulación dinámica (por eventos o cuantos). 3.3.1 Periodo de calentamiento En el caso de simulación de datos dependientes (simulación dinámica) pueden aparecer problemas de estabilización. Puede ocurrir que el sistema evolucione lentamente en el tiempo hasta alcanzar su distribución estacionaria, siendo muy sensible a las condiciones iniciales con las que se comienzó la simulación. En tal caso resulta conveniente ignorar los resultados obtenidos durante un cierto período inicial de tiempo (denominado período de calentamiento o estabilización), cuyo único objeto es conseguir que se estabilice la distribución de probabilidad. Como ejemplo comparamos la simulación del Ejemplo 3.3 con la obtenida considerando como punto de partida un día lluvioso (con una semilla distinta para evitar dependencia). set.seed(2) nsim &lt;- 10000 rx2 &lt;- logical(nsim) rx2[1] &lt;- TRUE # El primer día llueve for (i in 2:nsim) rx2[i] &lt;- if (rx2[i-1]) runif(1) &gt; beta else runif(1) &lt; alpha n &lt;- 1:nsim est &lt;- cumsum(rx)/n est2 &lt;- cumsum(rx2)/n plot(est, type=&quot;l&quot;, ylab=&quot;Probabilidad&quot;, xlab=&quot;Número de simulaciones&quot;, ylim=c(0,0.6)) lines(est2, lty = 2) # Ejemplo periodo calentamiento nburn = 2000 abline(v = 2000, lty = 3) # Prob. teor. cadenas Markov abline(h = 1/3, col=&quot;darkgray&quot;) En estos casos puede ser recomendable ignorar los primeros valores generados (por ejemplo los primeros 2000) y recalcular los estadísticos deseados. También trataremos este tipo de problemas en la diagnosis de algoritmos MCMC. Ejemplo 3.4 (simulación de un proceso autorregresivo, serie de tiempo) \\[X_t = \\mu + \\rho * (X_{t-1} - \\mu) + \\varepsilon_t\\] Podemos tener en cuenta que en este caso la varianza es: \\[\\textrm{var}(X_t)=\\operatorname{E}(X_t^2)-\\mu^2=\\frac{\\sigma_\\varepsilon^2}{1-\\rho^2}.\\] Establecemos los parámetros: nsim &lt;- 200 # Numero de simulaciones xmed &lt;- 0 # Media rho &lt;- 0.5 # Coeficiente AR nburn &lt;- 10 # Periodo de calentamiento (burn-in) Se podría fijar la varianza del error: evar &lt;- 1 # Varianza de la respuesta xvar &lt;- evar / (1 - rho^2) pero la recomendación sería fijar la varianza de la respuesta: xvar &lt;- 1 # Varianza del error evar &lt;- xvar*(1 - rho^2) Para simular la serie, al ser un \\(AR(1)\\), normalmente simularíamos el primer valor rx[1] &lt;- rnorm(1, mean = xmed, sd = sqrt(xvar)) o lo fijamos a la media (en este caso nos alejamos un poco de la distribución estacionaria, para que el periodo de calentamiento sea mayor). Después generamos los siguientes valores de forma recursiva: set.seed(1) x &lt;- numeric(nsim + nburn) # Establecer el primer valor x[1] &lt;- -10 # Simular el resto de la secuencia for (i in 2:length(x)) x[i] &lt;- xmed + rho*(x[i-1] - xmed) + rnorm(1, sd=sqrt(evar)) x &lt;- as.ts(x) plot(x) abline(v = nburn, lty = 2) Figura 3.11: Ejemplo de una simulación de una serie de tiempo autorregresiva. y eliminamos el periodo de calentamiento: rx &lt;- x[-seq_len(nburn)] Para simular una serie de tiempo en R se puede emplear la función arima.sim del paquete base stats. En este caso el periodo de calentamiento se establece mediante el parámetro n.start (que se fija automáticamente a un valor adecuado). Por ejemplo, podemos generar este serie autoregressiva con: rx2 &lt;- arima.sim(list(order = c(1,0,0), ar = rho), n = nsim, n.start = nburn, sd = sqrt(evar)) La recomendación es fijar la varianza de las series simuladas si se quieren comparar resultados considerando distintos parámetros de dependencia. "],["observaciones.html", "3.4 Observaciones", " 3.4 Observaciones Como comentarios finales, podríamos añadir que: En el caso de que la característica de interés de la distribución de \\(X\\) no sea la media, los resultados anteriores no serían en principio aplicables. Incluso en el caso de la media, con el CLT obtenemos intervalos de confianza puntuales, que no habría que confundir con bandas de confianza (es muy probable que no contengan el verdadero valor del parámetro en todo el rango). En muchos casos (por ejemplo, cuando la generación de múltiples secuencias de simulación supone un coste computacional importante), puede ser preferible emplear un método de remuestreo para aproximar la precisión de la aproximación (ver Sección ??). "],["sim-con.html", "Capítulo 4 Simulación de variables continuas", " Capítulo 4 Simulación de variables continuas En este capítulo se expondrán métodos generales para simular distribuciones continuas: el método de inversión y los basados en aceptación-rechazo. En todos los casos como punto de partida es necesario disponer de un método de generación de números pseudoaleatorios uniformes en \\((0,1)\\). "],["método-de-inversión.html", "4.1 Método de inversión", " 4.1 Método de inversión En general sería el método preferible para la simulación de una variable continua (siempre que se disponga de la función cuantil). Está basado en los siguientes resultados: Si \\(X\\) es una variable aleatoria con función de distribución \\(F\\) continua y estrictamente monótona (invertible), entonces: \\[U = F\\left( X \\right) \\sim \\mathcal{U}(0, 1)\\] ya que: \\[G\\left( u \\right) = P\\left( Y \\leq u \\right) = P\\left( F(X) \\leq u \\right) \\\\ = P\\left( X \\leq F^{-1}(u) \\right) = F\\left( F^{-1}(u) \\right) = u\\] El recíproco también es cierto, si \\(U \\sim \\mathcal{U}(0, 1)\\) entonces: \\[F^{-1}\\left( U \\right) \\sim X\\] A partir de este resultado se deduce el siguiente algoritmo genérico para simular una variable continua con función de distribución \\(F\\) invertible: Algoritmo 4.1 (Método de inversión) Generar \\(U \\sim \\mathcal{U}(0, 1)\\). Devolver \\(X = F^{-1}\\left( U \\right)\\). Ejemplo 4.1 (simulación de una distribución exponencial) La distribución exponencial \\(\\exp \\left( \\lambda \\right)\\) de parámetro \\(\\lambda&gt;0\\) tiene como función de densidad \\(f(x) =\\lambda e^{-\\lambda x}\\), si \\(x\\geq 0\\), y como función de distribución: \\[F(x)=\\left\\{ \\begin{array}{ll} 1-e^{-\\lambda x} &amp; \\text{si } x \\ge 0 \\\\ 0 &amp; \\text{si } x &lt; 0\\\\ \\end{array} \\right.\\] Teniendo en cuenta que: \\[1-e^{-\\lambda x}=u \\Leftrightarrow x=-\\frac{\\ln \\left( 1-u\\right) }{ \\lambda }\\] el algoritmo para simular esta variable mediante el método de inversión es: Generar \\(U \\sim \\mathcal{U}(0, 1)\\). Devolver \\(X=-\\dfrac{\\ln \\left( 1-U\\right) }{\\lambda }\\). En el último paso podemos emplear directamente \\(U\\) en lugar de \\(1-U\\), ya que \\(1 - U \\sim \\mathcal{U}(0, 1)\\). Esta última expresión para acelerar los cálculos es la que denominaremos forma simplificada. Figura 4.1: Ilustración de la simulación de una distribución exponencial por el método de inversión. El código para implementar este algoritmo en R podría ser el siguiente: tini &lt;- proc.time() lambda &lt;- 2 nsim &lt;- 10^5 set.seed(1) U &lt;- runif(nsim) X &lt;- -log(U)/lambda # -log(1-U)/lambda tiempo &lt;- proc.time() - tini tiempo ## user system elapsed ## 0.03 0.02 0.04 hist(X, breaks = &quot;FD&quot;, freq = FALSE, main = &quot;&quot;, xlim = c(0, 5), ylim = c(0, 2.5)) curve(dexp(x, lambda), lwd = 2, add = TRUE) Figura 4.2: Distribución de los valores generados de una exponencial mediante el método de inversión. Como se observa en la Figura 4.2 se trata de un método exacto (si está bien implementado) y la distribución de los valores generados se aproxima a la distribución teórica como cabría esperar con una muestra de ese tamaño. 4.1.1 Algunas distribuciones que pueden simularse por el método de inversión A continuación se incluyen algunas distribuciones que se pueden simular fácilmente mediante el método de inversión. Se adjunta una forma simplificada del método que tiene por objeto evitar cálculos innecesarios (tal y como se hizo en el ejemplo de la exponencial). Nombre Densidad \\(F(x)\\) \\(F^{-1}\\left( U\\right)\\) Forma simplificada \\(\\exp\\left( \\lambda\\right)\\) (\\(\\lambda&gt;0\\)) \\(\\lambda e^{-\\lambda x}\\), si \\(x\\geq0\\) \\(1-e^{-\\lambda x}\\) \\(-\\dfrac{\\ln\\left( 1-U\\right) }\\lambda\\) \\(-\\dfrac{\\ln U}\\lambda\\) Cauchy \\(\\dfrac1{\\pi\\left( 1+x^{2}\\right)}\\) \\(\\dfrac12+\\dfrac{\\arctan x}\\pi\\) \\(\\tan\\left( \\pi\\left( U-\\dfrac12\\right) \\right)\\) \\(\\tan\\pi U\\) Triangular en \\(\\left( 0,a\\right)\\) \\(\\dfrac2a\\left( 1-\\dfrac xa\\right)\\), si \\(0\\leq x\\leq a\\) \\(\\dfrac2a\\left(x-\\dfrac{x^{2}}{2a}\\right)\\) \\(a\\left( 1-\\sqrt{1-U}\\right)\\) \\(a\\left( 1-\\sqrt{U}\\right)\\) Pareto (\\(a,b&gt;0\\)) \\(\\dfrac{ab^{a}}{x^{a+1}}\\), si \\(x\\geq b\\) \\(1-\\left( \\dfrac bx\\right)^{a}\\) \\(\\dfrac b{\\left( 1-U\\right)^{1/a}}\\) \\(\\dfrac b{U^{1/a}}\\) Weibull (\\(\\lambda,\\alpha&gt;0\\)) \\(\\alpha\\lambda^{\\alpha}x^{\\alpha-1}e^{-\\left( \\lambda x\\right) ^{\\alpha}}\\), si \\(x\\geq0\\) \\(1-e^{-\\left( \\lambda x\\right)^{\\alpha}}\\) \\(\\dfrac{\\left( -\\ln\\left(1-U\\right) \\right)^{1/\\alpha}}\\lambda\\) \\(\\dfrac{\\left( -\\ln U\\right)^{1/\\alpha}}\\lambda\\) Ejercicio 4.1 (distribución doble exponencial) La distribución doble exponencial (o distribución de Laplace) de parámetro \\(\\lambda\\) tiene función de densidad: \\[f(x) =\\frac{\\lambda}{2}e^{-\\lambda\\left\\vert x\\right\\vert }\\text{, }x\\in\\mathbb{R}\\] y función de distribución: \\[F(x) =\\int_{-\\infty}^{x}f\\left( t\\right) dt=\\left\\{ \\begin{array}{ll} \\frac{1}{2}e^{\\lambda x} &amp; \\text{si } x&lt;0\\\\ 1-\\frac{1}{2}e^{-\\lambda x} &amp; \\text{si } x\\geq0 \\end{array} \\ \\right.\\] Escribir una función que permita generar, por el método de inversión, una muestra de \\(n\\) observaciones de esta distribución. ddexp &lt;- function(x, lambda = 1){ # Densidad doble exponencial lambda*exp(-lambda*abs(x))/2 } rdexp &lt;- function(lambda = 1){ # Simulación por inversión # Doble exponencial U &lt;- runif(1) if (U&lt;0.5) { return(log(2*U)/lambda) } else { return(-log(2*(1-U))/lambda) } } rdexpn &lt;- function(n = 1000, lambda = 1) { # Simulación n valores de doble exponencial x &lt;- numeric(n) for(i in 1:n) x[i]&lt;-rdexp(lambda) return(x) } Generar \\(10^{4}\\) valores de la distribución doble exponencial de parámetro \\(\\lambda=2\\) y obtener el tiempo de CPU que tarda en generar la secuencia. set.seed(54321) system.time(x &lt;- rdexpn(10^4, 2)) ## user system elapsed ## 0.09 0.00 0.09 Representar el histograma y compararlo con la densidad teórica. hist(x, breaks = &quot;FD&quot;, freq = FALSE, main=&quot;&quot;) # lines(density(x), col = &#39;blue&#39;) curve(ddexp(x, 2), add = TRUE) Figura 4.3: Distribución de los valores generados de una doble exponencial mediante el método de inversión. Como se trata de un método exacto de simulación, si está bien implementado, la distribución de los valores generados debería comportarse como una muestra genuina de la distribución objetivo. Nota: Esta distribución también se puede generar fácilmente simulando una distribución exponencial y asignando un signo positivo o negativo con equiprobabilidad (ver Ejemplo 4.6). 4.1.2 Ventajas e inconvenientes La principal ventaja de este método es que, en general, sería aplicable a cualquier distribución continua (como se muestra en la Sección ??, se puede extender al caso de que la función de distribución no sea invertible, incluyendo distribuciones discretas). Uno de los principales problemas es que puede no ser posible encontrar una expresión explícita para \\(F^{-1}\\left( u\\right)\\) (en ocasiones, como en el caso de la distribución normal, ni siquiera se dispone de una expresión explícita para la función de distribución). Además, aún disponiendo de una expresión explícita para \\(F^{-1}\\left( u\\right)\\), su evaluación directa puede requerir de mucho tiempo de computación. Como alternativa a estos inconvenientes se podrían emplear métodos numéricos para resolver \\(F(x) - u = 0\\) de forma aproximada, aunque habría que resolver numéricamente esta ecuación para cada valor aleatorio que se desea generar. Otra posibilidad, en principio preferible, sería emplear una aproximación a \\(F^{-1}\\left( u\\right)\\), dando lugar al método de inversión aproximada (como se indicó en la Sección 1.3.1, R emplea por defecto este método para la generación de la distribución normal). 4.1.3 Inversión aproximada En muchos casos en los que no se puede emplear la expresión exacta de la función cuantil \\(F^{-1}\\left( u\\right)\\), se dispone de una aproximación suficientemente buena que se puede emplear en el algoritmo anterior (se obtendrían simulaciones con una distribución aproximada a la deseada). Por ejemplo, para aproximar la función cuantil de la normal estándar, Odeh y Evans (1974) consideraron la siguiente función auxiliar11: \\[ g\\left( v\\right) =\\sqrt{-2\\ln v}\\frac{A\\left( \\sqrt{-2\\ln v}\\right) }{B\\left( \\sqrt{-2\\ln v}\\right) },\\] siendo \\(A(x) =\\sum_{i=0}^{4}a_{i}x^{i}\\) y \\(B(x) =\\sum_{i=0}^{4}b_{i}x^{i}\\) con: \\[\\begin{array}{ll} a_{0}=-0.322232431088 &amp; b_{0}=0.0993484626060 \\\\ a_{1}=-1 &amp; b_{1}=0.588581570495 \\\\ a_{2}=-0.342242088547 &amp; b_{2}=0.531103462366 \\\\ a_{3}=-0.0204231210245 &amp; b_{3}=0.103537752850 \\\\ a_{4}=-0.0000453642210148 &amp; b_{4}=0.0038560700634 \\end{array}\\] La aproximación consiste en utilizar \\(g\\left( 1-u\\right)\\) en lugar de \\(F^{-1}\\left( u\\right)\\) para los valores de \\(u\\in[10^{-20},\\frac12]\\) y \\(-g\\left( u\\right)\\) si \\(u\\in[\\frac12,1-10^{-20}]\\). Para \\(u\\notin [10^{-20},1-10^{-20}]\\) (que sólo ocurre con una probabilidad de \\(2\\cdot10^{-20}\\)) la aproximación no es recomendable. Algoritmo 4.2 (de Odeh y Evans) Generar \\(U \\sim U(0, 1)\\). Si \\(U&lt;10^{-20}\\) ó \\(U&gt;1-10^{-20}\\) entonces volver a 1. Si \\(U&lt;0.5\\) entonces hacer \\(X=g\\left(1-U\\right)\\) en caso contrario hacer \\(X=-g\\left( U\\right)\\). Devolver \\(X\\). En manuales de funciones matemáticas, como Abramowitz y Stegun (1964), se tienen aproximaciones de la función cuantil de las principales distribuciones (por ejemplo en la página 993 las correspondientes a la normal estándar). R emplea una aproximación similar, basada en el algoritmo de Wichura (1988) más preciso, y que está implementado en el fichero fuente qnorm.c. "],["AR.html", "4.2 Método de aceptación rechazo", " 4.2 Método de aceptación rechazo Se trata de un método universal alternativo al de inversión para el caso de que no se pueda emplear la función cuantil, pero se dispone de una expresión (preferiblemente sencilla) para la función de densidad objetivo \\(f\\left( x \\right)\\). La idea es simular una variable aleatoria bidimensional \\(\\left( X, Y\\right)\\) con distribución uniforme en el hipografo de \\(f\\) (el conjunto de puntos del plano comprendidos entre el eje OX y \\(f\\)): \\[A_{f}=\\left\\{ \\left( x,y\\right) \\in \\mathbb{R}^{2}:0\\leq y\\leq f(x) \\right\\}.\\] De esta forma la primera componente tendrá la distribución deseada (Figura 4.4): Figura 4.4: Puntos con distribución uniforme en el hipografo de una función de densidad. \\[ P\\left( a&lt;X&lt;b\\right) = \\frac{\\text{Area de }\\left\\{ \\left( x,y\\right) \\in \\mathbb{R}^{2}:a&lt;x&lt;b;~0\\leq y\\leq f(x) \\right\\} }{\\text{Area de } A_{f}} \\\\ = \\int_{a}^{b}f(x) dx \\] El resultado anterior es también válido para una cuasi-densidad \\(f^{\\ast}\\) (no depende de la constante normalizadora). El resultado general sería en siguiente: Si \\(X\\) es una variable aleatoria con función de densidad \\(f\\) y \\(U \\sim \\mathcal{U}(0, 1)\\) entonces \\[\\left( X,c\\cdot U\\cdot f(x) \\right) \\sim \\mathcal{U}\\left( A_{cf}\\right)\\] siendo \\(A_{cf}=\\left\\{ \\left( x, y \\right) \\in \\mathbb{R}^{2} : 0 \\leq y \\leq cf\\left( x \\right) \\right\\}\\). Recíprocamente si \\(\\left( X,Y\\right) \\sim \\mathcal{U}\\left(A_{cf}\\right)\\) entonces12 \\(X\\sim f\\). Para generar valores de una variable aleatoria bidimensional con distribución uniforme en \\(A_{f}\\) (o en \\(A_{f^{\\ast}}\\)), se emplea el resultado anterior para generar valores en \\(A_{cg} \\supset A_{f}\\), siendo \\(g\\) una densidad auxiliar (preferiblemente fácil de simular y similar a \\(f\\)). Teniendo en cuenta además que: Si \\(\\left( X,Y\\right) \\sim \\mathcal{U}\\left( A\\right)\\) y \\(B \\subset A\\Rightarrow \\left. \\left( X,Y\\right) \\right\\vert _{B} \\sim \\mathcal{U}\\left(B\\right)\\) Por tanto, si \\(\\left( T, Y \\right)\\) sigue una distribución uniforme en \\(A_{cg}\\), aceptando los valores de \\(\\left( T, Y\\right)\\) que pertenezcan a \\(A_{f}\\) (o a \\(A_{f^{\\ast}}\\)) se obtendrán generaciones con distribución uniforme sobre \\(A_{f}\\) (o \\(A_{f^{\\ast}}\\)) y la densidad de la primera componente \\(T\\) será \\(f\\). 4.2.1 Algoritmo Supongamos que \\(f\\) es la densidad objetivo y \\(g\\) es una densidad auxiliar (fácil de simular y similar a \\(f\\)), de forma que existe una constante \\(c&gt;0\\) tal que: \\[f(x) \\leq c\\cdot g(x) \\text{, }\\forall x\\in \\mathbb{R},\\] (de donde se deduce que el soporte de \\(g\\) debe contener el de \\(f\\)). Algoritmo 4.3 (Método de aceptación-rechazo; Von Neuman, 1951) Generar \\(U \\sim \\mathcal{U}(0, 1)\\). Generar \\(T\\sim g\\). Si \\(c\\cdot U\\cdot g\\left( T\\right) \\leq f\\left( T\\right)\\) devolver \\(X=T\\), en caso contrario volver al paso 1. 4.2.2 Densidades acotadas en un intervalo cerrado Sea \\(f\\) una función de densidad cualquiera con soporte en un intervalo cerrado \\([a,b]\\) (es decir, \\(\\{x : f(x) &gt; 0\\}=[a,b]\\)) de tal forma que existe una constante \\(M&gt;0\\) tal que \\(f(x) \\leq M\\) \\(\\forall x\\) (es decir, \\(f\\) es acotada superiormente). En este caso puede tomarse como densidad auxiliar \\(g\\), la de una \\(\\mathcal{U}(a,b)\\). En efecto, tomando \\(c = M\\left( b-a\\right)\\) y teniendo en cuenta que \\[g(x) = \\left\\{ \\begin{array}{ll}\\frac{1}{b-a} &amp; \\text{si } x \\in [a,b]\\\\ 0 &amp; \\text{en caso contrario} \\end{array} \\right.\\] se tiene que \\(f(x) \\leq M = \\frac{c}{b-a}=c \\cdot g(x)\\), \\(\\forall x \\in [a,b]\\). Así pues, el algoritmo quedaría como sigue: Generar \\(U,V\\sim \\mathcal{U}(0, 1)\\). Hacer \\(T = a + \\left( b-a \\right) V\\). Si \\(M \\cdot U\\leq f\\left( T \\right)\\) devolver \\(X = T\\), en caso contrario volver al paso 1. Nota: no confundir \\(M\\) con \\(c = M \\left( b - a \\right)\\). Ejemplo 4.2 (simulación de distribución beta a partir de la uniforme) Para simular una variable con función de densidad \\(\\mathcal{Beta}(\\alpha, \\beta)\\): \\[f(x)=\\frac{\\Gamma (\\alpha + \\beta )}{\\Gamma (\\alpha )\\Gamma (\\beta )} x^{\\alpha -1}(1-x)^{\\beta -1}\\text{ si }0 \\leq x \\leq 1,\\] (siguiendo la notación de la función dbeta(x, shape1, shape2) de R), podemos considerar como distribución auxiliar una \\(\\mathcal{U}(0,1)\\), con \\(g(x) = 1\\) si \\(0 \\leq x \\leq 1\\). Esta distribución está acotada y es unimodal, si \\(\\alpha\\) y \\(\\beta\\) son mayores13 que 1, y su moda es \\(\\frac{\\alpha - 1} {\\alpha + \\beta - 2}\\), por lo que: \\[c = M = \\max_{0 \\leq x \\leq 1}f(x) = f\\left( \\frac{\\alpha - 1} {\\alpha + \\beta - 2} \\right).\\] Por ejemplo, considerando \\(\\alpha = 2\\) y \\(\\beta = 4\\), si comparamos la densidad objetivo con la auxiliar reescalada (Figura 4.5), confirmamos que esta última está por encima (y llegan a tocarse, por lo que se está empleando la cota óptima; ver siguiente sección). # densidad objetivo: dbeta # densidad auxiliar: dunif s1 &lt;- 2 s2 &lt;- 4 curve(dbeta(x, s1, s2), -0.1, 1.1, lwd = 2) c &lt;- dbeta((s1 - 1)/(s1 + s2 - 2), s1, s2) # abline(h = c, lty = 2) segments(0, c, 1, c, lty = 2, lwd = 2) abline(v = 0, lty = 3) abline(v = 1, lty = 3) abline(h = 0, lty = 3) Figura 4.5: Densidad objetivo beta (línea continua) y densidad auxiliar unifome reescalada (línea discontinua). El siguiente código implementa el método de aceptación-rechazo para simular valores de la densidad objetivo (se incluye una variable global ngen para contar el número de generaciones de la distribución auxiliar): ngen &lt;- 0 rbeta2 &lt;- function(s1 = 2, s2 = 2) { # Simulación por aceptación-rechazo # Beta a partir de uniforme c &lt;- dbeta((s1 - 1)/(s1 + s2 - 2), s1, s2) while (TRUE) { U &lt;- runif(1) X &lt;- runif(1) ngen &lt;&lt;- ngen+1 if (c*U &lt;= dbeta(X, s1, s2)) return(X) } } rbeta2n &lt;- function(n = 1000, s1 = 2, s2 = 2) { # Simulación n valores Beta(s1, s2) x &lt;- numeric(n) for(i in 1:n) x[i]&lt;-rbeta2(s1, s2) return(x) } Empleando estas funciones podemos generar una muestra de \\(10^3\\) observaciones de una \\(\\mathcal{Beta}(2, 4)\\) (calculando de paso el tiempo de CPU): set.seed(1) nsim &lt;- 1000 ngen &lt;- 0 system.time(x &lt;- rbeta2n(nsim, s1, s2)) ## user system elapsed ## 0.01 0.00 0.02 Para analizar la eficiencia podemos emplear el número de generaciones de la distribución auxiliar (siguiente sección): {cat(&quot;Número de generaciones = &quot;, ngen) cat(&quot;\\nNúmero medio de generaciones = &quot;, ngen/nsim) cat(&quot;\\nProporción de rechazos = &quot;, 1-nsim/ngen, &quot;\\n&quot;)} ## Número de generaciones = 2121 ## Número medio de generaciones = 2.121 ## Proporción de rechazos = 0.5285243 Finalmente podemos representar la distribución de los valores generados y compararla con la densidad teórica: hist(x, breaks = &quot;FD&quot;, freq = FALSE, main = &quot;&quot;) curve(dbeta(x, s1, s2), col = 2, lwd = 2, add = TRUE) Figura 4.6: Distribución de los valores generados mediante el método de aceptación-rechazo. Al ser un método exacto de simulación (si está bien implementado), la distribución de los valores generados debería comportarse como una muestra genuina de la distribución objetivo. Ejercicio 4.2 Dar un algoritmo para simular la función de densidad dada por \\(f\\left(x\\right) = \\frac{1}{16} \\left( 3x^{2}+2x+2 \\right)\\) si \\(0 \\le x \\le 2\\), cero en otro caso. Estudiar su eficiencia. 4.2.3 Eficiencia del algoritmo Como medida de la eficiencia del algoritmo de aceptación-rechazo podríamos considerar el número de iteraciones del algoritmo, es decir, el número de generaciones de la densidad auxiliar y de comparaciones para aceptar un valor de la densidad objetivo. Este número \\(N\\) es aleatorio y sigue una distribución geométrica (número de pruebas necesarias hasta obtener el primer éxito) con parámetro \\(p\\) (probabilidad de éxito) la probabilidad de aceptación en el paso 3: \\[p = \\frac{\\text{area}\\left(A_{f}\\right)}{\\text{area}\\left( A_{cg}\\right)} = \\frac{1}{c}.\\] Por tanto: \\[E\\left( N \\right) = \\frac1p = c\\] es el número medio de iteraciones del algoritmo (el número medio de pares de variables \\(\\left( T,U\\right)\\) que se necesitan generar, y de comparaciones, para obtener una simulación de la densidad objetivo). Es obvio, por tanto, que cuanto más cercano a 1 sea el valor de \\(c\\) más eficiente será el algoritmo (el caso de \\(c=1\\) se correspondería con \\(g=f\\) y no tendría sentido emplear este método). Una vez fijada la densidad \\(g\\), el valor óptimo será: \\[c_{\\text{opt}}=\\max_{\\{x : g(x) &gt; 0\\}} \\frac{f(x)}{g(x)}.\\] Nota: Hay que tener en cuenta que la cota óptima es el número medio de iteraciones \\(c\\) solo si conocemos las constantes normalizadoras. Si solo se conoce la cuasidensidad \\(f^{\\ast}\\) de la distribución objetivo (o de la auxiliar), la correspondiente cota óptima: \\[\\tilde{c} = \\max_{\\{x : g(x) &gt; 0\\}} \\frac{f^{\\ast}(x)}{g(x)}\\] asumirá la constante desconocida, aunque siempre podemos aproximar por simulación el verdadero valor de \\(c\\) y a partir de él la constante normalizadora (ver Ejercicio 4.3). Basta con tener en cuenta que, si \\(f(x) = f^{\\ast}(x)/k\\): \\[\\frac{1}{c} = \\frac{\\text{area}\\left(A_{f^{\\ast}}\\right)}{\\text{area}\\left( A_{\\tilde{c}g}\\right)} = \\frac{k}{\\tilde{c}},\\] y por tanto \\(k= \\tilde{c}/c\\). Ejemplo 4.3 (simulación de la normal a partir de la doble exponencial) Se trata de simular la distribución normal estándar, con función de densidad: \\[f(x) =\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^{2}}{2}} \\text{, } x\\in\\mathbb{R}\\text{, }\\] empleando el método de aceptación-rechazo considerando como distribución auxiliar una doble exponencial con \\(\\lambda=1\\) (o distribución de Laplace): \\[g(x) =\\frac{1}{2}e^{-\\left| x \\right|} \\text{, } x\\in\\mathbb{R}.\\] Esta distribución se utilizó en el Ejercicio 4.1, donde se definió la densidad auxiliar ddexp(x, lambda) y la función rdexp(lambda) para generar un valor aleatorio de esta distribución. En este caso el soporte de ambas densidades es la recta real y el valor óptimo para \\(c\\) es: \\[c_{\\text{opt}} = \\max_{x\\in\\mathbb{R}}\\frac{f(x)}{g(x) } = \\max_{x\\in\\mathbb{R}} \\frac{\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^{2}}{2}}}{\\frac{1}{2}e^{-\\left| x\\right| }} = \\sqrt{\\frac{2}{\\pi}}\\max_{x\\in\\mathbb{R}}e^{\\varphi(x)} = \\sqrt{\\frac{2}{\\pi}}e^{\\max_{x\\in\\mathbb{R}}\\varphi(x)},\\] donde \\(\\varphi(x) = -\\frac{x^{2}}{2}+\\left| x \\right|\\). Dado que esta función es simétrica, continua en toda la recta real y diferenciable tantas veces como se desee salvo en \\(x=0\\), bastará encontrar su máximo absoluto en el intervalo \\([0,\\infty]\\): \\[\\begin{aligned} x &amp; &gt;0\\Rightarrow\\varphi^{\\prime}(x) =-x+1,\\varphi ^{\\prime\\prime}(x) =-1;\\\\ \\{x &amp; &gt;0,\\varphi^{\\prime}(x) =0\\}\\Leftrightarrow x=1. \\end{aligned}\\] Por tanto, como \\(\\varphi^{\\prime\\prime}(1) &lt;0\\), \\(\\varphi\\) alcanza un máximo relativo en \\(x=1\\) y otro de idéntico valor en \\(x=-1\\). Resulta fácil demostrar que ambos son máximos absolutos (por los intervalos de crecimiento y decrecimiento de la función). Como consecuencia: \\[c_{\\text{opt}} = \\sqrt{\\frac{2}{\\pi}}e^{\\varphi(1)} =\\sqrt{\\frac{2}{\\pi}}e^{1/2} =\\sqrt{\\frac{2e}{\\pi}} \\approx 1.3155.\\] Si comparamos la densidad objetivo con la auxiliar reescalada con los parámetros óptimos (Figura 4.7), vemos que esta última está por encima, como debería ocurrir, pero llegan a tocarse (lo que validaría el cálculo para la obtención de la cota óptima). # densidad objetivo: dnorm # densidad auxiliar: ddexp c.opt &lt;- sqrt(2*exp(1)/pi) lambda.opt &lt;- 1 curve(c.opt * ddexp(x), xlim = c(-4, 4), lty = 2) curve(dnorm, add = TRUE) Figura 4.7: Densidad objetivo (normal estándar, línea continua) y densidad auxiliar (doble exponencial, línea discontinua) reescalada. Alternativamente, en lugar de obtener la cota óptima de modo analítico, podríamos aproximarla numéricamente: # NOTA: Cuidado con los límites # optimize(f = function(x) dnorm(x)/ddexp(x), maximum = TRUE, interval = c(-0.5,0.5)) optimize(f = function(x) dnorm(x)/ddexp(x), maximum = TRUE, interval = c(0, 2)) ## $maximum ## [1] 1 ## ## $objective ## [1] 1.315489 Vemos que la aproximación numérica coincide con el valor óptimo real \\(c_{\\text{opt}} \\approx\\) 1.3154892 (que se alcanza en \\(x = \\pm 1\\)). Para establecer la condición de aceptación o rechazo se puede tener en cuenta que: \\[c\\cdot U\\cdot\\frac{g\\left( T\\right) }{f\\left( T\\right) }=\\sqrt{\\frac {2e}{\\pi}}U\\sqrt{\\frac{\\pi}{2}}\\exp\\left( \\frac{T^{2}}{2}-\\left\\vert T\\right\\vert \\right) =U\\cdot\\exp\\left( \\frac{T^{2}}{2}-\\left\\vert T\\right\\vert +\\frac{1}{2}\\right),\\] aunque en general puede ser recomendable emplear \\(c\\cdot U\\cdot g\\left( T\\right) \\leq f\\left( T\\right)\\). Teniendo en cuenta los resultados anteriores, podríamos emplear el siguiente código para generar los valores de la densidad objetivo: ngen &lt;- 0 rnormAR &lt;- function() { # Simulación por aceptación-rechazo # Normal estandar a partir de doble exponencial c.opt &lt;- sqrt(2*exp(1)/pi) lambda.opt &lt;- 1 while (TRUE) { U &lt;- runif(1) X &lt;- rdexp(lambda.opt) # rdexpn(1, lambda.opt) ngen &lt;&lt;- ngen + 1 # Comentar esta línea para uso normal # if (U*exp((X^2+1)*0.5-abs(X)) &lt;= 1) return(X) if (c.opt * U * ddexp(X, lambda.opt) &lt;= dnorm(X)) return(X) } } rnormARn &lt;- function(n = 1000) { # Simulación n valores N(0,1) x &lt;- numeric(n) for(i in 1:n) x[i] &lt;- rnormAR() return(x) } Generamos una muestra de \\(10^4\\) observaciones: set.seed(1) nsim &lt;- 10^4 ngen &lt;- 0 system.time(x &lt;- rnormARn(nsim)) ## user system elapsed ## 0.12 0.00 0.13 Evaluamos la eficiencia: {cat(&quot;Número de generaciones = &quot;, ngen) cat(&quot;\\nNúmero medio de generaciones = &quot;, ngen/nsim) cat(&quot;\\nProporción de rechazos = &quot;, 1-nsim/ngen, &quot;\\n&quot;)} ## Número de generaciones = 13178 ## Número medio de generaciones = 1.3178 ## Proporción de rechazos = 0.2411595 Estos valores serían aproximaciones por simulación de los correspondientes valores teóricos (valor medio \\(c \\approx 1.3155\\) y probabilidad de rechazo \\(1 - p = 1 - 1/c \\approx 0.23983\\)). A partir de ellos podríamos decir que el algoritmo es bastante eficiente. Finalmente comparamos la distribución de los valores generados con la densidad teórica: hist(x, breaks = &quot;FD&quot;, freq = FALSE, main = &quot;&quot;) curve(dnorm, add = TRUE) Figura 4.8: Distribución de los valores generados mediante el método de aceptación-rechazo. Podemos observar que la distribución de los valores generados es la que cabría esperar de una muestra de tamaño nsim de la distribución objetivo (lo que nos ayudaría a confirmar que el algoritmo está bien implementado, al ser un método exacto de simulación). 4.2.4 Elección de la densidad auxiliar El principal problema con este método es encontrar una densidad auxiliar \\(g\\) de forma que \\(c_{\\text{opt}}\\) sea próximo a 1. Una solución intermedia consiste en seleccionar una familia paramétrica de densidades \\(\\{g_{\\theta} : \\theta \\in \\Theta\\}\\) entre las que haya alguna que se parezca bastante a \\(f\\), encontrar el valor de \\(c\\) óptimo para cada densidad de esa familia: \\[c_{\\theta}=\\max_{x}\\frac{f(x) }{g_{\\theta}(x) }\\] y, finalmente, elegir el mejor valor \\(\\theta_{0}\\) del parámetro, en el sentido de ofrecer el menor posible \\(c_{\\theta}\\): \\[c_{\\theta_{0}}=\\min_{\\theta\\in\\Theta}\\max_{x}\\frac{f(x) }{g_{\\theta}(x)}.\\] Ejemplo 4.4 (simulación de la normal mediante la doble exponencial, continuación) Continuando con el Ejemplo 4.3 anterior sobre la simulación de una normal estándar mediante el método de aceptación-rechazo, en lugar de fijar la densidad auxiliar a una doble exponencial con \\(\\lambda=1\\), consideraremos el caso general de \\(\\lambda&gt;0\\): \\[g_{\\lambda}(x) = \\frac{\\lambda}{2}e^{-\\lambda\\left| x \\right|} \\text{, } x\\in\\mathbb{R}.\\] Si pretendemos encontrar el mejor valor de \\(\\lambda\\), en términos de eficiencia del algoritmo, debemos calcular: \\[c_{\\lambda_{0}} = \\min_{\\lambda&gt;0}\\max_{x\\in\\mathbb{R}}\\frac{f(x)}{g_{\\lambda(x)}} =\\min_{\\lambda&gt;0}\\max_{x\\in\\mathbb{R}}\\frac{\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^{2}}{2}}}{\\frac{\\lambda}{2}e^{-\\lambda \\left| x \\right| }}.\\] De forma totalmente análoga a la vista para el caso \\(\\lambda=1\\), se tiene que: \\[c_{\\lambda}=\\max_{x\\in\\mathbb{R}}\\frac{\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^{2}}{2}}}{\\frac{\\lambda}{2}e^{-\\lambda \\left| x \\right| }}=\\frac{1}{\\lambda}\\sqrt{\\frac{2}{\\pi}}\\max_{x\\in\\mathbb{R}}e^{\\varphi _{\\lambda(x)}}=\\frac{1}{\\lambda}\\sqrt{\\frac{2}{\\pi}}e^{\\max_{x\\in\\mathbb{R}}\\varphi_{\\lambda(x)} },\\] donde \\(\\varphi_{\\lambda}(x) =-\\frac{x^{2}}{2}+\\lambda\\left| x \\right|\\). De forma totalmente similar también puede probarse que \\(\\varphi_{\\lambda}\\) alcanza su máximo absoluto en los puntos \\(x=\\pm\\lambda\\), siendo dicho valor máximo \\(\\varphi_{\\lambda}\\left( \\pm\\lambda \\right) = \\frac{\\lambda^{2}}{2}\\). Como consecuencia: \\[c_{\\lambda}=\\frac{1}{\\lambda}\\sqrt{\\frac{2}{\\pi}}e^{\\varphi_{\\lambda}\\left( \\pm\\lambda \\right)} =\\frac{e^{\\frac{\\lambda^{2}}{2}}}{\\lambda}\\sqrt{\\frac{2}{\\pi}}.\\] Finalmente debemos encontrar \\(\\lambda_{0}\\) tal que \\(c_{\\lambda_{0}}=\\min_{\\lambda&gt;0}c_{\\lambda}\\). Como: \\[\\frac{\\partial c_{\\lambda}}{\\partial \\lambda} =\\sqrt{\\frac{2}{\\pi}}\\frac{e^{\\frac{\\lambda^{2}}{2}}\\left( \\lambda^{2}-1\\right) }{\\lambda^{2}},\\] entonces \\(\\frac{\\partial c_{\\lambda}}{\\partial \\lambda} = 0\\Leftrightarrow\\lambda=1\\), ya que \\(\\lambda&gt;0\\). Además: \\[\\left. \\frac{\\partial ^{2}c_{\\lambda}}{\\partial \\lambda^{2}}\\right|_{\\lambda=1} =\\left.\\sqrt{\\frac {2}{\\pi}}\\frac{e^{\\frac{\\lambda^{2}}{2}}\\left( \\lambda^{5}-\\lambda ^{3}+2\\lambda\\right) }{\\lambda^{4}}\\right|_{\\lambda=1}=2\\sqrt{\\frac{2e}{\\pi}}&gt;0,\\] luego en \\(\\lambda=1\\) se alcanza el mínimo. curve(exp(x^2/2)/x*sqrt(2/pi), 0.1, 2.5, xlab = expression(lambda), ylab = expression(c[lambda])) abline(v = 1, lty = 2) Figura 4.9: Representación de la cota óptima dependiedo del valor del parámetro. De esto se deduce que la mejor densidad auxiliar doble exponencial es la correspondiente a \\(\\lambda=1\\). Por tanto el algoritmo más eficiente, con esta familia de densidades auxiliares, es el expuesto en el Ejemplo 4.3. Alternativamente también podríamos aproximar simultáneamente el parámetro óptimo y la cota óptima de la densidad auxiliar numéricamente: # Obtención de valores c y lambda óptimos aproximados fopt &lt;- function(lambda) { # Obtiene c fijado lambda optimize(f = function(x) dnorm(x)/ddexp(x,lambda), maximum = TRUE, interval = c(0, 2))$objective } # Encontrar lambda que minimiza res &lt;- optimize(fopt, interval = c(0.5, 2)) lambda.opt2 &lt;- res$minimum c.opt2 &lt;- res$objective lambda.opt2 ## [1] 0.9999987 c.opt2 ## [1] 1.315489 4.2.5 Ejemplo: inferencia bayesiana El algoritmo de aceptación-rechazo se emplea habitualmente en inferencia bayesiana. Denotando por: \\(f(x|\\theta)\\) la densidad muestral. \\(\\pi(\\theta)\\) la densidad a priori. \\(\\mathbf{x}=(x_1,...,x_n)^{\\top}\\) la muestra observada. El objetivo sería simular la distribución a posteriori de \\(\\theta\\): \\[\\pi(\\theta|\\mathbf{x}) = \\frac{L(\\mathbf{x}|\\theta)\\pi(\\theta)}{\\int L(\\mathbf{x}|\\theta)\\pi(\\theta)d\\theta},\\] siendo \\(L(\\mathbf{x}|\\theta)\\) la función de verosimilitud (\\(L(\\mathbf{x}|\\theta) = \\prod\\limits_{i=1}^{n}f(x_i|\\theta)\\) suponiendo i.i.d.). Es decir: \\[\\pi(\\theta | \\mathbf{x})\\propto L(\\mathbf{x}| \\theta)\\pi(\\theta).\\] Como esta distribución cambia al variar la muestra observada, puede resultar difícil encontrar una densidad auxiliar adecuada para simular valores de la densidad a posteriori \\(\\pi(\\theta|\\mathbf{x})\\). Por ejemplo, podríamos pensar en emplear la densidad a priori \\(\\pi(\\theta)\\) como densidad auxiliar. Teniendo en cuenta que: \\(\\pi(\\theta |\\mathbf{x})/\\pi(\\theta)\\propto L(\\mathbf{x}|\\theta)\\) \\(L(\\mathbf{x}|\\theta)\\leq \\tilde{c}=L(\\mathbf{x}|\\hat{\\theta})\\) siendo \\(\\hat{\\theta}\\) el estimador MV de \\(\\theta\\). El algoritmo sería el siguiente: Generar \\(U \\sim \\mathcal{U}(0, 1)\\). Generar \\(\\tilde{\\theta}\\sim \\pi(\\theta)\\). Si \\(L(\\mathbf{x}|\\hat{\\theta})\\cdot U \\leq L(\\mathbf{x}|\\tilde{\\theta})\\) devolver \\(\\tilde{\\theta}\\), en caso contrario volver al paso 1. Aunque, como se muestra en el siguiente ejercicio, esta elección de densidad auxiliar puede ser muy poco adecuada, siendo preferible en la práctica emplear un método adaptativo que construya la densidad auxiliar de forma automática (Sección 4.3.1). Ejercicio 4.3 (Simulación de la distribución a posteriori a partir de la distribución a priori) Para la estimación Bayes de la media de una normal se suele utilizar como distribución a priori una Cauchy. Generar una muestra i.i.d. \\(X_{i}\\sim N(\\theta_{0},1)\\) de tamaño \\(n=10\\) con \\(\\theta_{0}=1\\). Utilizar una \\(Cauchy(0,1)\\) (rcauchy()) como distribución a priori y como densidad auxiliar para simular por aceptación-rechazo una muestra de la densidad a posteriori (emplear dnorm() para construir la verosimilitud). Obtener el intervalo de probabilidad/credibilidad al 95%. mu0 &lt;- 1 n &lt;- 10 nsim &lt;- 10^4 set.seed(54321) x &lt;- rnorm(n, mean = mu0) # Función de verosimilitud # lik1 &lt;- function(mu) prod(dnorm(x, mean = mu)) # escalar lik &lt;- Vectorize(function(mu) prod(dnorm(x, mean = mu))) # vectorial # Cota óptima # Estimación por máxima verosimilitud emv &lt;- optimize(f = lik, int = range(x), maximum = TRUE) emv ## $maximum ## [1] 0.7353805 ## ## $objective ## [1] 3.303574e-08 c &lt;- emv$objective En este caso concreto, ya sabríamos que el estimador máximo verosímil es la media muestral: mean(x) ## [1] 0.7353958 y por tanto: c &lt;- lik(mean(x)) c ## [1] 3.303574e-08 # f.cuasi &lt;- function(mu) sapply(mu, lik1)*dcauchy(mu) f.cuasi &lt;- function(mu) lik(mu)*dcauchy(mu) curve(c * dcauchy(x), xlim = c(-4, 4), ylim = c(0, c/pi), lty = 2, xlab = &quot;mu&quot;, ylab = &quot;cuasidensidad&quot;) curve(f.cuasi, add = TRUE) Figura 4.10: Comparación de la cuasidensidad a posteriori (línea contínua) con la densidad a priori reescalada (línea discontinua). Por ejemplo, podríamos emplear el siguiente código para generar simulaciones de la distribución a posteriori mediante aceptación-rechazo a partir de la distribución de Cauchy: ngen &lt;- nsim mu &lt;- rcauchy(nsim) ind &lt;- c*runif(nsim) &gt; lik(mu) # TRUE si no verifica condición # Volver a generar si no verifica condición while (sum(ind)&gt;0){ le &lt;- sum(ind) ngen &lt;- ngen + le mu[ind] &lt;- rcauchy(le) ind[ind] &lt;- c*runif(le) &gt; lik(mu[ind]) # TRUE si no verifica condición } { # Número generaciones cat(&quot;Número de generaciones = &quot;, ngen) cat(&quot;\\nNúmero medio de generaciones = &quot;, ngen/nsim) cat(&quot;\\nProporción de rechazos = &quot;, 1-nsim/ngen,&quot;\\n&quot;) } ## Número de generaciones = 59422 ## Número medio de generaciones = 5.9422 ## Proporción de rechazos = 0.8317122 A partir de la aproximación del número medio de generaciones podemos aproximar la constante normalizadora: cte &lt;- c*nsim/ngen # integrate(f.cuasi, -Inf, Inf) f.aprox &lt;- function(mu) f.cuasi(mu)/cte Finalmente, a partir de los valores generados podemos aproximar el intervalo de probabilidad al 95% (intervalo de credibilidad bayesiano): q &lt;- quantile(mu, c(0.025, 0.975)) q ## 2.5% 97.5% ## 0.05001092 1.26026227 # Representar estimador e IC Bayes hist(mu, freq=FALSE, breaks = &quot;FD&quot;, main=&quot;&quot;) # abline(v = mean(x), lty = 3) # Estimación frecuentista abline(v = mean(mu), lty = 2, lwd = 2) # Estimación Bayesiana abline(v = q, lty = 2) curve(f.aprox, col = &quot;blue&quot;, add = TRUE) Figura 4.11: Distribución de los valores generados y aproximación del intervalo de credibilidad. Repetir el apartado anterior con \\(n=100\\). Emplearemos también \\(X\\sim f\\) para indicar que \\(X\\) es una variable aleatoria con función de densidad \\(f\\). Si \\(\\alpha\\) o \\(\\beta\\) son iguales a 1 puede simularse fácilmente por el método de inversión y si alguno es menor que 1 esta densidad no está acotada. "],["modificaciones-del-método-de-aceptación-rechazo.html", "4.3 Modificaciones del método de aceptación-rechazo", " 4.3 Modificaciones del método de aceptación-rechazo En el tiempo de computación del algoritmo de aceptación-rechazo influye: La proporción de aceptación (debería ser grande). La dificultad de simular con la densidad auxiliar. El tiempo necesario para hacer la comparación en el paso 4. En ciertos casos el tiempo de computación necesario para evaluar \\(f(x)\\) puede ser alto. Para evitar evaluaciones de la densidad se puede emplear una función squeeze que aproxime la densidad por abajo (una envolvente inferior): \\[s(x)\\leq f(x) \\text{, }\\forall x\\in \\mathbb{R}.\\] Algoritmo 4.4 (Marsaglia, 1977) Generar \\(U \\sim \\mathcal{U}(0, 1)\\) y \\(T\\sim g\\). Si \\(c\\cdot U\\cdot g\\left( T\\right) \\leq s\\left( T\\right)\\) devolver \\(X=T\\), en caso contrario 2.a. si \\(c\\cdot U\\cdot g\\left( T\\right) \\leq f\\left( T\\right)\\) devolver \\(X=T\\), 2.b. en caso contrario volver al paso 1. Figura 4.12: Ilustración del algoritmo de aceptación-rechazo con envolvente inferior (función squeeze). Cuanto mayor sea el área bajo \\(s(x)\\) (más próxima a 1) más efectivo será el algoritmo. Se han desarrollado métodos generales para la construcción de las funciones \\(g\\) y \\(s\\) de forma automática (cada vez que se evalúa la densidad se mejoran las aproximaciones). Estos métodos se basan principalmente en que una transformación de la densidad objetivo es cóncava o convexa. 4.3.1 Muestreo por rechazo adaptativo (ARS) Supongamos que \\(f\\) es una cuasi-densidad log-cóncava (i.e. \\(\\frac{\\partial ^{2}}{\\partial x^{2}}\\log f(x) &lt;0, ~\\forall x\\)). Sea \\(S_n=\\left\\{ x_{i}:i=0,\\cdots ,n+1\\right\\}\\) con \\(f(x_{i})\\) conocidos. Denotamos por \\(L_{i,i+1}(x)\\) la recta pasando por \\(\\left( x_{i},\\log f(x_{i})\\right)\\) y \\(\\left( x_{i+1},\\log f(x_{i+1})\\right)\\) \\(L_{i,i+1}(x)\\leq \\log f(x)\\) en el intervalo \\(I_{i}=(x_{i},x_{i+1}]\\) \\(L_{i,i+1}(x)\\geq \\log f(x)\\) fuera de \\(I_{i}\\) En el intervalo \\(I_{i}\\) se definen las envolventes de \\(\\log f\\left( x\\right)\\): \\(\\underline{\\phi}_n(x)=L_{i,i+1}(x)\\) \\(\\overline{\\phi}_n(x)=\\min \\left\\{L_{i-1,i}(x),L_{i+1,i+2}(x)\\right\\}\\) Las envolventes de \\(f(x)\\) en \\(I_{i}\\) serán: \\(s_n(x)=\\exp \\left( \\underline{\\phi}_n(x)\\right)\\) \\(G_n(x)=\\exp \\left( \\overline{\\phi}_n(x)\\right)\\) Tenemos entonces que: \\[s_n(x)\\leq f(x) \\leq G_n(x)=c\\cdot g_n(x)\\] donde \\(g_n(x)\\) es una mixtura discreta de distribuciones tipo exponencial truncadas (las tasas pueden ser negativas), que se puede simular fácilmente combinando el método de composición (Sección 4.4) con el método de inversión. Algoritmo 4.5 (Gilks, 1992) Inicializar \\(n\\) y \\(s_n\\). Generar \\(U \\sim \\mathcal{U}(0, 1)\\) y \\(T\\sim g_n\\). Si \\(U\\cdot G_n\\left( T\\right) \\leq s_n\\left( T\\right)\\) devolver \\(X=T\\), en caso contrario, 3.a Si \\(U\\cdot G_n\\left( T\\right) \\leq f\\left( T\\right)\\) devolver \\(X=T\\). 3.b Hacer \\(n=n+1\\), añadir \\(T\\) a \\(S_n\\) y actualizar \\(s_n\\) y \\(G_n\\). Volver al paso 2. Gilks y Wild (1992) propusieron una ligera modificación empleando tangentes para construir la cota superior, de esta forma se obtiene un método más eficiente pero requiere especificar la derivada de la densidad objetivo (ver Figura 4.12). La mayoría de las densidades de la familia exponencial de distribuciones son log-cóncavas. Hörmann (1995) extendió esta aproximación al caso de densidades \\(T_{c}\\)-cóncavas: \\[T_{c}(x) = signo(c)x^{c} \\\\ T_{0}(x) = \\log (x).\\] Aparte de la transformación logarítmica, la transformación \\(T_{-1/2}(x)=-1/\\sqrt{x}\\) es habitualmente la más empleada. 4.3.2 Método del cociente de uniformes Se puede ver como una modificación del método de aceptación-rechazo, de especial interés cuando el soporte no es acotado. Si \\((U,V)\\) se distribuye uniformemente sobre: \\[C_{f^{\\ast}} = \\left\\{ (u, v) \\in \\mathbb{R}^{2} : 0&lt;u\\leq \\sqrt{f^{\\ast}(v/u)} \\right\\},\\] siendo \\(f^{\\ast}\\) una función no negativa integrable (cuasi-densidad), entonces \\(X=V/U\\) tiene función de densidad proporcional a \\(f^{\\ast}\\) (Kinderman y Monahan, 1977). Además \\(C_{f^{\\ast}}\\) tiene área finita, por lo que pueden generarse fácilmente los valores \\((U,V)\\) con distribución \\(\\mathcal{U}\\left(C_{f^{\\ast}}\\right)\\) a partir de componentes uniformes unidimensionales (aceptando los puntos dentro de \\(C_{f^{\\ast}}\\)). De modo análogo al método de aceptación-rechazo, hay modificaciones para acelerar los cálculos y automatizar el proceso, construyendo regiones mediante polígonos: \\[C_{i}\\subset C_{f^{\\ast}}\\subset C_{s}.\\] También se puede extender al caso multivariante y considerar transformaciones adicionales. Ver por ejemplo el paquete rust. Ejemplo 4.5 (simulación de la distribución de Cauchy mediante cociente de uniformes) Si consideramos la distribución de Cauchy: \\[f(x) = \\frac{1}{\\pi (1 + x^2)} \\text{, } x\\in \\mathbb{R},\\] eliminando la constante por comodidad \\(f(x) \\propto 1/(1 + x^2)\\), se tiene que: \\[\\begin{aligned} C_{f^{\\ast}} &amp; = \\left\\{ (u, v) \\in \\mathbb{R}^{2} : 0 &lt;u \\leq \\frac{1}{\\sqrt{1 + (v/u)^2}} \\right\\} \\\\ &amp; = \\left\\{ (u, v) \\in \\mathbb{R}^{2} : u &gt; 0, u^2 \\leq \\frac{u^2}{u^2 + v^2} \\right\\} \\\\ &amp; = \\left\\{ (u, v) \\in \\mathbb{R}^{2} : u &gt; 0, u^2 + v^2 \\leq 1 \\right\\}, \\end{aligned}\\] dando como resultando que \\(C_{f^{\\ast}}\\) es el semicírculo de radio uno, y podemos generar valores con distribución uniforme en esta región a partir de \\(\\mathcal{U}\\left([0,1]\\times[-1,1] \\right)\\). Por tanto, podemos emplear el siguiente código para generar valores de la densidad objetivo: rcauchy.rou &lt;- function(nsim) { # Cauchy mediante cociente de uniformes ngen &lt;- nsim u &lt;- runif(nsim, 0, 1) v &lt;- runif(nsim, -1, 1) x &lt;- v/u ind &lt;- u^2 + v^2 &gt; 1 # TRUE si no verifica condición # Volver a generar si no verifica condición while (le &lt;- sum(ind)){ # mientras le = sum(ind) &gt; 0 ngen &lt;- ngen + le u &lt;- runif(le, 0, 1) v &lt;- runif(le, -1, 1) x[ind] &lt;- v/u ind[ind] &lt;- u^2 + v^2 &gt; 1 # TRUE si no verifica condición } attr(x, &quot;ngen&quot;) &lt;- ngen return(x) } set.seed(1) nsim &lt;- 10^4 rx &lt;- rcauchy.rou(nsim) hist(rx, breaks = &quot;FD&quot;, freq = FALSE, main = &quot;&quot;, xlim = c(-6, 6)) curve(dcauchy, add = TRUE) Figura 4.13: Distribución de los valores generados mediante el método de cociente de uniformes. # Número generaciones ngen &lt;- attr(rx, &quot;ngen&quot;) {cat(&quot;Número de generaciones = &quot;, ngen) cat(&quot;\\nNúmero medio de generaciones = &quot;, ngen/nsim) cat(&quot;\\nProporción de rechazos = &quot;, 1-nsim/ngen,&quot;\\n&quot;)} ## Número de generaciones = 12751 ## Número medio de generaciones = 1.2751 ## Proporción de rechazos = 0.2157478 "],["composicion.html", "4.4 Método de composición (o de simulación condicional)", " 4.4 Método de composición (o de simulación condicional) En ocasiones la densidad de interés se puede expresar como una mixtura discreta de densidades: \\[f(x)=\\sum_{j=1}^{k}p_{j}f_{j}(x)\\] con \\(\\sum_{j=1}^{k}p_j=1\\), \\(p_j\\geq 0\\) y \\(f_j\\) densidades (sería también válido para funciones de distribución o variables aleatorias, incluyendo el caso discreto). Algoritmo 4.6 (simulación de una mixtura discreta) Generar \\(J\\) con distribución \\(P\\left( J=j \\right) = p_j\\). Generar \\(X\\sim f_J\\). Ejemplo 4.6 (distribución doble exponencial) A partir de la densidad de la distribución doble exponencial: \\[f(x) =\\frac{\\lambda }{2}e^{-\\lambda \\left\\vert x\\right\\vert }% \\text{, }\\forall x\\in \\mathbb{R},\\] se deduce que: \\[f(x) =\\frac{1}{2}f_{1}(x) +\\frac{1}{2}f_{2}\\left( x\\right)\\] siendo: \\[f_{1}(x) = \\left\\{ \\begin{array}{ll} \\lambda e^{-\\lambda x} &amp; \\text{si } x\\geq 0 \\\\ 0 &amp; \\text{si } x&lt;0 \\end{array} \\right., \\ f_{2}(x) = \\left\\{ \\begin{array}{ll} \\lambda e^{\\lambda x} &amp; \\text{si } x&lt;0 \\\\ 0 &amp; \\text{si } x\\geq 0 \\end{array} \\ \\right.\\] El algoritmo resultante sería el siguiente (empleando dos números pseudo aleatorios uniformes, el primero para seleccionar el índice y el segundo para generar un valor de la correspondiente componente mediante el método de inversión): Generar \\(U,V\\sim \\mathcal{U}(0, 1)\\). Si \\(U&lt;0.5\\) devolver \\(X=-\\ln \\left( 1-V\\right)/\\lambda\\). En caso contrario devolver \\(X=\\ln(V)/\\lambda\\). Observaciones: En ocasiones se hace un reciclado de los números aleatorios (solo se genera una uniforme, e.g. \\(V=2(U-0.5)\\) si \\(U\\in (0.5,1)\\)). En ciertas ocasiones por comodidad, para simular una muestra de tamaño \\(n\\), se simulan muestras de tamaño \\(np_{i}\\) con densidad \\(f_{i}\\) y se combinan aleatoriamente. Otro ejemplo de una mixtura discreta es el estimador tipo núcleo de la densidad (ver e.g. la ayuda de la función density() de R o la Sección 3.3 del libro Técnicas de Remuestreo). Simular a partir de una estimación de este tipo es lo que se conoce como bootstrap suavizado. En el caso de una mixtura continua tendríamos: \\[f(x)=\\int g(x|y)h(y)dy\\] Algoritmo 4.7 (simulación de una mixtura continua) Generar \\(Y\\sim h\\). Generar \\(X\\sim g(\\cdot |Y)\\). Este algoritmo es muy empleado en Inferencia Bayesiana y en la simulación de algunas variables discretas (como la Binomial Negativa, denominada también distribución Gamma-Poisson, o la distribución Beta-Binomial; ver Sección ??), ya que el resultado sería válido cambiando las funciones de densidad \\(f\\) y \\(g\\) por funciones de masa de probabilidad. "],["notables-cont.html", "4.5 Métodos específicos para la generación de algunas distribuciones notables", " 4.5 Métodos específicos para la generación de algunas distribuciones notables En el pasado se ha realizado un esfuerzo considerable para desarrollar métodos eficientes para la simulación de las distribuciones de probabilidad más importantes. Estos algoritmos se describen en la mayoría de los libros clásicos de simulación (e.g. Cao, 2002, Capítulo 5)14, principalmente porque resultaba necesario implementar estos métodos durante el desarrollo de software estadístico. Hoy en día estos algoritmos están disponibles en numerosas bibliotecas y no es necesario su implementación (por ejemplo, se puede recurrir a R o emplear su librería matemática disponible en https://svn.r-project.org/R/trunk/src/nmath). Sin embargo, además de que muchos de ellos servirían como ilustración de la aplicación de los métodos generales expuestos en secciones anteriores, pueden servir como punto de partida para la generación de otras distribuciones. Entre los distintos métodos disponibles para la generación de las distribuciones continuas más conocidas podríamos destacar: Método de Box-Müller para la generación de normales independientes (pero que se puede generalizar para otras distribuciones y dependencia). Algoritmos de Jöhnk (1963) y Cheng (1978) para la generación de la distribución beta (como ejemplo de las eficiencias de los métodos de aceptación-rechazo). 4.5.1 Método de Box-Müller Se basa en la siguiente propiedad. Dadas dos variables aleatorias independientes \\(E \\sim \\exp\\left( 1\\right)\\) y \\(U \\sim \\mathcal{U}( 0, 1 )\\), las variables \\(\\sqrt{2E} \\cos 2\\pi U\\) y \\(\\sqrt{2E}\\operatorname{sen} 2\\pi U\\) son \\(\\mathcal{N}( 0, 1 )\\) independientes. Algoritmo 4.8 (de Box-Müller, 1958) Generar \\(U,V\\sim \\mathcal{U}(0, 1)\\). Hacer \\(W_1=\\sqrt{-2\\ln U}\\) y \\(W_2=2\\pi V\\). Devolver \\(X_1=W_1\\cos W_2\\), \\(X_2=W_1\\operatorname{sen}W_2\\). Podemos hacer que la función rnorm() de R emplee este algoritmo estableciendo el parámetro normal.kind a \"Box-Muller\" en una llamada previa a set.seed() o RNGkind(). Este método está relacionado con el denominado método FFT (transformada de Fourier; e.g. Davies y Harte, 1987) para la generación de una normal multidimensional con dependencia, que resulta ser equivalente al Circular embedding (Dietrich and Newsam, 1997). La idea de estos métodos es que, considerando módulos exponenciales y fases uniformes generamos normales independientes, pero cambiando la varianza de los módulos (\\(W_1\\)) podemos inducir dependencia. Adicionalmente, cambiando la distribución de las fases (\\(W_2\\)) se generan distribuciones distintas de la normal. 4.5.2 Simulación de la distribución beta Existen multitud de algoritmos para simular la distribución \\(\\mathcal{Beta}(a, b)\\). Probablemente, el más sencillo de todos es el que se obtiene a partir de la distribución gamma o de la chi-cuadrado, si se dispone de un algoritmo para generar estas distribuciones, empleando la definición habitual de la distribución beta: Si \\(Y \\sim \\mathcal{Gamma}(a, s)\\) y \\(Z \\sim \\mathcal{Gamma}(b, s)\\) son independientes, entonces \\[X=\\frac{Y}{Y+Z} \\sim \\mathcal{Beta}(a, b).\\] Como la distribución resultante no depende de \\(s\\) y \\(\\chi^2_{n} \\overset{d}{=} \\mathcal{Gamma}\\left(\\tfrac{n}{2}, \\tfrac{1}{2}\\right)\\), se podría considerar \\(Y \\sim \\chi^2_{2a}\\) y \\(Z \\sim \\chi^2_{2b}\\) independientes. También se podrían emplear resultado conocidos relacionados con esta distribución, como por ejemplo que la distribución del estadístico de orden \\(k\\) de una muestra de tamaño \\(n\\) de una distribución uniforme tiene una distribución beta: \\[U_{(k)} \\sim \\mathcal{Beta}(k,n+1-k).\\] El resultado es el algoritmo de Fox (1963), que podría ser adecuado para simular esta distribución cuando \\(a, b \\in \\mathbb{N}\\) y son valores pequeños. Algoritmo 4.9 (de Fox, 1963) Generar \\(U_1, U_2, \\ldots, U_{a+b-1} \\sim \\mathcal{U}(0, 1)\\). Ordenar: \\(U_{(1)}\\leq U_{(2)}\\leq\\cdots\\leq U_{(a+b-1)}\\). Devolver \\(X=U_{(a)}\\). Es obvio que este algoritmo puede resultar muy lento si alguno de los dos parámetros es elevado (pues habrá que simular muchas uniformes para conseguir un valor simulado de la beta). Además, en función de cuál de los dos parámetros, \\(a\\) ó \\(b\\), sea mayor, resultará más eficiente, en el paso 2, comenzar a ordenar por el mayor, luego el segundo mayor, y así sucesivamente, o hacerlo empezando por el menor. En cualquier caso, es obvio que no es necesario ordenar todos los valores \\(U_{i}\\) generados, sino tan sólo encontrar el que ocupa el lugar \\(a\\)-ésimo. Un método válido aunque \\(a\\) ó \\(b\\) no sean enteros es el dado por el algoritmo de Jöhnk (1964). Algoritmo 4.10 (de Jöhnk, 1964) Generar \\(U_1, U_2\\sim \\mathcal{U}(0, 1)\\). Hacer \\(V = U_1^{\\frac1a}\\), \\(W = U_2^{\\frac1b}\\) y \\(S = V+W\\). Si \\(S \\leq 1\\) devolver \\(X = \\frac VS\\), en caso contrario volver al paso 1. El método resulta extremadamente ineficiente para \\(a\\) ó \\(b\\) mayores que 1. Esto es debido a que la condición \\(S\\leq1\\) del paso 3 puede tardar muchísimo en verificarse. Por este motivo, el algoritmo de Jöhnk sólo es recomendable para \\(a&lt;1\\) y \\(b&lt;1\\). Como remedio a esto puede usarse el algoritmo de Cheng (1978) que es algo más complicado de implementar15 pero mucho más eficiente. Algoritmo 4.11 (de Cheng, 1978) Inicialización: Hacer \\(\\alpha = a + b\\). Si \\(\\min(a,b) \\leq1\\) entonces hacer \\(\\beta=\\frac1{\\min( a,b)}\\), en otro caso hacer \\(\\beta=\\sqrt{\\frac{\\alpha-2}{2pq-\\alpha}}\\). Hacer \\(\\gamma=a+\\frac1\\beta\\). Simulación: Generar \\(U_1, U_2\\sim \\mathcal{U}(0, 1)\\). Hacer \\(V=\\beta\\cdot\\ln\\left( \\frac{U_1}{1-U_1}\\right)\\) y \\(W=a\\cdot e^{V}\\). Si \\(\\alpha\\cdot\\ln\\left( \\frac\\alpha{b+W}\\right) +\\gamma V-\\ln4 \\ge \\ln\\left( U_1^{2}U_2\\right)\\) devolver \\(X=\\frac W{b+W}\\), en caso contrario volver al paso 1. Cuidado con la notación y la parametrización empleadas, puede variar entre referencias. Por ejemplo, en Cao (2002) la notación de la distribución Gamma es ligeramente distinta a la empleada en R y en el presente libro. R implementa este algoritmo en el fichero fuente rbeta.c. "],["referencias.html", "Referencias", " Referencias Bibliografía básica Cao, R. (2002). Introducción a la simulación y a la teoría de colas. NetBiblo. Cao R. y Fernández-Casal R. (2020). Técnicas de Remuestreo. https://rubenfcasal.github.io/book_remuestreo. Gentle, J.E. (2003). Random number generation and Monte Carlo methods. SpringerVerlag. Jones, O. et al. (2009). Introduction to Scientific Programming and Simulation Using R. CRC. Ripley, B.D. (1987). Stochastic Simulation. John Wiley &amp; Sons. Robert, C.P. y G. Casella (2010). Introducing Monte Carlo Methods with R. Springer. Ross, S.M. (1999).Simulación. Prentice Hall. Suess, E.A. y Trumbo, B.E. (2010). Introduction to probability simulation and Gibbs sampling with R. Springer. "],["bibliografía-complementaria.html", "Bibliografía complementaria", " Bibliografía complementaria Libros Azarang, M. R. y García Dunna, E. (1996). Simulación y análisis de modelos estocásticos. McGraw-Hill. Bratley, P., Fox, B.L. y Schrage L.E. (1987). A guide to simulation. Springer-Verlag. Davison, A.C. y Hinkley, D.V. (1997). Bootstrap Methods and Their Application. Cambridge University Press. Devroye, L. (1986). Non-uniform random variate generation. Springer-Verlag. Evans, M. y Swartz, T. (2000). Approximating integrals via Monte Carlo and determinstic methods. Oxford University Press. Gentle, J.E. (1998). Random number generation and Monte Carlo methods. Springer-Verlag. Hyndman, R.J. y Athanasopoulos, G. (2018). Forecasting: principles and practice. OTexts. Disponible online: 2nd edition (forecast), 3rd edition (fable). Hofert, M. (2018). Elements of Copula Modeling with R, Springer. Hörmann, W. et al. (2004). Automatic Nonuniform Random Variate Generation. Springer. Knuth, D.E. (1969). The Art of Computer Programming. Volume 2. Addison-Wesley. Knuth, D.E. (2002). The Art of Computer Programming. Volume 2, third edition, ninth printing. Addison-Wesley. Law, A.M. y Kelton, W.D. (1991). Simulation, modeling and analysis. McGraw-Hill. Liu, J.S. (2004). Monte Carlo strategies in scientific computing. Springer. Moeschlin, O., Grycko, E., Pohl, C. y Steinert, F. (1998). Experimental stochastics. Springer-Verlag. Nelsen, R.B. (2006). An introduction to copulas, 2ª ed., Springer. Nelson, R. (1995). Probability, stochastic processes, and queueing theory: the mathematics of computer performance modelling. Springer-Verlag. Pardo, L. y Valdés, T. (1987). Simulación. Aplicaciones prácticas a la empresa. Díaz de Santos. Robert, C.P. y G. Casella (2004). Monte Carlo statistical methods. Springer. Shao, J. (2003). Mathematical statistics. Springer. Artículos Demirhan, H. y Bitirim, N. (2016). CryptRndTest: an R package for testing the cryptographic randomness. The R Journal, 8(1), 233-247. Downham, D.Y. (1970). Algorithm AS 29: The runs up and down test. Journal of the Royal Statistical Society. Series C (Applied Statistics), 19(2), 190-192. Gilks, W.R. y Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. Journal of the Royal Statistical Society. Series C (Applied Statistics), 41(2), 337-348. Kinderman, A.J. y Monahan, J.F. (1977). Computer generation of random variables using the ratio of uniform deviates. ACM Transactions on Mathematical Software (TOMS), 3(3), 257-260. LEcuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159164. LEcuyer, P. y Simard, R. (2007). TestU01: A C library for empirical testing of random number generators. ACM Transactions on Mathematical Software (TOMS), 33(4), 1-40. Marsaglia, G. y Tsang, W.W. (2002). Some difficult-to-pass tests of randomness. Journal of Statistical Software, 7(3), 1-9. Marsaglia, G., Zaman, A. y Tsang, W.W. (1990). Toward a universal random number generator. Stat. Prob. Lett., 9(1), 35-39. Matsumoto, M. y Nishimura, T. (1998). Mersenne Twister: A 623-dimensionally equidistributed uniform pseudo-random number generator, ACM Transactions on Modeling and Computer Simulation, 8, 330. Odeh, R.E. y Evans, J.O. (1974). The percentage points of the normal distribution. Journal of the Royal Statistical Society. Series C (Applied Statistics), 23(1), 96-97. Patefield, W.M. (1981). Algorithm AS 159: An efficient method of generating random r x c tables with given row and column totals. Applied Statistics, 30, 9197. Park, S.K. y Miller , K.W. (1988). Random number generators: good ones are hard to find. Communications of the ACM, 31(10), 1192-1201. Park, S.K., Miller, K.W. y Stockmeyer, P.K. (1993). Technical correspondence. Communications of the ACM, 36(7), 108-110. Wichura, M.J. (1988) Algorithm AS 241: The Percentage Points of the Normal Distribution. Applied Statistics, 37, 477484. "],["links.html", "A Enlaces", " A Enlaces Repositorio: rubenfcasal/simbook Recursos para el aprendizaje de R: En este post se muestran algunos recursos que pueden ser útiles para el aprendizaje de R y la obtención de ayuda. Bookdown: Técnicas de Remuestreo (github). Introducción a RMarkdown Advanced R: 1ª ed, 2ª ed R packages "],["gof-aleat.html", "B Bondad de Ajuste y Aleatoriedad", " B Bondad de Ajuste y Aleatoriedad En los métodos clásicos de inferencia estadística es habitual asumir que los valores observados \\(X_1,\\ldots, X_n\\) (o los errores de un modelo) constituyen una muestra aleatoria simple de una variable aleatoria \\(X\\). Se están asumiendo por tanto dos hipótesis estructurales: la independencia (aleatoriedad) y la homogeneidad (misma distribución) de las observaciones (o de los errores). Adicionalmente, en inferencia paramétrica se supone que la distribución se ajusta a un modelo paramétrico específico \\(F_{\\theta}(x)\\), siendo \\(\\theta\\) un parámetro que normalmente es desconocido. Uno de los objetivos de la inferencia no paramétrica es desarrollar herramientas que permitan verificar el grado de cumplimiento de las hipótesis anteriores16. Los procedimientos habituales incluyen métodos descriptivos (principalmente gráficos), contrastes de bondad de ajuste (también de homogeneidad o de datos atípicos) y contrastes de aleatoriedad. En este apéndice se describen brevemente algunos de los métodos clásicos, principalmente con la idea de que pueden ser de utilidad para evaluar resultados de simulación y para la construcción de modelos del sistema real (e.g. para modelar variables que se tratarán como entradas del modelo general). Se empleará principalmente el enfoque de la estadística no paramétrica, aunque también se mostrarán algunas pequeñas diferencias entre su uso en inferencia y en simulación. Los métodos genéricos no son muy adecuados para evaluar generadores aleatorios (e.g. LEcuyer y Simard, 2007). La recomendación sería emplear baterías de contrastes recientes, como las descritas en la Sección 2.3.2. No obstante, en la última sección se describirán, únicamente con fines ilustrativos, algunos de los primeros métodos diseñados específicamente para generadores aleatorios. El otro objetivo de la inferencia estadística no paramétrica es desarrollar procedimientos alternativos (métodos de distribución libre) que sean válidos cuando no se verifica alguna de las hipótesis estructurales. "],["gof.html", "B.1 Métodos de bondad de ajuste", " B.1 Métodos de bondad de ajuste A partir de \\(X_1,\\ldots,X_n\\) m.a.s. de \\(X\\) con función de distribución \\(F\\), interesa realizar un contraste de la forma: \\[\\left \\{ \\begin{array}{l} H_0:F=F_0\\\\ H_1:F\\neq F_0 \\end{array} \\right.\\] En este caso interesará distinguir principalmente entre hipótesis nulas simples (especifican un único modelo) y compuestas (especifican un conjunto o familia de modelos). Por ejemplo: \\(H_0\\) simple \\(H_0\\) compuesta \\(\\left \\{ \\begin{array}{l} H_0:F= \\mathcal{N}(0,1)\\\\ H_1:F\\neq \\mathcal{N}(0,1) \\end{array} \\right.\\) \\(\\left \\{ \\begin{array}{l} H_0:F= \\mathcal{N}(\\mu,\\sigma^2)\\\\ H_1:F\\neq \\mathcal{N}(\\mu,\\sigma^2) \\end{array} \\right.\\) Entre los métodos gráficos habituales estarían: histograma, gráfico de la densidad suavizada, gráfico de tallo y hojas, gráfico de la distribución empírica (o versión suavizada) y gráficos P-P o Q-Q. Entre los métodos de contrastes de hipótesis generales (\\(H_0:F=F_0\\)) destacarían las pruebas: Chi-cuadrado de Pearson, Kolmogorov-Smirnov, Cramer-von Mises o Anderson-Darling. Además de los específicos de normalidad (\\(H_0:F= \\mathcal{N}(\\mu,\\sigma^2)\\)): Kolmogorov-Smirnov-Lilliefors, Shapiro-Wilks y los de asimetría y apuntamiento. B.1.1 Histograma Se agrupan los datos en intervalos \\(I_{k}=\\left[ L_{k-1},L_{k}\\right)\\) con \\(k=1, \\ldots, K\\) y a cada intervalo se le asocia un valor (altura de la barra) igual a la frecuencia absoluta de ese intervalo \\(n_k = \\sum_{i=1}^{n}\\mathbf{1}\\left( X_i \\in [L_{k-1},L_{k}) \\right)\\), si la longitud de los intervalos es constante, o proporcional a dicha frecuencia (de forma que el área coincida con la frecuencia relativa y pueda ser comparado con una función de densidad): \\[\\hat{f}_n(x)=\\frac{n_{i}}{n\\left( L_{k}-L_{k-1}\\right)}\\] Como ya se ha visto anteriormente, en R podemos generar este gráfico con la función hist() del paquete base. Algunos de los principales parámetros (con los valores por defecto) son los siguientes: hist(x, breaks = &quot;Sturges&quot;, freq = NULL, plot = TRUE, ...) breaks: puede ser un valor numérico con el número de puntos de discretización, un vector con los puntos de discretización, una cadena de texto que los determine (otras opciones son \"Scott\" y \"FD\"; en este caso llamará internamente a la función nclass.xxx() donde xxx se corresponde con la cadena de texto), o incluso una función personalizada que devuelva el número o el vector de puntos de discretización. freq: lógico (TRUE por defecto si los puntos de discretización son equidistantes), determina si en el gráfico se representan frecuencias o densidades. plot: lógico, se puede establecer a FALSE si no queremos generar el gráfico y solo nos interesan el objeto con los resultados (que devuelve de forma invisible, por ejemplo para discretizar los valores en intervalos). Ejemplo: datos &lt;- c(22.56,22.33,24.58,23.14,19.03,26.76,18.33,23.10, 21.53,9.06,16.75,23.29,22.14,16.28,18.89,27.48,10.44, 26.86,27.27,18.74,19.88,15.76,30.77,21.16,24.26,22.90, 27.14,18.02,21.53,24.99,19.81,11.88,24.01,22.11,21.91, 14.35,11.14,9.93,20.22,17.73,19.05) hist(datos, freq = FALSE) curve(dnorm(x, mean(datos), sd(datos)), add = TRUE) Si el número de valores es muy grande (por ejemplo en el caso de secuencias aleatorias), nos puede interesar establecer la opción breaks = \"FD\" para aumentar el número de intervalos de discretización. En cualquier caso, como se muestra en la Figura B.1, la convergencia del histograma a la densidad teórica se podría considerar bastante lenta. Alternativamente se podría considerar una estimación suave de la densidad, por ejemplo empleando la estimación tipo núcleo implementada en la función density(). Figura B.1: Convergencia del histograma a la densidad teórica. B.1.2 Función de distribución empírica La función de distribución empírica \\(F_n\\left( x \\right)=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\left( X_i\\leq x\\right)\\) asigna a cada número real \\(x\\) la frecuencia relativa de observaciones menores o iguales que \\(x\\). Para obtener las frecuencias relativas acumuladas, se ordena la muestra \\(X_{(1)}\\leq X_{(2)}\\leq \\cdots \\leq X_{(n)}\\) y: \\[F_n(x)=\\left \\{ \\begin{array}{cll} 0 &amp; \\text{si } &amp;x&lt;X_{\\left( 1\\right) }\\\\ \\tfrac{i}n &amp; \\text{si } &amp; X_{\\left( i\\right) }\\leq x&lt;X_{\\left( i+1\\right) }\\\\ 1 &amp; \\text{si } &amp; X_{\\left( n\\right) }\\leq x \\end{array} \\right.\\] Ejemplo: fn &lt;- ecdf(datos) curve(ecdf(datos)(x), xlim = extendrange(datos), type = &#39;s&#39;, ylab = &#39;distribution function&#39;, lwd = 2) curve(pnorm(x, mean(datos), sd(datos)), add = TRUE) Figura B.2: Comparación de la distribución empírica de los datos de ejemplo con la función de distribución de la aproximación normal. B.1.3 Gráficos P-P y Q-Q El gráfico de probabilidad (o de probabilidad-probabilidad) es el gráfico de dispersión de: \\[\\left \\{ \\left( F_0(x_{i}), F_n(x_{i}) \\right) :i=1,\\cdots,n\\right \\}\\] siendo \\(F_n\\) la función de distribución empírica y \\(F_0\\) la función de distribución bajo \\(H_0\\) (con la que desea comparar, si la hipótesis nula es simple) o una estimación bajo \\(H_0\\) (si la hipótesis nula es compuesta; e.g. si \\(H_0:F= \\mathcal{N}(\\mu,\\sigma^2)\\), \\(\\hat{F}_0\\) función de distribución de \\(\\mathcal{N}(\\hat{\\mu},\\hat{\\sigma}^2)\\)). Si \\(H_0\\) es cierta, la nube de puntos estará en torno a la recta \\(y=x\\) (probabilidades observadas próximas a las esperadas bajo \\(H_0\\)). El gráfico Q-Q (cuantil-cuantil) es equivalente al anterior pero en la escala de la variable: \\[\\left\\{ \\left( q_{i}, x_{(i)}\\right) : i=1, \\cdots, n \\right\\}\\] siendo \\(x_{(i)}\\) los cuantiles observados y \\(q_{i}=F_0^{-1}(p_{i})\\) los esperados17 bajo \\(H_0\\). Ejemplo: qqnorm(datos) qqline(datos) require(car) qqPlot(datos, &quot;norm&quot;) ## [1] 10 38 B.1.4 Contraste chi-cuadrado de Pearson Se trata de un contraste de bondad de ajuste: \\[\\left \\{ \\begin{array}{l} H_0:F=F_0\\\\ H_1:F\\neq F_0\\end{array} \\right.\\] desarrollado inicialmente para variables categóricas. En el caso general, podemos pensar que los datos están agrupados en \\(k\\) clases: \\(C_1,\\cdots,C_{k}\\). Por ejemplo, si la variable es categórica o discreta, cada clase se puede corresponder con una modalidad. Si la variable es continua habrá que categorizarla en intervalos. Si la hipótesis nula es simple, cada clase tendrá asociada una probabilidad \\(p_{i}=P\\left( X\\in C_{i} \\right)\\) bajo \\(H_0\\) . Si por el contrario es compuesta, se trabajará con una estimación de dicha probabilidad (y habrá que correguir la distribución aproximada del estadístico del contraste). Clases Discreta Continua \\(H_0\\) simple \\(H_0\\) compuesta \\(C_1\\) \\(x_1\\) \\([L_0,L_1)\\) \\(p_1\\) \\(\\hat{p}_1\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(C_{k}\\) \\(x_{k}\\) \\([L_{k-1},L_{k})\\) \\(p_{k}\\) \\(\\hat{p}_{k}\\) \\(\\sum_{i}p_{i}=1\\) \\(\\sum_{i}\\hat{p}_{i}=1\\) Se realizará un contraste equivalente: \\[\\left\\{ \\begin{array}[c]{l} H_0:\\text{Las probabilidades son correctas}\\\\ H_1:\\text{Las probabilidades no son correctas} \\end{array} \\right.\\] Si \\(H_0\\) es cierta, la frecuencia relativa \\(f_{i}\\) de la clase \\(C_{i}\\) es una aproximación de la probabilidad teórica, \\(f_{i}\\approx p_{i}\\). Equivalentemente, las frecuencias observadas \\(n_{i}=n\\cdot f_{i}\\) deberían ser próximas a las esperadas \\(e_{i}=n\\cdot p_{i}\\) bajo \\(H_0\\), sugiriendo el estadístico del contraste (Pearson, 1900): \\[\\chi^2=\\sum_{i=1}^{k}\\frac{(n_{i}-e_{i})^2}{e_{i}}\\underset{aprox.}{\\sim }\\chi_{k-r-1}^2,\\text{ si }H_0\\text{ cierta}\\] siendo \\(k\\) el número de clases y \\(r\\) el número de parámetros estimados (para aproximar las probabilidades bajo \\(H_0\\)). Clases \\(n_{i}\\) observadas \\(p_{i}\\) bajo \\(H_0\\) \\(e_{i}\\) bajo \\(H_0\\) \\(\\frac{(n_{i}-e_{i})^2}{e_{i}}\\) \\(C_1\\) \\(n_1\\) \\(p_1\\) \\(e_1\\) \\(\\frac{(n_1-e_1)^2}{e_1}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(C_{k}\\) \\(n_{k}\\) \\(p_{k}\\) \\(e_{k}\\) \\(\\frac{(n_{k}-e_{k})^2}{e_{k}}\\) Total \\(\\sum_{i}n_{i}=n\\) \\(\\sum_{i}p_{i}=1\\) \\(\\sum_{i}e_{i}=n\\) \\(\\chi^2=\\sum_{i=1}^{k}\\frac{(n_{i}-e_{i})^2}{e_{i}}\\) Cuando \\(H_0\\) es cierta el estadístico tiende a tomar valores pequeños y grandes cuando es falsa. Por tanto se rechaza \\(H_0\\), para un nivel de significación \\(\\alpha\\), si: \\[\\sum_{i=1}^{k}\\frac{(n_{i}-e_{i})^2}{e_{i}}\\geq \\chi_{k-r-1,1-\\alpha}^2\\] Si realizamos el contraste a partir del p-valor o nivel crítico: \\[p=P\\left( {\\chi_{k-r-1}^2\\geq \\sum \\limits_{i=1}^{k}{\\frac{(n_{i}-e_{i})^2}{e_{i}}}}\\right)\\] rechazaremos \\(H_0\\) si \\(p\\leq \\alpha\\) (y cuanto menor sea se rechazará con mayor seguridad) y aceptaremos \\(H_0\\) si \\(p&gt;\\) \\(\\alpha\\) (con mayor seguridad cuanto mayor sea). Este método está implementado en la función chisq.test() para el caso discreto (no corrige los grados de libertad). Ejemplo: x &lt;- trunc(5 * runif(100)) chisq.test(table(x)) # NOT &#39;chisq.test(x)&#39;! ## ## Chi-squared test for given probabilities ## ## data: table(x) ## X-squared = 3.5, df = 4, p-value = 0.4779 La distribución exacta del estadístico del contraste es discreta (se podría aproximar por simulación, por ejemplo empleando los parámetros simulate.p.value = TRUE y B = 2000 de la función chisq.test(); ver también el Ejercicio ?? de la Sección ?? para el caso del contraste chi-cuadrado de independencia). Para que la aproximación continua \\(\\chi^2\\) sea válida: El tamaño muestral debe ser suficientemente grande (p.e. \\(n&gt;30\\)). La muestra debe ser una muestra aleatoria simple. Los parámetros deben estimarse (si es necesario) por máxima verosimilitud. Las frecuencias esperadas \\(e_{i}=n\\cdot p_{i}\\) deberían ser todas \\(\\geq5\\) (realmente esta es una restricción conservadora, la aproximación puede ser adecuada si no hay frecuencias esperadas inferiores a 1 y menos de un 20% inferiores a 5). Si la frecuencia esperada de alguna clase es \\(&lt;5\\), se suele agrupar con otra clase (o con varias si no fuese suficiente con una) para obtener una frecuencia esperada \\(\\geq5\\): Cuando la variable es nominal (no hay una ordenación lógica) se suele agrupar con la(s) que tiene(n) menor valor de \\(e_{i}\\). Si la variable es ordinal (o numérica) debe juntarse la que causó el problema con una de las adyacentes. Si la variable de interés es continua, una forma de garantizar que \\(e_{i}\\geq5\\) consiste en tomar un número de intervalos \\(k\\leq \\lfloor n/5 \\rfloor\\) y de forma que sean equiprobables \\(p_{i}=1/k\\), considerando los puntos críticos \\(x_{i/k}\\) de la distribución bajo \\(H_0\\). Por ejemplo, se podría emplear la función simres::chisq.cont.test() (fichero test.R), que imita a las incluidas en R: simres::chisq.cont.test ## function(x, distribution = &quot;norm&quot;, nclass = floor(length(x)/5), ## output = TRUE, nestpar = 0, ...) { ## # Función distribución ## q.distrib &lt;- eval(parse(text = paste(&quot;q&quot;, distribution, sep = &quot;&quot;))) ## # Puntos de corte ## q &lt;- q.distrib((1:(nclass - 1))/nclass, ...) ## tol &lt;- sqrt(.Machine$double.eps) ## xbreaks &lt;- c(min(x) - tol, q, max(x) + tol) ## # Gráficos y frecuencias ## if (output) { ## xhist &lt;- hist(x, breaks = xbreaks, freq = FALSE, ## lty = 2, border = &quot;grey50&quot;) ## # Función densidad ## d.distrib &lt;- eval(parse(text = paste(&quot;d&quot;, distribution, sep = &quot;&quot;))) ## curve(d.distrib(x, ...), add = TRUE) ## } else { ## xhist &lt;- hist(x, breaks = xbreaks, plot = FALSE) ## } ## # Cálculo estadístico y p-valor ## O &lt;- xhist$counts # Equivalente a table(cut(x, xbreaks)) pero más eficiente ## E &lt;- length(x)/nclass ## DNAME &lt;- deparse(substitute(x)) ## METHOD &lt;- &quot;Pearson&#39;s Chi-squared test&quot; ## STATISTIC &lt;- sum((O - E)^2/E) ## names(STATISTIC) &lt;- &quot;X-squared&quot; ## PARAMETER &lt;- nclass - nestpar - 1 ## names(PARAMETER) &lt;- &quot;df&quot; ## PVAL &lt;- pchisq(STATISTIC, PARAMETER, lower.tail = FALSE) ## # Preparar resultados ## classes &lt;- format(xbreaks) ## classes &lt;- paste(&quot;(&quot;, classes[-(nclass + 1)], &quot;,&quot;, classes[-1], &quot;]&quot;, ## sep = &quot;&quot;) ## RESULTS &lt;- list(classes = classes, observed = O, expected = E, ## residuals = (O - E)/sqrt(E)) ## if (output) { ## cat(&quot;\\nPearson&#39;s Chi-squared test table\\n&quot;) ## print(as.data.frame(RESULTS)) ## } ## if (any(E &lt; 5)) ## warning(&quot;Chi-squared approximation may be incorrect&quot;) ## structure(c(list(statistic = STATISTIC, parameter = PARAMETER, p.value = PVAL, ## method = METHOD, data.name = DNAME), RESULTS), class = &quot;htest&quot;) ## } ## &lt;bytecode: 0x0000000013587770&gt; ## &lt;environment: namespace:simres&gt; Continuando con el ejemplo anterior, podríamos contrastar normalidad mediante: chisq.cont.test(datos, distribution = &quot;norm&quot;, nestpar = 2, mean=mean(datos), sd=sd(datos)) ## ## Pearson&#39;s Chi-squared test table ## classes observed expected residuals ## 1 ( 9.06000,14.49908] 6 5.125 0.3865103 ## 2 (14.49908,16.94725] 3 5.125 -0.9386680 ## 3 (16.94725,18.77800] 4 5.125 -0.4969419 ## 4 (18.77800,20.41732] 6 5.125 0.3865103 ## 5 (20.41732,22.05663] 4 5.125 -0.4969419 ## 6 (22.05663,23.88739] 8 5.125 1.2699625 ## 7 (23.88739,26.33556] 4 5.125 -0.4969419 ## 8 (26.33556,30.77000] 6 5.125 0.3865103 ## ## Pearson&#39;s Chi-squared test ## ## data: datos ## X-squared = 3.6829, df = 5, p-value = 0.5959 B.1.5 Contraste de Kolmogorov-Smirnov Se trata de un contraste de bondad de ajuste diseñado para distribuciones continuas (similar a la prueba de Cramer-von Mises o a la de Anderson-Darling, implementadas en el paquete goftest de R, que son en principio mejores). Se basa en comparar la función de distribución \\(F_0\\) bajo \\(H_0\\) con la función de distribución empírica \\(F_n\\): \\[\\begin{aligned} &amp; D_n=\\sup_{x}|F_n(x)-F_0(x)|,\\\\ &amp; = \\max_{1 \\leq i \\leq n} \\left\\{ |F_n(X_{(i)})-F_0(X_{(i)})|,|F_n(X_{(i-1)})-F_0(X_{(i)})| \\right\\} \\end{aligned}\\] Teniendo en cuenta que \\(F_n\\left( X_{(i)}\\right) = \\frac{i}n\\): \\[\\begin{aligned} D_n &amp; =\\max_{1\\leq i\\leq n}\\left \\{ \\frac{i}n-F_0(X_{(i)}),F_0(X_{(i)})-\\frac{i-1}n\\right \\} \\\\ &amp; =\\max_{1\\leq i\\leq n}\\left \\{ D_{n,i}^{+},D_{n,i}^{-}\\right \\} \\end{aligned}\\] Si \\(H_0\\) es simple y \\(F_0\\) es continua, la distribución del estadístico \\(D_n\\) bajo \\(H_0\\) no depende \\(F_0\\) (es de distribución libre). Esta distribución está tabulada (para tamaños muestrales grandes se utiliza la aproximación asintótica). Se rechaza \\(H_0\\) si el valor observado \\(d\\) del estadístico es significativamente grande: \\[p = P \\left( D_n \\geq d \\right) \\leq \\alpha.\\] Este método está implementado en la función ks.test() del paquete base de R: ks.test(x, y, ...) donde x es un vector que contiene los datos, y es una función de distribución (o una cadena de texto que la especifica; también puede ser otro vector de datos para el contraste de dos muestras) y ... representa los parámetros de la distribución. Continuando con el ejemplo anterior, para contrastar \\(H_0:F= \\mathcal{N}(20,5^2)\\) podríamos emplear: ks.test(datos, pnorm, mean = 20, sd = 5) # One-sample ## ## One-sample Kolmogorov-Smirnov test ## ## data: datos ## D = 0.13239, p-value = 0.4688 ## alternative hypothesis: two-sided Si \\(H_0\\) es compuesta, el procedimiento habitual es estimar los parámetros desconocidos por máxima verosimilitud y emplear \\(\\hat{F}_0\\) en lugar de \\(F_0\\). Sin embargo, al proceder de esta forma es de esperar que \\(\\hat{F}_0\\) se aproxime más que \\(F_0\\) a la distribución empírica, por lo que los cuantiles de la distribución de \\(D_n\\) pueden ser demasiado conservativos (los \\(p\\)-valores tenderán a ser mayores de lo que deberían) y se tenderá a aceptar la hipótesis nula (puede ser preferible aproximar el \\(p\\)-valor mediante simulación; como se muestra en el Ejercicio ?? de la Sección ??). Para evitar este problema, en el caso de contrastar normalidad se desarrolló el test de Lilliefors, implementado en la función lillie.test() del paquete nortest (también hay versiones en este paquete para los métodos de Cramer-von Mises y Anderson-Darling). Por ejemplo: ks.test(datos, pnorm, mean(datos), sd(datos)) # One-sample Kolmogorov-Smirnov test ## ## One-sample Kolmogorov-Smirnov test ## ## data: datos ## D = 0.097809, p-value = 0.8277 ## alternative hypothesis: two-sided library(nortest) lillie.test(datos) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: datos ## D = 0.097809, p-value = 0.4162 Típicamente \\(\\left \\{ p_{i}=\\frac{\\left(i-0.5 \\right)}n : i=1, \\cdots, n \\right\\}\\). "],["diag-aleat.html", "B.2 Diagnosis de la independencia", " B.2 Diagnosis de la independencia Los métodos clásicos de inferencia estadística se basan en suponer que las observaciones \\(X_{1},\\ldots,X_{n}\\) son una muestra aleatoria simple (m.a.s.) de \\(X\\). Por tanto suponen que las observaciones son independientes (o los errores, en el caso de un modelo de regresión). La ausencia de aleatoriedad es difícil de corregir y puede influir notablemente en el análisis estadístico. Si existe dependencia entre las observaciones muestrales (e.g. el conocimiento de \\(X_{i}\\) proporciona información sobre los valores de \\(X_{i+1}\\), \\(X_{i+2}\\), \\(\\ldots\\)), los métodos clásicos no serán en principio adecuados (pueden conducir a conclusiones erróneas). Esto es debido principalmente a que introduce un sesgo en los estimadores de las varianzas (diseñados asumiendo independencia). Los correspondientes intervalos de confianza y contrastes de hipótesis tendrán una confianza o una potencia distinta de la que deberían (aunque las estimaciones de los parámetros pueden no verse muy afectadas). Si \\(X_{1}\\) e \\(X_{2}\\) son independientes (\\(Cov(X_{1},X_{2})=0\\)): \\[Var(X_{1}+X_{2})=Var(X_{1})+Var(X_{2})\\] En el caso general (dependencia): \\[Var(X_{1}+X_{2})=Var(X_{1})+Var(X_{2})+2Cov(X_{1},X_{2})\\] Típicamente \\(Cov(X_{1},X_{2})&gt;0\\) por lo que con los métodos clásicos (basados en independencia) se suelen producir subestimaciones de las varianzas (IC más estrechos y tendencia a rechazar \\(H_{0}\\) en contrastes). Ejemplo: datos simulados Consideramos un proceso temporal estacionario con dependencia exponencial (la dependencia entre las observaciones depende del salto entre ellas; ver Ejemplo ?? en la Sección ??). n &lt;- 100 # Nº de observaciones t &lt;- seq(0, 1, length = n) mu &lt;- rep(0, n) # Media # mu &lt;- 0.25 + 0.5*t # mu &lt;- sin(2*pi*t) # Matriz de covarianzas t.dist &lt;- as.matrix(dist(t)) t.cov &lt;- exp(-t.dist) # str(t.cov) # num [1:100, 1:100] 1 0.99 0.98 0.97 0.96 ... # Simulación de las observaciones set.seed(1) library(MASS) z &lt;- rnorm(n) x1 &lt;- mu + z # Datos independientes x2 &lt;- mvrnorm(1, mu, t.cov) # Datos dependientes plot(t, mu, type=&quot;l&quot;, lwd = 2, ylim = c(-3,3), ylab = &#39;x&#39;) lines(t, x1, col = &#39;blue&#39;) lines(t, x2, col = &#39;red&#39;) legend(&quot;bottomright&quot;, legend = c(&quot;Datos independientes&quot;, &quot;Datos dependientes&quot;), col = c(&#39;blue&#39;, &#39;red&#39;), lty = 1) En el caso anterior la varianza es uno con ambos procesos. Las estimaciones suponiendo independencia serían: var(x1) ## [1] 0.8067621 var(x2) ## [1] 0.1108155 En el caso de datos dependientes se produce una clara subestimación de la varianza B.2.1 Métodos para detectar dependencia Es de esperar que datos cercanos en el tiempo (o en el espacio) sean más parecidos (dependientes) que datos más alejados, hablaríamos entonces de dependencia temporal (espacial o espacio-temporal). En esta sección nos centraremos en el caso de dependencia temporal (unidimensional). Entre los métodos para detectar este tipo de dependencia destacaríamos: Gráficos: Secuencial / Dispersión frente al tiempo Dispersión retardado Correlograma Contrastes: Tests basados en rachas Test de Ljung-Box B.2.2 Gráfico secuencial El gráfico de dispersión \\(\\{(i,X_{i}) : i = 1, \\ldots, n \\}\\) permite detectar la presencia de un efecto temporal (en la tendencia o en la variabilidad). Es importante mantener/guardar el orden de recogida de los datos. Si existe una tendencia los datos no son homogéneos (debería tenerse en cuenta la variable índice, o tiempo, como variable explicativa). Podría indicar la presencia de un efecto aprendizaje. Comandos R: plot(as.ts(x)) Ejemplo: old.par &lt;- par(mfrow = c(1, 2)) plot(datos, type = &#39;l&#39;) plot(as.ts(datos)) Figura B.3: Ejemplos de gráficos secuenciales. par(old.par) Es habitual que este tipo de análisis se realice sobre los residuos de un modelo de regresión (e.g. datos &lt;- residuals(modelo)) Este gráfico también podría servir para detectar dependencia temporal: Valores próximos muy parecidos (valores grandes seguidos de grandes y viceversa) indicarían una posible dependencia positiva. Valores próximos dispares (valores grandes seguidos de pequeños y viceversa) indicarían una posible dependencia negativa. old.par &lt;- par(mfrow = c(1, 3)) plot(x2, type = &#39;l&#39;, ylab = &#39;&#39;, main = &#39;Dependencia positiva&#39;) plot(x1, type = &#39;l&#39;, ylab = &#39;&#39;, main = &#39;Independencia&#39;) x3 &lt;- x2 * c(1, -1) plot(x3, type = &#39;l&#39;, ylab = &#39;&#39;, main = &#39;Dependencia negativa&#39;) par(old.par) pero suele ser preferible emplear un gráfico de dispersión retardado. B.2.3 Gráfico de dispersion retardado El gráfico de dispersión \\(\\{(X_{i},X_{i+1}) : i = 1, \\ldots, n-1 \\}\\) permite detectar dependencias a un retardo (relaciones entre valores separados por un instante) Comando R:plot(x[-length(x)], x[-1], xlab = \"X_t\", ylab = \"X_t+1\") old.par &lt;- par(mfrow = c(1, 3)) plot(x2[-length(x2)], x2[-1], xlab = &quot;X_t&quot;, ylab = &quot;X_t+1&quot;, main = &#39;Dependencia positiva&#39;) plot(x1[-length(x1)], x1[-1], xlab = &quot;X_t&quot;, ylab = &quot;X_t+1&quot;, main = &#39;Independencia&#39;) plot(x3[-length(x3)], x3[-1], xlab = &quot;X_t&quot;, ylab = &quot;X_t+1&quot;, main = &#39;Dependencia negativa&#39;) par(old.par) Se puede generalizar al gráfico \\(\\{(X_{i},X_{i+k}) : i = 1, \\ldots, n-k \\}\\) que permite detectar dependencias a \\(k\\) retardos (separadas \\(k\\) instantes). Ejemplo # Gráfico de dispersion retardado plot(datos[-length(datos)], datos[-1], xlab = &quot;X_t&quot;, ylab = &quot;X_t+1&quot;) El correspondiente coeficiente de correlación es una medida numérica del grado de relación lineal (denominado autocorrelación de orden 1). cor(datos[-length(datos)], datos[-1]) ## [1] 0.01344127 Ejemplo: Calidad de un generador aleatorio En el caso de una secuencia muy grande de número pseudoaleatorios (supuestamente independientes), sería muy dificil distinguir un patrón a partir del gráfico anterior. La recomendación en R sería utilizar puntos con color de relleno: plot(u[-length(u)], u[-1], xlab=&quot;U_t&quot;, ylab=&quot;U_t+1&quot;, pch=21, bg=&quot;white&quot;) Figura B.4: Ejemplos de gráficos de dispensión retardados de dos secuencias de longitud 10000. Si se observa algún tipo de patrón indicaría dependencia (se podría considerar como una versión descriptiva del denominado Parking lot test). Se puede generalizar también a \\(d\\)-uplas \\((X_{t+1},X_{t+2},\\ldots,X_{t+d})\\) (ver ejemplo del generador RANDU en Figura 2.1 de la Sección 2.1). B.2.4 El correlograma Para estudiar si el grado de relación (lineal) entre \\(X_{i}\\) e \\(X_{i+k}\\) podemos utilizar el coeficiente de correlación: \\[\\rho\\left( X_{i},X_{i+k}\\right) = \\frac{Cov\\left( X_{i},X_{i+k}\\right) } {\\sigma\\left( X_{i}\\right) \\sigma\\left( X_{i+k}\\right) }\\] En el caso de datos homogéneos (estacionarios) la correlación sería función únicamente del salto: \\[\\rho\\left( X_{i},X_{i+k}\\right) \\equiv\\rho\\left( k\\right)\\] denominada función de autocorrelación simple (fas) o correlograma. Su estimador es el correlograma muestral: \\[r(k)=\\frac{\\sum_{i=1}^{n-k}(X_{i}-\\overline{X})(X_{i+k}-\\overline{X})} {\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\] Comando R:acf(x) En caso de independencia es de esperar que las autocorrelaciones muestrales sean próximas a cero (valores grandes indicarían dependencia positiva o negativa según el signo). old.par &lt;- par(mfrow = c(1, 3)) acf(x1, main = &#39;Independencia&#39;) acf(x2, main = &#39;Dependencia positiva&#39;) acf(x3, main = &#39;Dependencia negativa&#39;) par(old.par) Suponiendo normalidad e independencia, asintóticamente: \\[r(k)\\underset{aprox.}{\\sim}N\\left( \\rho(k),\\frac{1}{n}\\right)\\] Si el tamaño muestral es grande, podríamos aceptar \\(H_{0}:\\) \\(\\rho\\left( k\\right) = 0\\) si:\\[|r(k)|&lt;\\dfrac{2}{\\sqrt{n}}\\] En el gráfico de autocorrelaciones muestrales (también denominado correlograma) se representan las estimaciones \\(r(k)\\) de las autocorrelaciones correspondientes a los primeros retardos (típicamente \\(k&lt;n/4\\)) y las correspondientes bandas de confianza (para detectar dependencias significativas). Ejemplo acf(datos) # correlaciones La función acf también permite estimar el covariograma18. covar &lt;- acf(x2, type = &quot;covariance&quot;) B.2.5 Test de rachas Permite contrastar si el orden de aparición de dos valores de una variable dicotómica es aleatorio. Supongamos que \\(X\\) toma los valores \\(+\\) y \\(-\\) y que observamos una muestra del tipo: \\[++++---+++--++++++----\\] y nos interesa contrastar: \\[\\left\\{ \\begin{array}[c]{l} H_{0}:\\mathit{La\\ muestra\\ es\\ aleatoria}\\\\ H_{1}:\\mathit{La\\ muestra\\ no\\ es\\ aleatoria} \\end{array} \\right.\\] Una racha es una secuencia de observaciones iguales (o similares): \\[\\underbrace{++++}_{1}\\underbrace{---}_{2}\\underbrace{+++}_{3} \\underbrace{--}_{4}\\underbrace{++++++}_{5}\\underbrace{----}_{6}\\] Una muestra con muchas o pocas rachas sugeriría que la muestra no es aleatoria (con dependencia negativa o positiva, respec.). Estadístico del contraste: \\[R=\\text{&quot;Número total de rachas en la muestra&quot;}\\] Bajo la hipótesis nula de aleatoriedad: \\[R\\underset{aprox.}{\\sim}N\\left( 1+\\frac{2n_{1}n_{2}}{n}, \\frac{2n_{1}n_{2}(2n_{1}n_{2}-n)}{n^{2}(n-1)}\\right)\\] siendo \\(n_{1}\\) y \\(n_{2}\\) el número de signos \\(+\\) y \\(-\\) en la muestra, respectivamente (\\(n_{1}+n_{2}=n\\)). Para tamaños muéstrales pequeños (\\(n&lt;40\\)), esta aproximación no es buena y conviene utilizar la distribución exacta (o utilizar corrección por continuidad). Los valores críticos de esta distribución están tabulados. Este contraste se emplea también para variables continuas, se fija un punto de corte para dicotomizarlas. Normalmente se toma como punto de corte la mediana. En este caso si \\(k=n_{1}\\) (\\(\\simeq n_{2}\\)): \\[R\\underset{aprox.}{\\sim}N\\left( k+1,\\frac{k(k-1)}{2k-1}\\right)\\] Se rechaza la hipótesis nula de aleatoriedad si el número de rachas es significativamente pequeño o grande. Si el tamaño muestral es grande, el \\(p\\)-valor será: \\[p \\simeq 2 P\\left( Z \\geq \\left\\vert \\frac{R-E(R)}{\\sqrt{Var(R)}} \\right\\vert \\right)\\] Comandos R: tseries::runs.test(as.factor(x &gt; median(x))) Ejemplo library(tseries) runs.test(as.factor(datos &gt; median(datos))) ## ## Runs Test ## ## data: as.factor(datos &gt; median(datos)) ## Standard Normal = -0.4422, p-value = 0.6583 ## alternative hypothesis: two.sided Alternativamente, para evitar el cálculo del punto de corte (la mediana), requerido para dicotomizar la variable continua, se podría emplear una modificación de este contraste, el denominado test de rachas ascendentes y descendentes, en el que se generan los valores \\(+\\) y \\(-\\) dependiendo de si el valor de la secuencia es mayor o menor que el anterior (ver e.g. Downham, 1970). Este contraste es más adecuado para generadores aleatorios. B.2.6 El contraste de Ljung-Box Es un test muy utilizado (en series de tiempo) para contrastar la hipótesis de independencia. Se contrasta la hipótesis nula de que las primeras \\(m\\) autocorrelaciones son cero: \\[\\left\\{\\begin{array}[c]{l} H_{0}:\\rho_{1}=\\rho_{2}=\\ldots=\\rho_{m}=0\\\\ H_{1}:\\rho_{i}\\neq0\\text{ para algún } i \\end{array} \\right.\\] Se elige un \\(m\\) tal que la estimación \\(r(m)\\) de \\(\\rho_{m}=\\rho(m)\\) sea fiable (e.g. \\(10\\log_{10}n\\)). El estadístico del contraste: \\[Q=n(n+2)\\sum_{k=1}^{m}\\frac{r(k)^{2}}{n-k}\\underset{aprox.}{\\sim}\\chi _{m}^{2}\\text{, si }H_{0}\\text{ es cierta.}\\] Se rechaza \\(H_{0}\\) si el valor observado es grande (\\(Q\\geq \\chi_{m,1-\\alpha}^{2}\\)): \\[p=P\\left( {\\chi_{m}^{2}}\\geq Q\\right)\\] Comandos R: Box.test(x, type=Ljung) Box.test(x, lag, type=Ljung) Ejemplo Box.test(datos, type=&quot;Ljung&quot;) # Contrasta si la primera autocorrelación es nula ## ## Box-Ljung test ## ## data: datos ## X-squared = 0.0078317, df = 1, p-value = 0.9295 Box.test(datos, lag=5, type=&quot;Ljung&quot;) # Contrasta si las 5 primeras autocorrelaciones son nulas ## ## Box-Ljung test ## ## data: datos ## X-squared = 1.2556, df = 5, p-value = 0.9394 NOTA: Cuando se trabaja con residuos de un modelo lineal, para contrastar que la primera autocorrelación es cero, es preferible emplear el test de Durbin-Watson implementado en la función dwtest() del paquete lmtest. En algunos campos, como en estadística espacial, en lugar del covariograma se suele emplear el semivariograma \\(\\gamma(k) = C(0) - C(k)\\). "],["contrastes-específicos-para-generadores-aleatorios.html", "B.3 Contrastes específicos para generadores aleatorios", " B.3 Contrastes específicos para generadores aleatorios Los contrastes generales anteriores pueden ser muy poco adecuados para testear generadores de números pseudoaleatorios (ver e.g. LEcuyer y Simard, 2007). Por ese motivo se han desarrollado contrastes específicos, principalmente con el objetivo de encontrar un generador con buenas propiedades criptográficas. Muchos de estos contrastes están basados en la prueba chi-cuadrado y trabajan con enteros en lugar de los valores uniformes. El procedimiento habitual consiste en fijar un entero positivo \\(K\\), y discretizar los valores uniformes \\(U_{1},U_{2},\\ldots,U_{n}\\), de la forma: \\[X_i = \\left\\lfloor K\\cdot U_{i}\\right\\rfloor + 1 ,\\] donde \\(\\left\\lfloor u\\right\\rfloor\\) denota la parte entera de \\(u\\). De esta forma se consigue una sucesión de enteros aleatorios supuestamente independientes con distribución uniforme en \\(\\{1, \\ldots, K\\}\\). En esta sección se describirán algunos de los métodos tradicionales en este campo con fines ilustrativos. Si realmente el objetivo es diagnosticar la calidad de un generador, la recomendación sería emplear las baterías de contrastes más recientes descritas en la Sección 2.3.2. B.3.1 Contraste de frecuencias Empleando la discretización anterior se simplifica notablemente el contraste chi-cuadrado de bondad de ajuste a una uniforme, descrito en la Sección B.1.4 e implementado en la función chisq.cont.test(). En este caso bastaría con contrastar la equiprobabilidad de la secuencia de enteros (empleando directamente la función chisq.test()) y este método de denomina contraste de frecuencias (frequency test). Por ejemplo: # Generar set.seed(1) u &lt;- runif(1000) # Discretizar k &lt;- 10 x &lt;- floor(k*u) + 1 # Test chi-cuadrado f &lt;- table(factor(x, levels = seq_len(k))) chisq.test(f) ## ## Chi-squared test for given probabilities ## ## data: f ## X-squared = 10.26, df = 9, p-value = 0.3298 Este código está implementado en la función simres::freq.test() (fichero test.R) y podríamos emplear: library(simres) freq.test(u, nclass = k) # Alternativamente # chisq.cont.test(u, distribution = &quot;unif&quot;, nclass = k, output = FALSE, min = 0, max = 1) B.3.2 Contraste de series El contraste anterior se puede generalizar a contrastar la uniformidad de las \\(d\\)-uplas \\((X_{t+1},X_{t+2},\\ldots,X_{t+d})\\) con \\(t=(i-1)d\\), \\(i=1,\\ldots,m\\) siendo \\(m=\\lfloor n/d \\rfloor\\). La idea es que troceamos el hipercubo \\([0, 1]^d\\) en \\(K^d\\) celdas equiprobables. Considerando como categorías todos los posibles valores de las \\(d\\)-uplas, podemos emplear el estadístico chi-cuadrado para medir la discrepancia entre las frecuencias observadas y las esperadas, iguales todas a \\(\\frac{m}{K^d}\\). Las elecciones más frecuentes son \\(d=2\\) (contraste de pares seriados) y \\(K=8\\), \\(10\\) ó \\(20\\). Por ejemplo, la función serial.test() del paquete randtoolbox implementa este contraste para \\(d=2\\). Para que la prueba chi-cuadrado sea fiable el valor de \\(n\\) debería ser grande en comparación con el número de categorías \\(K^d\\) (e.g. \\(n \\geq 5dK^d\\)). Si se considera un valor \\(d \\geq 3\\) puede ser necesario reducir considerablemente el valor de \\(K\\) para evitar considerar demasiadas categorías. Alternativamente se podrían considerar distintas técnicas para agrupar estas categorías, por ejemplo como se hace en el contraste del poker o del coleccionista descritos a continuación. B.3.3 El contraste del poker En el contrate del poker clásico se consideran conjuntos sucesivos de cinco enteros (\\(d=5\\)) y, para cada uno, se determina cuál de las siguientes posibilidades se da: Un mismo entero se repite cinco veces (abreviadamente, \\(AAAAA\\)). Un mismo entero se repite cuatro veces y otro distinto aparece una vez (\\(AAAAB\\)). Un entero se repite tres veces y otro distinto se repite dos (\\(AAABB\\)). Un entero se repite tres veces y otros dos distintos aparecen una vez cada uno (\\(AAABC\\)). Un entero se repite dos veces, otro distinto se repite también dos veces y un tercer entero diferente aparece una sóla vez (\\(AABBC\\)). Un entero se repite dos veces y otros tres distintos aparecen una vez cada uno (\\(AABCD\\)). Los cinco enteros que aparecen son todos distintos (\\(ABCDE\\)). Bajo las hipótesis de aleatoriedad y uniformidad, se pueden calcular las probabilidades de estas modalidades. Por ejemplo para \\(K=10\\) obtendríamos: \\[\\begin{aligned} P(AAAAA) &amp; =0.0001, P(AAAAB)=0.0045, P(AAABB)=0.0090,\\\\ P(AAABC) &amp; =0.0720, P(AABBC)=0.1080, P(AABCD)=0.5040,\\\\ P(ABCDE) &amp; =0.3024. \\end{aligned}\\] Es frecuente que las clases \\(AAAAA\\) y \\(AAAAB\\) se agrupen a la hora de aplicar el test chi-cuadrado, ya que, en caso contrario, la restricción habitual \\(e_{i}\\geq5\\) llevaría a que \\(0.0001\\cdot\\frac{n}{5}\\geq5\\), es decir, \\(n\\geq250\\,000\\). Es habitual simplificar el contraste anterior para facilitar su implementación definiendo las categorías según el número de enteros distintos de entre los cinco observados. Así obtendríamos: \\[\\begin{aligned} P(\\text{1 entero diferente}) &amp; = 0.0001, P(\\text{2 enteros diferentes}) = 0.0135,\\\\ P(\\text{3 enteros diferentes}) &amp; = 0.1800, P(\\text{4 enteros diferentes}) = 0.5040,\\\\ P(\\text{5 enteros diferentes}) &amp; = 0.3024, \\end{aligned}\\] procediendo también a agrupar las dos primeras modalidades. En el caso general de considerar \\(d\\)-uplas (manos de \\(d\\) cartas con \\(K\\) posibilidades cada una), la probabilidad de obtener \\(c\\) valores (cartas) diferentes es (e.g. Knuth, 2002, Sección 3.3.2, p. 64): \\[P(C=c) = \\frac{K!}{(K-c)!K^d}S(d,c),\\] donde \\(S(d,c)\\) es el número de Stirling de segunda clase, definido como el número de formas que existen de hacer una partición de un conjunto de \\(d\\) elementos en \\(c\\) subconjuntos: \\[S(d,c) = \\frac{1}{c!}\\sum_{i=0}^{c} (-1)^{i} \\binom{c}{i} (c-i)^d.\\] Por ejemplo, la función poker.test() del paquete randtoolbox implementa este contraste para el caso de \\(d=K\\). B.3.4 El contraste del coleccionista Por simplicidad describiremos el caso de \\(d=1\\) (con \\(K\\) categorías). Considerando la sucesión de enteros aleatorios se procede (como un coleccionista) a contabilizar cuál es el número, \\(Q\\), (aleatorio) de valores consecutivos hasta que se completa la colección de todos los enteros entre \\(1\\) y \\(K\\). Obviamente, bajo las hipótesis de aleatoriedad y uniformidad, cada posible entero entre \\(1\\) y \\(K\\) tiene la misma probabilidad de aparecer en cada generación y, por tanto, resulta posible calcular la distribución de probabilidad de \\(Q\\). De esta forma podemos utilizar los valores calculados de las probabilidades \\[P( Q = K ), P( Q = K+1 ),\\ldots, P( Q = M -1 ), P( Q \\geq M ),\\] para obtener las frecuencias esperadas de cada clase y confrontarlas con las observadas vía el estadístico chi-cuadrado (e.g. Knuth, 2002, Sección 3.3.2, p. 65). Existen varias elecciones comunes de \\(K\\) y \\(M\\). Tomando \\(K=5\\) con clases \\(Q=5\\), \\(Q=6\\), \\(\\ldots\\), \\(Q=19\\), \\(Q\\geq20\\), las probabilidades vendrían dadas por: \\[\\begin{array}{rr} P( Q = 5 ) = 0.03840000, \\ &amp; P( Q = 6 ) = 0.07680000,\\\\ P( Q = 7 ) = 0.09984000, \\ &amp; P( Q = 8 ) = 0.10752000,\\\\ P( Q = 9 ) = 0.10450944, \\ &amp; P( Q = 10 ) = 0.09547776,\\\\ P( Q = 11 ) = 0.08381645, \\ &amp; P( Q = 12 ) = 0.07163904,\\\\ P( Q = 13 ) = 0.06011299, \\ &amp; P( Q = 14 ) = 0.04979157,\\\\ P( Q = 15 ) = 0.04086200, \\ &amp; P( Q = 16 ) = 0.03331007,\\\\ P( Q = 17 ) = 0.02702163, \\ &amp; P( Q = 18 ) = 0.02184196,\\\\ P( Q = 19 ) = 0.01760857, \\ &amp; P( Q\\geq20 ) = 0.07144851. \\end{array}\\] Para \\(K=10\\) se podrían considerar las siguientes categorías (con sus correspondientes probabilidades): \\[\\begin{array}{rr} P( 10 \\leq Q \\leq 19 ) = 0.17321155, \\ &amp; P( 20 \\leq Q \\leq 23 ) = 0.17492380,\\\\ P( 24 \\leq Q \\leq 27 ) = 0.17150818, \\ &amp; P( 28 \\leq Q \\leq 32 ) = 0.17134210,\\\\ P( 33 \\leq Q \\leq 39 ) = 0.15216056, \\ &amp; P( Q \\geq 40 ) =0.15685380. \\end{array}\\] "],["int-num.html", "C Integración numérica", " C Integración numérica En muchos casos nos puede interesar la aproximación de una integral definida. En estadística, además del caso de Inferencia Bayesiana (que se trató en el Capítulo 11 empleando Integración Monte Carlo y MCMC), nos puede interesar por ejemplo aproximar mediante simulación el error cuadrático integrado medio (MISE) de un estimador. Por ejemplo, en el caso de una densidad univariante sería de la forma: \\[MISE\\left( \\hat{f} \\right) =\\int E\\left[ \\left( \\hat{f}(x) - f(x) \\right)^2\\right] dx\\] Cuando el numero de dimensiones es pequeño, nos puede interesar emplear un método numérico para aproximar este tipo de integrales. "],["integración-numérica-unidimensional.html", "C.1 Integración numérica unidimensional", " C.1 Integración numérica unidimensional Supongamos que nos interesa aproximar una integral de la forma: \\[I = \\int_a^b h(x) dx.\\]. Consideraremos como ejemplo: \\[\\int_0^1 4x^4 dx = \\frac{4}{5}\\]. fun &lt;- function(x) return(4 * x^4) curve(fun, 0, 1) abline(h = 0, lty = 2) abline(v = c(0, 1), lty = 2) C.1.1 Método del trapezoide La regla de los trapecios es una forma de aproximar la integral utilizando \\(n\\) trapecios. Si se consideran \\(n\\) subintervalos en \\([a,b]\\) de longitud \\(h= \\frac{b-a}{n}\\) (i.e. \\(n + 1\\) puntos regularmente espaciados cubriendo el dominio), y se aproxima linealmente la función en cada subintervalo, se obtiene que: \\[\\int_a^b f(x)\\, dx \\approx \\frac{h}{2} [f(a)+2f(a+h)+2f(a+2h)+...+f(b)]\\] trapezoid.vec &lt;- function(f.vec, h = 0.01) { # Integración numérica unidimensional entre a y b # utilizando el método del trapezoide # (se aproxima f linealmente en cada intervalo) n &lt;- length(f.vec) return(h*(f.vec[1]/2 + sum(f.vec[2:(n-1)]) + f.vec[n]/2)) } trapezoid &lt;- function(fun, a = 0, b = 1, n = 100) { # Integración numérica de fun (función unidimensional) entre a y b # utilizando el método del trapezoide con n subdivisiones # (se aproxima f linealmente en cada intervalo) # Se asume a &lt; b y n entero positivo h &lt;- (b-a)/n x.vec &lt;- seq(a, b, by = h) f.vec &lt;- sapply(x.vec, fun) return(trapezoid.vec(f.vec, h)) } trapezoid(fun, 0, 1, 20) ## [1] 0.8033325 El error en esta aproximación se corresponde con: \\[ \\frac{(b-a)^3}{12n^2}\\,f&#39;&#39;(\\xi), \\] para algún \\(a\\leq \\xi \\leq b\\) (dependiendo del signo de la segunda derivada, i.e. de si la función es cóncava o convexa, el error será negativo ó positivo). El error máximo absoluto es \\(\\frac{(b-a)^3}{12n^2}\\max_{a\\leq \\xi \\leq b}\\left|f&#39;&#39;(\\xi)\\right|\\). En el caso general multidimensional sería \\(O(n^{-\\frac{2}{d}})\\). C.1.2 Regla de Simpson Se divide el intervalo \\(n\\) subintervalos de longitud \\(h= \\frac{b-a}{n}\\) (con \\(n\\) par), considerando \\(n + 1\\) puntos regularmente espaciados \\(x_i = a + ih\\), para \\(i = 0, 1, ..., n\\). Aproximando de forma cuadrática la función en cada subintervalo \\([x_{j-1},x_{j+1}]\\) (considerando 3 puntos), se obtiene que: \\[ \\int_a^b f(x) \\, dx \\approx \\frac{h}{3} \\bigg[ f(x_0)+2\\sum_{j=1}^{(n/2)-1}f(x_{2j})+ 4\\sum_{j=1}^{n/2}f(x_{2j-1})+f(x_n) \\bigg],\\] simpson &lt;- function(fun, a, b, n = 100) { # Integración numérica de fnt entre a y b # utilizando la regla de Simpson con n subdivisiones # (se aproxima fun de forma cuadrática en cada par de intervalos) # fnt es una función de una sola variable # Se asume a &lt; b y n entero positivo par n &lt;- max(c(2*(n %/% 2), 4)) h &lt;- (b-a)/n x.vec1 &lt;- seq(a+h, b-h, by = 2*h) x.vec2 &lt;- seq(a+2*h, b-2*h, by = 2*h) f.vec1 &lt;- sapply(x.vec1, fun) f.vec2 &lt;- sapply(x.vec2, fun) return(h/3*(fun(a) + fun(b) + 4*sum(f.vec1) + 2*sum(f.vec2))) # Una cota del error en valor absoluto es: # h^4*(b-a)*max(c(f.vec1, fvec.2))^4/180. } simpson(fun, 0, 1, 20) ## [1] 0.8000033 El máximo error (en el caso unidimensional) viene dado por la expresión: \\[\\frac{(b-a)^5}{180n^4}\\,\\max_{a\\leq \\xi \\leq b}\\left| f^{(4)}(\\xi) \\right|.\\] En el caso general multidimensional sería \\(O(n^{-\\frac{4}{d}})\\). C.1.3 Cuadratura adaptativa En lugar de evaluar la función en una rejilla regular (muestrear por igual el dominio), puede interesar ir añadiendo puntos sólo en los lugares donde se mejore la aproximación (en principio donde hay mayor área). quadrature &lt;- function(fun, a, b, tol=1e-8) { # numerical integration using adaptive quadrature simpson2 &lt;- function(fun, a, b) { # numerical integral using Simpson&#39;s rule # assume a &lt; b and n = 2 return((b-a)/6 * (fun(a) + 4*fun((a+b)/2) + fun(b))) } quadrature_internal &lt;- function(S.old, fun, a, m, b, tol, level) { level.max &lt;- 100 if (level &gt; level.max) { cat (&quot;recursion limit reached: singularity likely\\n&quot;) return (NULL) } S.left &lt;- simpson2(fun, a, m) S.right &lt;- simpson2(fun, m, b) S.new &lt;- S.left + S.right if (abs(S.new-S.old) &gt; tol) { S.left &lt;- quadrature_internal(S.left, fun, a, (a+m)/2, m, tol/2, level+1) S.right &lt;- quadrature_internal(S.right, fun, m, (m+b)/2, b, tol/2, level+1) S.new &lt;- S.left + S.right } return(S.new) } level = 1 S.old &lt;- (b-a) * (fun(a) + fun(b))/2 S.new &lt;- quadrature_internal(S.old, fun, a, (a+b)/2, b, tol, level+1) return(S.new) } quadrature(fun, 0, 1) ## [1] 0.8 Fuente: r-blogger Guangchuang Yu C.1.4 Comandos de R integrate(fun, 0, 1) # Permite límites infinitos ## 0.8 with absolute error &lt; 8.9e-15 ## Cuidado: fun debe ser vectorial... require(MASS) area(fun, 0, 1) ## [1] 0.8000001 "],["integración-numérica-bidimensional.html", "C.2 Integración numérica bidimensional", " C.2 Integración numérica bidimensional Supongamos que nos interesa aproximar una integral de la forma: \\[I=\\int_{a_x}^{b_x}\\int_{a_y}^{b_y}f(x, y)dy dx\\]. Consideraremos como ejemplo: \\[\\int_{-1}^{1} \\int_{-1}^{1} \\left( x^2 - y^2 \\right) dx dy = 0\\]. f2d &lt;- function(x,y) x^2 - y^2 Es habitual (especialmente en simulación) que la función se evalúe en una rejilla: ax = -1 ay = -1 bx = 1 by = 1 nx = 21 ny = 21 x &lt;- seq(ax, bx, length = nx) y &lt;- seq(ay, by, length = ny) z &lt;- outer(x, y, f2d) hx &lt;- x[2]-x[1] hy &lt;- y[2]-y[1] C.2.1 Representación gráfica Puede ser de utilidad las herramientas de los paquetes plot3D y plot3Drgl (también se pueden utilizar las funciones spersp, simage, spoints y splot del paquete npsp). if(!require(plot3D)) stop(&#39;Required pakage `plot3D` not installed.&#39;) # persp3D(z = z, x = x, y = y) persp3D.f2d &lt;- function(f2d, ax=-1, bx=1, ay=-1, by=1, nx=21, ny=21, ...) { x &lt;- seq(ax, bx, length = nx) y &lt;- seq(ay, by, length = ny) z &lt;- outer(x, y, f2d) persp3D(x, y, z, ...) } persp3D.f2d(f2d, -1, 1, -1, 1, 50, 50, ticktype = &quot;detailed&quot;) C.2.2 Método del trapezoide Error \\(O(n^{-\\frac{2}{d}})\\). trapezoid.mat &lt;- function(z, hx, hy) { # Integración numérica bidimensional # utilizando el método del trapezoide (se aproxima f linealmente) f.vec &lt;- apply(z, 1, function(x) trapezoid.vec(x, hx)) return(trapezoid.vec(f.vec, hy)) } # trapezoid.mat(z, hx, hy) trapezoid.f2d &lt;- function(f2d, ax=-1, bx=1, ay=-1, by=1, nx=21, ny=21) { x &lt;- seq(ax, bx, length = nx) y &lt;- seq(ay, by, length = ny) hx &lt;- x[2]-x[1] hy &lt;- y[2]-y[1] z &lt;- outer(x, y, f2d) trapezoid.mat(z, hx, hy) } trapezoid.f2d(f2d, -1, 1, -1, 1, 101, 101) ## [1] -8.881784e-18 C.2.3 Comandos de R Suponiendo que la función es vectorial, podemos emplear: integrate( function(y) { sapply(y, function(y) { integrate(function(x) f2d(x,y), ax, bx)$value }) }, ay, by) ## -2.775558e-17 with absolute error &lt; 1.1e-14 Si la función no es vectorial y solo admite parámetros escalares: integrate(function(y) { sapply(y, function(y) { integrate(function(x) { sapply(x, function(x) f2d(x,y)) }, ax, bx)$value }) }, ay, by) Fuente: tolstoy.newcastle.edu.au. Alternativamente se podría emplear la función adaptIntegrate() del paquete cubature. "],["soluciones.html", "D Soluciones ejercicios", " D Soluciones ejercicios A continuación se muestran soluciones de algunos de los ejercicios no resueltos en el texto. "],["capítulo-1-introducción-a-la-simulación.html", "D.1 Capítulo 1 Introducción a la simulación", " D.1 Capítulo 1 Introducción a la simulación D.1.1 Ejercicio 1.1 Enunciado 1.1: Sea \\((X,Y)\\) es un vector aleatorio con distribución uniforme en el cuadrado \\([-1,1]\\times\\lbrack-1,1]\\) de área 4. Aproximar mediante simulación \\(P\\left(X + Y \\leq 0 \\right)\\) y compararla con la probabilidad teórica (obtenida aplicando la regla de Laplace \\(\\frac{\\text{área favorable}}{\\text{área posible}}\\)). Generamos nsim = 10000 valores del proceso bidimensional: set.seed(1) nsim &lt;- 10000 x &lt;- runif(nsim, -1, 1) y &lt;- runif(nsim, -1, 1) La probabilidad teórica es 1/2 y la aproximación por simulación es la frecuencia relativa del suceso en los valores generados (para calcularla podemos aprovechar que R maneja internamente los valores lógicos como 1, TRUE, y 0, FALSE): indice &lt;- (x + y &lt; 0) sum(indice)/nsim ## [1] 0.4996 Alternativamente (la frecuencia relativa es un caso particular de la media) se puede obtener de forma más simple como: mean(indice) ## [1] 0.4996 Aproximar el valor de \\(\\pi\\) mediante simulación a partir de \\(P\\left( X^2 +Y^2 \\leq 1 \\right)\\). set.seed(1) n &lt;- 10000 x &lt;- runif(n, -1, 1) y &lt;- runif(n, -1, 1) indice &lt;- (x^2+y^2 &lt; 1) mean(indice) ## [1] 0.7806 pi/4 ## [1] 0.7853982 pi_aprox &lt;- 4*mean(indice) pi_aprox ## [1] 3.1224 Generamos el correspondiente gráfico (ver Figura D.1) (los puntos con color negro tienen distribución uniforme en el círculo unidad; esto está relacionado con el método de aceptación-rechazo, ver Ejemplo ??, o con el denominado método hit-or-miss). # Colores y símbolos dependiendo de si el índice correspondiente es verdadero: color &lt;- ifelse(indice, &quot;black&quot;, &quot;red&quot;) simbolo &lt;- ifelse(indice, 1, 4) plot(x, y, pch = simbolo, col = color, xlim = c(-1, 1), ylim = c(-1, 1), xlab=&quot;X&quot;, ylab=&quot;Y&quot;, asp = 1) # asp = 1 para dibujar circulo symbols(0, 0, circles = 1, inches = FALSE, add = TRUE) symbols(0, 0, squares = 2, inches = FALSE, add = TRUE) Figura D.1: Valores generados con distribución uniforme bidimensional, con colores y símbolos indicando si están dentro del círculo unidad. D.1.2 Ejercicio 1.2 Enunciado 1.2: Consideramos el experimento de Bernoulli consistente en el lanzamiento de una moneda. Empleando la función sample, obtener 1000 simulaciones del lanzamiento de una moneda (0 = cruz, 1 = cara), suponiendo que no está trucada. Aproximar la probabilidad de cara a partir de las simulaciones. set.seed(1) nsim &lt;- 10000 x &lt;- sample(c(cara = 1, cruz = 0), nsim, replace = TRUE, prob = c(0.5,0.5)) mean(x) ## [1] 0.4953 barplot(100*table(x)/nsim, ylab = &quot;Porcentaje&quot;) # Representar porcentajes Figura D.2: Frecuencias relativas de los valores generados con distribución Bernoulli (aproximaciones por simulación de las probabilidades teóricas). En R pueden generarse valores de la distribución de Bernoulli mediante la función rbinom(nsim, size=1, prob). Generar un gráfico de lineas considerando en el eje \\(X\\) el número de lanzamientos (de 1 a 10000) y en el eje \\(Y\\) la frecuencia relativa del suceso cara (puede ser recomendable emplear la función cumsum). set.seed(1) nsim &lt;- 1000 p &lt;- 0.4 x &lt;- rbinom(nsim, size = 1, prob = p) # Simulamos una Bernoulli # Alternativa programación: x &lt;- runif(nsim) &lt; p mean(x) ## [1] 0.394 n &lt;- 1:nsim plot(n, cumsum(x)/n, type=&quot;l&quot;, ylab=&quot;Proporción de caras&quot;, xlab=&quot;Número de lanzamientos&quot;, ylim=c(0,1)) abline(h=p, lty=2, col=&quot;red&quot;) Figura D.3: Gráfico de convergencia de la aproximación por simulación a la probabilidad teórica. D.1.3 Ejercicio 1.3 Enunciado 1.3: Simular el paso de corriente a través del siguiente circuito, donde figuran las probabilidades de que pase corriente por cada uno de los interruptores: Considerar que cada interruptor es una variable aleatoria de Bernoulli independiente para simular 1000 valores de cada una de ellas. Nota: R maneja internamente los valores lógicos como 1 (TRUE) y 0 (FALSE). Recíprocamente, cualquier número puede ser tratado como lógico (al estilo de C). El entero 0 es equivalente a FALSE y cualquier entero distinto de 0 a TRUE. set.seed(1) nsim &lt;- 10000 x1 &lt;- rbinom(nsim, size=1, prob=0.8) x2 &lt;- rbinom(nsim, size=1, prob=0.9) z1 &lt;- x1 | x2 # Operador lógico &quot;O&quot; x3 &lt;- rbinom(nsim, size=1, prob=0.6) x4 &lt;- rbinom(nsim, size=1, prob=0.5) z2 &lt;- x3 | x4 z3 &lt;- z1 | z2 x5 &lt;- rbinom(nsim, size=1, prob=0.7) fin &lt;- z3 &amp; x5 # Operador lógico &quot;Y&quot; mean(fin) ## [1] 0.692 D.1.4 Ejercicio 1.4 Enunciado 1.4 (el problema del Caballero de Méré): En 1651, el Caballero de Méré le planteó a Pascal una pregunta relacionada con las apuestas y los juegos de azar: ¿es ventajoso apostar a que en cuatro lanzamientos de un dado se obtiene al menos un seis? Este problema generó una fructífera correspondencia entre Pascal y Fermat que se considera, simbólicamente, como el nacimiento del Cálculo de Probabilidades. Escribir una función que simule el lanzamiento de \\(n\\) dados. El parámetro de entrada es el número de lanzamientos \\(n\\), que toma el valor 4 por defecto, y la salida debe ser TRUE si se obtiene al menos un 6 y FALSE en caso contrario. deMere &lt;- function(n = 4){ lanz &lt;- sample(1:6, replace=TRUE, size=n) return(6 %in% lanz) } n &lt;- 4 lanz &lt;- sample(1:6, replace=TRUE, size=n) lanz ## [1] 3 5 1 6 6 %in% lanz ## [1] TRUE Utilizar la función anterior para simular \\(nsim=10000\\) jugadas de este juego y calcular la proporción de veces que se gana la apuesta (obtener al menos un 6 en \\(n\\) lanzamientos), usando \\(n=4\\). Comparar el resultado con la probabilidad teórica \\(1-(5/6)^{n}\\). set.seed(1) n &lt;- 4 nsim &lt;- 10000 mean(replicate(nsim, deMere(n))) ## [1] 0.5148 1-(5/6)^n ## [1] 0.5177469 D.1.5 Ejercicio 1.5 Enunciado 1.5 (variación del problema del coleccionista, cadena de Markov): Continuando con el ejemplo de la Sección 1.1.1 (álbum con \\(n = 75\\) cromos y sobres con \\(m = 6\\)). A partir de \\(nsim=2000\\) simulaciones de coleccionistas de cromos, aproximar por simulación la evolución del proceso de compra de un coleccionista (número de cromos distintos dependiendo de los sobres comprados). Generamos nsim = 2000 simulaciones de coleccionistas de cromos: # Parámetros n &lt;- 75 # Número total de cromos m &lt;- 6 # Número de cromos en cada sobre repe &lt;- TRUE # Repetición de cromos en cada sobre # Número de simulaciones nsim &lt;- 2000 # Resultados simulación nsobres &lt;- numeric(nsim) # Número de sobres evol &lt;- vector(&quot;list&quot;, nsim) # Evolución del número de cromos # Por comodidad se podría haber fijado un número máximo de cromos # evol &lt;- matrix(nrow = max_len, ncol = nsim) # Fijar semilla set.seed(1) # Bucle simulación for (isim in 1:nsim) { # seed &lt;- .Random.seed # .Random.seed &lt;- seed # Simular album &lt;- logical(n) evolucion &lt;- c() i &lt;- 0 # Número de sobres repeat{ i &lt;- i + 1 sobre &lt;- sample(n, m, replace = repe) album[sobre] &lt;- TRUE ncromos &lt;- sum(album) evolucion &lt;- c(evolucion, ncromos) if (ncromos == n) { nsobres[isim] &lt;- i evol[[isim]] &lt;- evolucion break } } } # simres::plot.sr(nsobres) evol contiene las realizaciones de la cadena de Markov. # plot(evol[[1]], type = &quot;l&quot;) Combinar realizaciones del proceso (evoluciones del número de cromos): # Se extienden a la máxima longitud max_len &lt;- max(lengths(evol)) # max(sapply(evol, length)) evol &lt;- sapply(evol, function(x) c(x, rep(n, max_len - length(x)))) str(evol) ## num [1:167, 1:2000] 6 12 16 21 23 25 30 34 37 38 ... Aproximar cuantiles (intervalos de predicción): alpha &lt;- 0.05 limits &lt;- apply(evol, 1, quantile, probs = c(alpha, 0.5, 1-alpha)) str(limits) ## num [1:3, 1:167] 5 6 6 10 11 12 14 16 18 18 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:3] &quot;5%&quot; &quot;50%&quot; &quot;95%&quot; ## ..$ : NULL Ejemplo, aproximación de los límites (y mediana) para el número de cromos en el álbum después de comprar 20 sobres: limits[, 20] ## 5% 50% 95% ## 55 60 64 hist(evol[20, ], breaks = &quot;FD&quot;, freq = FALSE, main = &quot;&quot;, xlab = &quot;Número de cromos distintos (en 20 sobres)&quot;) abline(v = limits[, 20], lty = 2) Representar las realizaciones del proceso y los intervalos de predicción puntuales: matplot(1:max_len, evol, type = &quot;l&quot;, col = &quot;lightgray&quot;, lty = 1, xlab=&quot;Número de sobres&quot;, ylab=&quot;Número de cromos distintos&quot;) matlines(1:max_len, t(limits), lty = c(2, 1, 2), col = 1) "],["capítulo-2-generación-de-números-pseudoaleatorios.html", "D.2 Capítulo 2 Generación de números pseudoaleatorios", " D.2 Capítulo 2 Generación de números pseudoaleatorios D.2.1 Ejercicio 2.1 Enunciado 2.1: Uno de los primeros generadores utilizados fue el denominado método de los cuadrados medios propuesto por Von Neumann (1946). Con este procedimiento se generan números pseudoaleatorios de 4 dígitos de la siguiente forma: Se escoge un número de cuatro dígitos \\(x_0\\) (semilla). Se eleva al cuadrado (\\(x_0^2\\)) y se toman los cuatro dígitos centrales (\\(x_1\\)). Se genera el número pseudo-aleatorio como \\[u_1=\\frac{x_1}{10^{4}}.\\] Volver al paso ii y repetir el proceso. Para obtener los \\(k\\) (número par) dígitos centrales de \\(x_{i}^2\\) se puede utilizar que: \\[x_{i+1}=\\left\\lfloor \\left( x_{i}^2-\\left\\lfloor \\dfrac{x_{i}^2}{10^{(2k-\\frac{k}2)}}\\right\\rfloor 10^{(2k-\\frac{k}2)}\\right) /10^{\\frac{k}2}\\right\\rfloor\\] Este algoritmo está implementado en la función simres::rvng() (ver también simres::rng(); fichero rng.R): simres::rvng ## function(n, seed = as.numeric(Sys.time()), k = 4) { ## seed &lt;- seed %% 10^k ## aux &lt;- 10^(2*k-k/2) ## aux2 &lt;- 10^(k/2) ## u &lt;- numeric(n) ## for(i in 1:n) { ## z &lt;- seed^2 ## seed &lt;- trunc((z - trunc(z/aux)*aux)/aux2) ## u[i] &lt;- seed/10^k ## } ## # Almacenar semilla y parámetros ## assign(&quot;.rng&quot;, list(seed = seed, type = &quot;vm&quot;, parameters = list(k = k)), ## envir = globalenv()) ## # .rng &lt;&lt;- list(seed = seed, type = &quot;vm&quot;, parameters = list(k = k)) ## # Para continuar con semilla y parámetros: ## # with(.rng, rvng(n, seed, parameters$k)) ## # Devolver valores ## return(u) ## } ## &lt;bytecode: 0x00000000331268d0&gt; ## &lt;environment: namespace:simres&gt; Estudiar las características del generador de cuadrados medios a partir de una secuencia de 500 valores. Emplear únicamente métodos gráficos. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
